lv9vkqc,1gj1fyn,"How many APIs can this thing handle at a time? Or put differently, how  many APIs are you using at the moment with this project and is it working as intended?",2024-11-03 16:48:25,1
lva7loe,1gj1fyn,"Not an agent, but cool and good job!",2024-11-03 18:01:00,1
lva6r77,1gj1fyn,I have configured 10 APIs - and they are CRUD APIs. Seems to pick those up with no trouble. Although you have to give less verbose descriptions of your APIs,2024-11-03 17:55:53,2
lv9tw2a,1gj1e9p,?,2024-11-03 16:38:38,10
lv9rj8b,1gj1e9p,Add a downwards bend to the horizontal pipe so your tokens slide out!,2024-11-03 16:25:00,17
lva1rh3,1gj1e9p,"I saw a paper recently that might solve this: ""DumpSTAR\* - Distributed Ultra-Matrix Protocol for Superfluous Token Analysis and Recycling""",2024-11-03 17:25:38,7
lva5vu0,1gj1e9p,Wild these comments are either 100% in the know or out of it and I’m out of it,2024-11-03 17:50:37,4
lv9rqvb,1gj1e9p,Is this Bin Laden meme?,2024-11-03 16:26:13,3
lva3mds,1gj1e9p,"Gotta make grade!  

Source: Spent $20k on plumbing this year.",2024-11-03 17:36:57,2
lv9u47h,1gj14oa,"Since this got a decent amount of upvotes and no comments I'll share what I've learned so far in case it's helpful to others.

Seems like RVC (https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/en/README.en.md) is a great option, but can be further improved by using it in combination with XTTS 2.

I'm going to try with just RVC first, then will try to incorporate XTTS 2. Will do my best to update here!",2024-11-03 16:39:54,6
lv9vcuh,1gj14oa,In my experience MaskCGT and OpenVoice did the best job. But I was trying on short clips for in context editing the video (using the but being replaced as the base).,2024-11-03 16:47:07,2
lva6imp,1gj14oa,"This is the best I've found, but I haven't been paying attention the last couples months so it could already be well outdated: 

[https://huggingface.co/coqui/XTTS-v2](https://huggingface.co/coqui/XTTS-v2)

UI here: [https://github.com/BoltzmannEntropy/xtts2-ui?tab=readme-ov-file](https://github.com/BoltzmannEntropy/xtts2-ui?tab=readme-ov-file)",2024-11-03 17:54:27,1
lva6nzh,1gj14oa,"GPTsovits, fish, either of the F5s. Nothing has enough soul. It will sound like your reference audio. All of those can be finetuned on your longer samples.

RVC can mask shittyness of the TTS as mentioned. Not sure if it's needed for these, as they clone pretty well. If you find a really good emotional TTS that can't mimic, RVC would save you there.",2024-11-03 17:55:22,1
lv9u0aw,1gj14oa,This is the video:https://youtu.be/MKw4DmH\_\_n4,2024-11-03 16:39:17,1
lva05w5,1gj14oa,"RVC changes the input audio voice to the cloned voice, and it's pretty good at it. The main issue is that it can't work by itself and relies on an input audio file in order to convert it. If the input voice is from a human, RVC conversion sounds great. And RVC doesn't need transcriptions for the audio files when training the model too.

But if you use a TTS as the input audio (like XTTS2), then it doesn't sound as good. All TTS I've used so far has artifacts in the audio or it doesn't sound natural. RVC can mask those issues a bit, but doesn't entirely remove it.

I hope I'm wrong, but half of the XTTS2 + RVC readings doesn't sound natural to me. Or maybe I'm just using XTTS2 wrong. Chatgpt has a good TTS that sounds great, but I doubt they'll ever release it.

Edit: also, this guy says StyleTTS2 sounds more natural. https://www.reddit.com/r/LocalLLaMA/comments/180mrv7/is_it_possible_to_finetune_coquixttsv2_as_a/kq1xmwn/

metavoice also seems good and there's a github page for it as well. I never tried this or styleTTS2 though https://replicate.com/camenduru/metavoice/examples",2024-11-03 17:15:54,5
lv9uacy,1gj14oa,"""This video isn't available any more"" :(",2024-11-03 16:40:54,2
lva30n1,1gj14oa,"The video link worked for me just now, but it's in Chinese.",2024-11-03 17:33:14,2
lv9ee52,1gizq17,I can use them offline. I live on a farm with dial up.,2024-11-03 15:12:57,46
lv9drle,1gizq17,I don't have to worry they are making the model dumber over time to save costs,2024-11-03 15:09:38,51
lv9uef8,1gizq17,Rebuild the civilization from my laptop after WW3,2024-11-03 16:41:32,13
lv9kyrc,1gizq17,"For language models, it is more about data privacy. I handle sensitive data that cannot leave my machine. 

I use ComfyUI for image generation. With image generation  premium providers you're heavily regulated. Also, image generation costs a lot. But the best thing about local image generation is the amount of control you have as a user/creator. You would not believe the amount of control you have compared to using something like Dal-e or Gemini/grok etc.",2024-11-03 15:48:30,12
lv9k2ql,1gizq17,"I’ve just began with LMStudio MLX models.  
I’ve always been hesitant to share private documents, contracts and info in general with ChatGPT and Claude.  
Pixtral is great at extracting text even from bad images. Then manipulate the text with qwen 2.5.  
I have an M1 max 32gb. I can run qwen 2.5 14B 8bit at roughly 15/20 tokens per second. Quite respectable and its performance is comparable to the original GPT-4.  
  

I would like to upgrade and get a new mac with more RAM to run bigger models, but they would run slow since the bandwidth isn’t much bigger. A 30B model would run at 7/8 tokens per second and it’s probably the sweet spot in terms of performance and speed/usability. That means it would probably be useless for me to get more than 64GB of Ram.",2024-11-03 15:43:35,22
lv9j95s,1gizq17,"Nsfw, afaik. Privacy, full control over the process and quantization",2024-11-03 15:39:05,6
lv9gy5i,1gizq17,"I can say out-of-pocket shit without getting warnings or having to worry about a three letter agency show up at my house, or having the model chastise me.

Sometimes I just want to vent and say horrible shit without it leaving my house.",2024-11-03 15:26:39,19
lv9i3cl,1gizq17,"Batch processing is cheaper locally if you have a fixed multi shot prompt as most providers don't offer prefix caching savings. I've processed around 8B tokens on 8B model locally last month, that's around $400 if I used the cheapest api provider, but I paid like $10 or $20 for electricity.",2024-11-03 15:32:51,7
lv9e586,1gizq17,"For me it was about the cost. I wanted to churn through code bases. For some use cases the model doesn't have to be a walking encyclopedia, but rather a simple code chunk summarizer. Using OpenAI's APIs I was burning through 20 usd a weekend. Self hosting a llama just allowed me to iterate cheaper.",2024-11-03 15:11:39,11
lv9o6fv,1gizq17,Exceptional speed. Highly personalized to my domain.,2024-11-03 16:06:06,3
lv9hari,1gizq17,Using a custom-built sampler,2024-11-03 15:28:32,5
lv9r4dk,1gizq17,"I've been playing around with code help for a large monolith, and I can do so without IP problems and can repeatedly run different queries and tests without exceeding token limits. 

Even if I end up using a paid service, at least I understand how LLMs work much better than I did before",2024-11-03 16:22:40,2
lva09ft,1gizq17,There are hundreds of uncensored models you can download - you can ask questions that are forbidden online.,2024-11-03 17:16:29,2
lv9klsf,1gizq17,"Certain use cases that I think it enables for you are anywhere that you don’t want to share with a closed source model. For example, you can use an open source LLM for code completion/suggestions without turning over your code base to one of the big tech guys",2024-11-03 15:46:29,1
lva11hm,1gizq17,Small models and real time processing of text.,2024-11-03 17:21:14,1
lva7moj,1gizq17,"I feel more close to my code, if that sounds weird. Instead of copilot giving me the next snippet that is shared with _n_ amount of folks, I can use a local deepseek or starcoder llm instance that serves just me. Is it better than copilot? Nope, but at least I know that my LLM wont galavant around with my code to other potential competitors.",2024-11-03 18:01:11,1
lv9q1lr,1gizq17,"""google search"" without Internet access",2024-11-03 16:16:46,1
lv9uaoe,1gizq17,have you ever considered starlink?,2024-11-03 16:40:56,-3
lv9fsin,1gizq17,"The instability and inconsistencies are discombobulating to the soul, making you wonder if they’re actually unstable or if you’re the one who is inconsistent. However, when you use a local LLM as a reference point, it becomes very clear that services like Claude fluctuate significantly in both competence and usage quota.",2024-11-03 15:20:28,15
lv9x8rw,1gizq17,That’s a good use case,2024-11-03 16:58:20,7
lva5nts,1gizq17,Actually so cool the thought of this. A llama 3.2 would be insane for querying simple general knowledge.,2024-11-03 17:49:15,1
lv9qf6l,1gizq17,"See people, a net positive of AI. This guy could have been feeding shit sandwiches to homeless people and deflate children's footballs instead.",2024-11-03 16:18:51,4
lv9ok7i,1gizq17,"Ċa you elaborate on your setup? Planning to make the switch but unsure if it will end up being a mess & wasted effort on,y to go back to claude & copilot",2024-11-03 16:08:17,2
lv9gvky,1gizq17,I'm not a math guy but I'm sure it's gonna be a long long time before you see ROI at $20/wk,2024-11-03 15:26:16,2
lv9sesh,1gizq17,Can you expand on highly personalized to your domain?,2024-11-03 16:30:02,1
lv9sgaw,1gizq17,What model sizes do you trust for this?,2024-11-03 16:30:16,3
lv9wdi9,1gizq17,"Of course. It's quite expensive though when I have several computers with GPU's, plus i'm saving on API costs. I can use dial up for Claude.ai when I want a better performing model for more complex problems.",2024-11-03 16:53:08,5
lv9r99m,1gizq17,Oh definitely.,2024-11-03 16:23:26,1
lv9ptld,1gizq17,"Nothing comes close to copilot and Claude in terms of not wasting developers time. Had 3x3090 on a gaming motherboard and while it was super fun tinkering experience, the local LLMs don't seem to be worth it.",2024-11-03 16:15:30,1
lv9ifrv,1gizq17,"20 was the limit I didn't go over. I could easily burn it in 4 hours, if not being careful. For a point of reference, using aider to refactoring few files could easily cost few cents. Putting few files plus to generate a diff can saturate 64k context window easily. You also get randomly incorrect responses and need to retry.",2024-11-03 15:34:45,5
lv9w5kh,1gizq17,"The ontology within  the insurance field between claims, doctors, medications, referrals etc isn't well documented in the public. So we take that ontology graph and train the closed-sourced model to be great at understanding that - then apply guardrails that the model sticks to that domain. So while the model itself may have its weights adjusted on general Q/A it would be good for hyper-personalized training in its domain.",2024-11-03 16:51:50,4
lv9zilo,1gizq17,"what do you mean by ""trust""? do you believe in stuff you find in search? I can point fake knowledge even in wikipedia, and google/facebook/reddit is filled with incorrect information, so why local LLM is worse?",2024-11-03 17:11:55,-1
lv9tvnm,1gizq17,Were you PCIe lane limited?  Gaming mobo probably only supported 1x16 or maybe 2x8 slots.  A server board can support up to 128 lanes to unlock the full potential of those cards.,2024-11-03 16:38:34,2
lv9lrez,1gizq17,"This is one of the reasons I built my own rig, but my experience with Aider and self hosted LLMs are a bit disappointing. How much (v)ram do you have and which model? 

I'm running Gwen 72B Q3 on 40gb vram and most of the times the responses do not adhere to the search and replace format",2024-11-03 15:52:58,3
lva21cq,1gizq17,"You really took that the wrong way. 

I’m asking because smaller models have less of a knowledge basis and get more things wrong. I probably wouldn’t use a 1b model as an offline google search. 

So I’m asking from your usage, if you trust 7b models? 13b? 70b? More?",2024-11-03 17:27:19,2
lv9wb7o,1gizq17,"Yes. The motherboard supports 2x8 config, but I didn't bother to enable it. Since I was mostly running existing models and not learning any (although would really wish to do so), I didn't see much point. It's 1x16 and whatever is  wired to the rest of the slots.

I might switch at some point to an Epyc build, but only once I'm really bottlenecked. For my occasional multigpu tinkering (even for blender), it's surprisingly a non issue",2024-11-03 16:52:45,2
lv9og44,1gizq17,"I had 3x3090, but one burned out. Same experience in general: aider just needs the best of the best to function. I think it'll stay that way until someone specifically finetunes a model on conversations using it's diff format.",2024-11-03 16:07:38,2
lva28ov,1gizq17,"I understood your question. Again, what do you mean by ""trust""?",2024-11-03 17:28:33,-2
lva3k6s,1gizq17,That you can believe the information being given to you is accurate,2024-11-03 17:36:34,1
lva3pa9,1gizq17,You can get lots of correct info from Llama 8B,2024-11-03 17:37:26,1
lva3z25,1gizq17,It’s like pulling teeth with you to get any sort of answer lmao. Thank you,2024-11-03 17:39:00,-1
lva2tvz,1giz9px,Thanks I'll try it out,2024-11-03 17:32:06,1
lv8y1yc,1giy3pt,I prefer gemma 2 2b,2024-11-03 13:46:54,18
lv92rnl,1giy3pt,Qwen2.5 for summaries and emails. Gemma2 for creative writing. Often I can go down to 1.5b for summaries with qwen,2024-11-03 14:11:53,12
lv8zpqg,1giy3pt,"Phi 3.5 for math and reasoning.
Gemma 2 2b for creative writing.
Llama 3 3b/Qwen 2.5 3b for instruction following/roleplay/chatting.",2024-11-03 13:55:39,11
lv8xjpw,1giy3pt,phi-3.5 and gemma2-2b,2024-11-03 13:44:16,6
lva3tme,1giy3pt,"I would use DeepSeek Coder V2 Lite (active param \~2.7B) for coding assistance. By the way, how about Granite-3.0 models?",2024-11-03 17:38:08,1
lv990ep,1giy3pt,Phi3.5:14b can't even handle maths problems my 11 year old asks it. I don't see how the little version could be useful.,2024-11-03 14:44:26,-3
lv9jlaa,1giy3pt,"at least for me, 3B mainly for summarization and checking English Structures for writing. Not yet for small 3B model for advanced task e.g. Coding, Answering Maths or real-life problems.",2024-11-03 15:40:57,1
lv8qxag,1giw2lr,"disclaimer: idk for sure with quad RTX cards cuz I'm poor, but I do have quad Pascal at PCIe 3.0 x8 and they used to be dual x4

4-way TP reaches something like 5 GB/sec (non scientific, I just stared at Nvidia-smi), so expect from 4 GB/sec to 8 GB/sec will help 10-20%

With 2-way on Q4 70B I saw 8.0 -> 8.5 TPS going to all x8 from x4.",2024-11-03 13:09:27,3
lv8ww2n,1giw2lr,I’ve also got the same case and how the heck did you. Fit the gpus? What risers did you use?,2024-11-03 13:40:49,2
lv9dp40,1giw2lr,"For reference, I've got 4x3090 connected with pcie4.0 x16 lanes each (and 2x nvlinks). Using VLLM with an awq quant of Mistral-Large-Instruct-2407 I get ~31t/s output generation. You could try that and compare to see what you're leaving on the table. 


NVLinking the 3090s may help you a bit. In theory, 1/3rd of the communication between each 3090 and the other cards could go through the NVLink, l freeing up PCIe bandwidth for the 3090s to talk to the 4090s.


Edit: your best bet may a tensor parallel size of two with pipeline parallel size 2. If the 4090s can pair with each other, they can not be slowed down by the slow interconnect with the 3090s.",2024-11-03 15:09:14,1
lva53n3,1giw2lr,"I get roughly ~16-17 using 3x3090 on 3.0x16. I imagine you gain a token/s.  Would have to load mistral up rather than the Q6 qwen I have.

     155 tokens generated in 9.07 seconds (Queue: 0.0 s, Process: 6583 cached tokens and 1 new tokens at 57.58 T/s, 
Generate: 17.12 T/s, Context: 6584 tokens)",2024-11-03 17:45:49,1
lv91vku,1giw2lr,"I've been playing tetris a long time  
  
1st GPU is  here:  
[https://www.amazon.com/Lian-Li-Universal-Vertical-Compatible/dp/B0CF9SYCZT/ref=sr\_1\_1?crid=1P14NOQP9UITS&dib=eyJ2IjoiMSJ9.VOjthnEajPXMERDk2qaamOUJsZMfTJ8T2bCTL4oREcNLO0gVTrWzwsbZkHIlHcg4avYcDv5JV6H9n-qblILNJkeYmKwNXdqNYjgjuxIUDD6WohpVmfyqCRPFeldGHN7kOI2fTrn6FDRduIK1h5JDlbvk0zN-GZofBy5xplYeKHjk\_nzkXFrXZblfqAd-rDu9JnQhqF2I5b3I3xBQ1\_nrbXAi4LG23qzvTfAtI3sUoI0.xWOeEI8GLkkZ2gtRUeZ2t8CuKFQuLcbkCmxQJMoEuD8&dib\_tag=se&keywords=lian%2Bli%2Bvertical%2Bgpu%2Bmount&qid=1730671263&sprefix=Lian%2Bli%2Bvertic%2Caps%2C167&sr=8-1&th=1](https://www.amazon.com/Lian-Li-Universal-Vertical-Compatible/dp/B0CF9SYCZT/ref=sr_1_1?crid=1P14NOQP9UITS&dib=eyJ2IjoiMSJ9.VOjthnEajPXMERDk2qaamOUJsZMfTJ8T2bCTL4oREcNLO0gVTrWzwsbZkHIlHcg4avYcDv5JV6H9n-qblILNJkeYmKwNXdqNYjgjuxIUDD6WohpVmfyqCRPFeldGHN7kOI2fTrn6FDRduIK1h5JDlbvk0zN-GZofBy5xplYeKHjk_nzkXFrXZblfqAd-rDu9JnQhqF2I5b3I3xBQ1_nrbXAi4LG23qzvTfAtI3sUoI0.xWOeEI8GLkkZ2gtRUeZ2t8CuKFQuLcbkCmxQJMoEuD8&dib_tag=se&keywords=lian%2Bli%2Bvertical%2Bgpu%2Bmount&qid=1730671263&sprefix=Lian%2Bli%2Bvertic%2Caps%2C167&sr=8-1&th=1)

2nd GPU is here:  
[https://www.amazon.com/Lian-Li-O11DEXL-1W-Upright-Compatilble-Included/dp/B0CHJTP22B/ref=sr\_1\_4?crid=L8K3VJPBGGVB&dib=eyJ2IjoiMSJ9.JoIk9Z1tiftzFdyqvHCGXk6I3RP3BhWvt\_s1K8Fv\_k9RghFhvWiJHpiDGySLhhq\_lUp8l7N2L0cPagX8Bha0ibWL9qe\_Amat\_dSJ1BNjqIoTQDY9lpexMQIImz-HBLfjAIEQYE\_HxywluER8OmFGhB2Fik8rAUfMfm0AO9gFNz0fAyZNj42YJ6S36VLrM9gfv4XWU0gwm-P4t75JNdyZs08SF1x0b0qTUvpDbN6JE50.iMvVqotWUz5qe83\_zfpwnJ2Xp8urwBWv\_4KO8wsN8Ko&dib\_tag=se&keywords=lian+li+gpu+mount&qid=1730671342&sprefix=lian+li+gpu+mount%2Caps%2C168&sr=8-4](https://www.amazon.com/Lian-Li-O11DEXL-1W-Upright-Compatilble-Included/dp/B0CHJTP22B/ref=sr_1_4?crid=L8K3VJPBGGVB&dib=eyJ2IjoiMSJ9.JoIk9Z1tiftzFdyqvHCGXk6I3RP3BhWvt_s1K8Fv_k9RghFhvWiJHpiDGySLhhq_lUp8l7N2L0cPagX8Bha0ibWL9qe_Amat_dSJ1BNjqIoTQDY9lpexMQIImz-HBLfjAIEQYE_HxywluER8OmFGhB2Fik8rAUfMfm0AO9gFNz0fAyZNj42YJ6S36VLrM9gfv4XWU0gwm-P4t75JNdyZs08SF1x0b0qTUvpDbN6JE50.iMvVqotWUz5qe83_zfpwnJ2Xp8urwBWv_4KO8wsN8Ko&dib_tag=se&keywords=lian+li+gpu+mount&qid=1730671342&sprefix=lian+li+gpu+mount%2Caps%2C168&sr=8-4)

3rd GPU is [https://www.amazon.com/gp/product/B07YDGK7K7/ref=ppx\_yo\_dt\_b\_search\_asin\_image?ie=UTF8&th=1](https://www.amazon.com/gp/product/B07YDGK7K7/ref=ppx_yo_dt_b_search_asin_image?ie=UTF8&th=1)

4th GPU is just a generic pcie 3.0 cable that runs under the motherboard into the back compartment

https://preview.redd.it/eyc0sxd3eryd1.jpeg?width=3024&format=pjpg&auto=webp&s=eb8d2b3183119681df0459d0cd78003795b1d805",2024-11-03 14:07:05,2
lv9k00c,1giw2lr,"P.S. it probably won't make a difference, but it might be worth trying swapping the connections between the 3090s and the 4090s (If you don't get an NVLink). If you have the faster cards on the slower connections, perhaps you'll get a speed up because it doesn't matter as much that they're waiting longer to receive all of the data they need to start performing calculations and/or they will be able to start sending data sooner after finishing calculations.


Just a theory. If you validate it please share!",2024-11-03 15:43:10,2
lv8uvmv,1giuvwm,"You would take your table and convert it into plain text instruction/answer pairs with formatting you want the model to follow.

See here: [https://wandb.ai/capecape/alpaca\_ft/reports/How-to-fine-tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2](https://wandb.ai/capecape/alpaca_ft/reports/How-to-fine-tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)

Note that fine tuning in this way will not teach the model the actual data in your dataset. It will only teach the model the patterns it should follow. So for example you can teach an LLM to summarize using a dataset of long/summarized instruction/answer pairs you wrote about your own life story. The resulting fine tune will summarize provided text in a way similar to how you did, but it will not have learned anything about your life story.",2024-11-03 13:30:07,2
lv9c50c,1giuvwm,"I'm a fan of treating the context like a fine-tuning dataset, it can be very similar, except much fewer examples are needed... The transformer model has learned how to learn

Plus it has the benefit of easily learning factual knowledge in-context, unlike fine-tuning

Also you don't need the instructions if you're doing this, in fact you will get better performance if you avoid chat/instruct tokens (or just use a base model) so you don't get those chatbot biases

Also you can maximize the performance by keeping the kv cache around, especially for classification, you'd only need to generate one token for each class you want to test for https://www.reddit.com/r/LocalLLaMA/comments/1cmoj95/a_fairly_minimal_example_reusing_kv_between/

If you don't need to test for multiple classes you can turn it into a much simpler few-shot multiple-choice problem and only ever need to generate one token to classify https://www.reddit.com/r/MachineLearning/comments/1cm9r0y/comment/l2yv6y5/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",2024-11-03 15:00:53,1
lv93gm7,1giuvwm,"Thanks a lot. Fien-tuning this way involves ""touching"" the model weights and hence some serious compute, right?   
  
Is it somehow possible to do in-context learning, but without repeating the training set in the context every time? Sorry if it looks too silly a question :-/",2024-11-03 14:15:38,1
lv814k2,1gitsek,Here's the link to the original vulnerability mentioning affected drivers https://nvidia.custhelp.com/app/answers/detail/a_id/5586,2024-11-03 10:58:19,60
lv7ycae,1gitsek,TLDR: update your Nvidia driver,2024-11-03 10:44:44,122
lv7zolg,1gitsek,"Man, sure would be nice if the article mentioned the affected driver versions and what version I need to update to. Oh well!",2024-11-03 10:51:18,76
lv8mt8k,1gitsek,Forbes is a cancer of a media institution. Can we please ban them as a source?,2024-11-03 12:48:28,39
lv8d3ph,1gitsek,Sounds like privilege escalation and out of bounds error. How would anyone even exploit this without being on the system already? WebGPU?,2024-11-03 11:59:22,15
lv8p5z4,1gitsek,"For people NOT using VGPU this is the only LINUX affecting one:
per.
https://nvidia.custhelp.com/app/answers/detail/a_id/5586/~/security-bulletin%3A-nvidia-gpu-display-driver---october-2024

CVE‑2024‑0126 	NVIDIA GPU Display Driver for Windows and Linux contains a vulnerability which could allow a privileged attacker to escalate permissions. A successful exploit of this vulnerability might lead to code execution, denial of service, escalation of privileges, information disclosure, and data tampering.


And the affected / updated  versions are there listed as:

Driver Branch 	Affected Driver Versions 	Updated Driver Version

R565 	(All driver versions prior to 565.57.01) 	565.57.01

R550 	(All driver versions prior to 550.127.05) 	550.127.05

R535 	(All driver versions prior to 535.216.01) 	535.216.01",2024-11-03 13:00:35,8
lv8gxqd,1gitsek,"so looks like you jsut need to get 566.03 driver, for windows at least",2024-11-03 12:18:56,5
lv8pgoa,1gitsek,"There was also some weeks ago another one relevant to the nvidia container toolkit:

https://www.bleepingcomputer.com/news/security/critical-flaw-in-nvidia-container-toolkit-allows-full-host-takeover/",2024-11-03 13:02:03,2
lv8k1hu,1gitsek,Is this just their way to force an upgrade to buggy new drivers? Their releases were filled with bugs for a long while.,2024-11-03 12:34:36,2
lv9vykj,1gitsek,"I think its an overhyped bug. It's a local privilege escalation, it affects multi-users systems as any user could hack or bring the whole system down. But for most users, there is really no risk. Drivers crash constantly anyway.",2024-11-03 16:50:41,1
lva5eew,1gitsek,"AMD gang, rise up",2024-11-03 17:47:38,1
lv92nb9,1gitsek,"> Urgent New

>> Updated 10/22/2024 - https://nvidia.custhelp.com/app/answers/detail/a_id/5586

>> Article - Oct 25, 2024

Good morning OP.",2024-11-03 14:11:14,-1
lv9s2pz,1gitsek,Thank god this only affects gamers,2024-11-03 16:28:09,-1
lv9ou9s,1gitsek,"thank you, forbes is such garbage",2024-11-03 16:09:54,5
lv9ogff,1gitsek,"Well, that sucks. I've been avoiding updating because Nvidia's latest updates have been terrible.",2024-11-03 16:07:41,6
lv88tqc,1gitsek,The linked Nvidia article from u/x2z6d has all the info you need,2024-11-03 11:37:40,10
lv90asl,1gitsek,"I believe anytime code is running on your computer that isn't running as admin already. Could come from a downloaded exe, or any other exploit like how counterstrike used to have a remote code execution vuln.",2024-11-03 13:58:40,4
lv98waz,1gitsek,Multi tenancy/shared GPU environments like cloud.,2024-11-03 14:43:51,1
lv92ci0,1gitsek,Can you link your bug reports?,2024-11-03 14:09:38,-2
lv9gtx8,1gitsek,Ah... OK. Link to where you posted it earlier.,2024-11-03 15:26:01,3
lv9phga,1gitsek,For real! An update of nvidia drivers on Linux usually means bricked linux box. 😂  I have a script that purposely avoids nvidia driver updates for this very reason.,2024-11-03 16:13:35,4
lv91o2w,1gitsek,https://www.reddit.com/r/LocalLLaMA/s/Ce3prPUy3N,2024-11-03 14:05:57,5
lv96x9j,1gitsek,"I have to pay a trillion-dollar company that abuses their position on the market AND do the testing for them? I'd file report for OSS, for such an overpriced product I'd expect it not to be broken with an update after I bought it. I hope it's not an unreasonable expectation from your point of view.",2024-11-03 14:33:35,2
lv9m62t,1gitsek,"> I'd file report for OSS

https://github.com/NVIDIA/open-gpu-kernel-modules/issues

Which one of these are yours?

> do the testing for them? 

Reporting bugs is not testing.

If you wanna complain about bugs, complain about bugs.  
In a bug report.",2024-11-03 15:55:11,2
lv97q79,1git52m,There’s nothing wrong with trying as long as you have the time and resources. If your language is similar to any other popular language that’d probably work best. If it’s not then that’s harder. These LLMs have seen millions of tokens for most languages to use them well. So if your language is very different then you can’t just use a few samples. You have to build/generate a fairly large amount of code and train/finetune. If the language is similar to any other popular existing language the. You might get away with a smaller dataset. Even in this case I think you’re still looking at 100s of not 1000s of samples. Because you’re not exactly customizing a parser or something here. The model has to learn all the differences well enough,2024-11-03 14:37:47,2
lv8em64,1git52m,"It doesn’t work as well, no",2024-11-03 12:07:05,2
lv8m8yz,1git52m,https://old.reddit.com/r/LocalLLaMA/comments/1fyx27y/im_pretty_happy_with_how_my_method_worked_out/,2024-11-03 12:45:37,2
lva6pja,1git52m,"That gave me an interesting thought: why not ditch my grammar and parser and bolt on a syntax really familiar to LLMs like JSON?

Turns out there is a cottage industry of JSON-based PLs out there!",2024-11-03 17:55:37,1
lv7h4r6,1girzia,"Hi, I have posted about this personal hobby project a while ago and people seems to like it. Thus I refined it a bit, added some new features, and made it more usable. So I wanted to post about this again.



Currently this project's scope includes downloading and managing models from huggingface and either chat with them or do text gerneration with them while providing the information of what alternative words the AI could have chosen and their corresponding probabilities. There is a slider for the minimum probability of the words that get displayed and a toggleable heatmap overlay to show how uncertain the AI is on a word (how many alternative words), making it easy to find alternative paths to explore. All explored paths are saved so you can freely switch between them. 



The project is fully open sourced on [https://github.com/TC-Zheng/ActuosusAI](https://github.com/TC-Zheng/ActuosusAI) and I will be continue experimenting on fun new features while keep improving the old ones. If you have any issues or suggestions please let me know.",2024-11-03 09:20:13,54
lv8h0dp,1girzia,"it is wild to see how they massacred the model with the safety BS. 8 seconds in: the word that leads to the useful outcome is 1.3 % vs ""cannot"" 44.99%.

could be a useful tool to compare the uncensored version and see if the ""uncensoring"" worked and to what degree.",2024-11-03 12:19:19,18
lv7izj8,1girzia,This is something that I've wanted to have available for a while but haven't made myself. I'll have to try it out.,2024-11-03 09:29:19,16
lv86znk,1girzia,"This should be a standard part of every LLM suite. It would continously remind people that they're using a non-deterministic system based on chance and statistics.

Brilliant work.",2024-11-03 11:28:16,15
lv80tgx,1girzia,"This is a cool tool, but what it shows is certainly not its ""inner thoughts"".",2024-11-03 10:56:48,25
lv7rxwt,1girzia,"I remember we could do something similar in chatgpt playground page ? Access % and temperature and see what happens.

Congrats and super cool to have it enabled for all models !! Really massive to check up RAG and finetuning !!!",2024-11-03 10:13:28,7
lv7xdid,1girzia,"This is just too damn cool. I don't have docker installed, but I might give it a crack just to try this. Thanks for all your work on this, shocked something like this isn't common.

Can you directly edit the AI's response in addition to choosing different options?",2024-11-03 10:40:03,3
lv83gs7,1girzia,"This is going to be exceedingly useful for experiments, thank you for putting this out there under apache 2.0.",2024-11-03 11:10:11,3
lv7l3mm,1girzia,Great idea. Reminded me of this: https://youtu.be/zlJtohZSsfY?si=VXgZG8q6ca67_hC-,2024-11-03 09:39:54,2
lv7wjgj,1girzia,This is such a great way to explore the possibility space of responses!  Thanks for sharing!,2024-11-03 10:36:02,3
lv8846p,1girzia,Which models will this work with? And they have to be locally hosted right?,2024-11-03 11:34:02,2
lv89w6w,1girzia,This seems great! I'll give it a try!,2024-11-03 11:43:08,2
lv8n9iy,1girzia,"Damn. Having the possibility to do this is interesting, it's a little bit like doing research on Google. You're crossing the ""sources"" to understand the different scenarios.",2024-11-03 12:50:47,2
lv7sppp,1girzia,this is absolutely fucking amazing. decision tree X ai has so much potential its actually mindblowing! please share any projects related to this,2024-11-03 10:17:16,4
lv7yhah,1girzia,I guess it doesn't support base models? ,2024-11-03 10:45:24,1
lv92207,1girzia,"The llama.cpp web ui can display the colors, and on click, can display the probabilities, and I always thought how cool it would be to click on one choice and go from there.",2024-11-03 14:08:05,1
lva549e,1girzia,"This project is great! I'm quite sure someone came up with a quite cool idea earlier about increasing LLM's ability of creative writing: Instead of giving possible words, it provides multiple ""routes""(sentence/short paragraph) that are likely to happen for the output. Do you think it's something technically similar that you may implement into this project?",2024-11-03 17:45:55,1
lva6vdy,1girzia,"Okay, this is cool AF.",2024-11-03 17:56:35,1
lv7ii32,1girzia,That's amazing. How are you measuring the certainty?,2024-11-03 09:26:54,8
lv92abh,1girzia,"Man, I've been following this last post, the implementation looks very interesting.  

Do you have a roadmap yet for future features?  
I really want to see a tool that allows to visualize the possible paths an LLM can take (think of it as a tree with every token above a certain % being a node).  
I am aware that it would be rough performance wise, but it should be fairly parallelizabe, shouldn't it?",2024-11-03 14:09:18,1
lv96kub,1girzia,Really annoying that most models' default behaviour is to go straight to writing disclaimers.  Some days it feels like they were trained exclusively on fine print lol,2024-11-03 14:31:45,2
lv98d9k,1girzia,"Of course the safety team won't be using any tools similar to this, until ith reaches 100% of BS for refusals :D",2024-11-03 14:41:12,0
lv9ykrw,1girzia,"Also that there isn't a complete chain of thought before the AI spits out a response. For instance, the AI can say something like ""I was just thinking about this last week"" but that's not true, the AI didn't think about it until just after it was asked.",2024-11-03 17:06:12,1
lv83z1w,1girzia,"Well, I was expecting something else from the title, too, and I think it would have been best described with ""exploring word probabilities and alternative generations"", or something like that. Interesting anyway.",2024-11-03 11:12:46,9
lv8uti1,1girzia,"When I wrote inner thoughts I meant things that people have in their mind that they could have said but decided not to. But yeah now after thinking about it, I guess gpt o1's chain of thought kind of thing would be closer to what we meant by inner thoughts.",2024-11-03 13:29:49,4
lv9x9x6,1girzia,"At first I downvoted for the wording, but then I watched a bit more and thought it was pretty cool the way they visualized and implemented showing the logit probabilities.",2024-11-03 16:58:31,1
lv8xwyb,1girzia,"Currently it's not possible to edit the responses directly. It will take some time to implement but I am planning to do that. 

Also installing docker is an extremely simple process, but if I continue develop this and more people want to use it I guess I might make it an actual app or deploy it as website.",2024-11-03 13:46:10,2
lv8ylry,1girzia,"Currently it's intended to act like a local application where you will run the models on your computer, but it's developed as a web app so you can host it somewhere else if you know how to do that. 

And all llms from hugging face with no quantization or gguf quantization should work unless they are missing some metadata like chat template etc.",2024-11-03 13:49:50,1
lv7jkox,1girzia,"Basically, the hotter the color, the more alternative words you will see when you click on the word. This can also be controlled by the minimum probability slider, so if for example you don't want see words that the LLM has only 1-2% change producing, you can move the slider up and the heat map will update accordingly.",2024-11-03 09:32:16,15
lv8kf3x,1girzia,LLM's themselves assigns probabilities for outputs and temperature variable controls in a scale whether or not the model should pick other outputs than the most probable one.,2024-11-03 12:36:26,3
lv9yc7k,1girzia,"If it takes millions of dollars to train, I think corporate would want it to not give opportunity for any lawsuits.  So long as they made an effort to put in some guard rails, it would be easier to defend against any negative outcomes that result. ""While every effort had been made to ensure the safety and accuracy of the results, users should know that... Blah blah disclaimer""",2024-11-03 17:04:48,1
lv7jy2v,1girzia,"I meant on the implementation side. I see you're using llama-cpp-python and never knew that any of the probabilites can get throught it's API.

EDIT. Ah, okay. You're actually directly using transformers:

[https://github.com/TC-Zheng/ActuosusAI/blob/main/backend/actuosus\_ai/ai\_interaction/text\_generation\_service.py#L159](https://github.com/TC-Zheng/ActuosusAI/blob/main/backend/actuosus_ai/ai_interaction/text_generation_service.py#L159)

llama is there for some helper functions, not running the model. Ok ok",2024-11-03 09:34:07,12
lv7ldqa,1girzia,"No, I am actually using llama-cpp-python for inferencing gguf models. The llama\_get\_logits returns the logits from the last forward pass, and the probabilities are computed from the logits.",2024-11-03 09:41:17,21
lv7nl85,1girzia,"I didn't know that either, good to know.",2024-11-03 09:52:24,8
lv7nmyd,1girzia,"There's also a similar feature in the latest kobocpp build. I mean, token probabilities. 

 [Release koboldcpp-1.77 · LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp/releases/tag/v1.77)

It isn't compatible with streaming, though... 

Are you using the python wrapper to pseudostream in chunks?",2024-11-03 09:52:38,4
lv7quxb,1girzia,"Yeah, I think it would make sense to port it back to the text-generation-webui, kobold and others. Guessing someone will do that at some point",2024-11-03 10:08:05,3
lv854vk,1girzia,"my point is, it goes through some APIs",2024-11-03 11:18:43,2
lv8toqd,1girn7t,"There's some kind of hidden irony, that the UI for a program called ""tl;dw"" looks like that.

The pull of NotebookLM is like you click two buttons to have your info presented in whatever way you want, yours looks like a test case trying to break gradio's UI

No offense, I'm sure it's great, but I'll wait until there's an AI generated podcast explaining it to me.",2024-11-03 13:23:53,2
lv7bun9,1gir1by,It’s not 8k. It’s 2k. That’s what it was trained on.,2024-11-03 08:54:00,30
lv7eilf,1gir1by,"You may want to look at qwen2.5 0.5b, probably the tiniest model you'll get with a decent size context of 32k",2024-11-03 09:07:16,9
lv7e3x9,1gir1by,"Genuine question because I am curious, what are you using it for?

I have trouble imagining use cases to really small models, like 1B ok I see it,  but beyond that I struggle.

I would like to hear what you are doing with it if you do not mind sharing?",2024-11-03 09:05:15,2
lv7c35g,1gir1by,"I suspect any tiny model would struggle with long context, except maybe a mamba one?

Is 7B too big? There are some that shine at 128k, like chatglm, mistral megabeam, or internlm",2024-11-03 08:55:11,1
lv7k799,1gir1by,"If you have enough 128k context training data you can change the positional embeddings to use Yarn (https://arxiv.org/abs/2309.00071) instead of raw RoPE, and perform a few training steps on the longer docs to get it ""used"" to using that granularity of positional encodings. 
A step directly up to 128k may be a bit steep, but you can also always increment by 2x every couple hundred million tokens up to the context length you want.",2024-11-03 09:35:23,1
lv7orhx,1gir1by,"I don't see any reason to use long context since a lot of models fall apart after 32k, even gemini pro doesn't do its 1m context as well as you'd think",2024-11-03 09:57:56,-4
lv7lmgz,1gir1by,One use case is rewriting and summarizing English text locally on a phone; Meta stated that this is what they trained Llama3.2 1B for.,2024-11-03 09:42:31,6
lv8x8cr,1gir1by,You can use very small models for fast tool calling and routing to other models as long as your use case is constrained and clear.,2024-11-03 13:42:37,2
lv7p3nh,1gir1by,"Cool, I am watching for small models cross-language translation. I think I need to wait a little bit more for this, we had a lot of progress for sure. Cohere seems to contribute a lot to this. But we still need 7-8B to more for really reliable output now on this.

But in the pace things are going I am hopeful for improvements and better optimization in the future.",2024-11-03 09:59:32,1
lv79o6h,1giqxph,Would be cool to include f2-tts (came with f5) and maskgct (https://huggingface.co/amphion/MaskGCT),2024-11-03 08:43:11,10
lv7t5fy,1giqxph,XTTSv2 has a streaming mode that reduces latency to a fraction of a second if you are playing the audio immediately.,2024-11-03 10:19:26,8
lv84uya,1giqxph,Now we need a quality of voice comparison.,2024-11-03 11:17:18,6
lv784nv,1giqxph,"I recently conducted an analysis comparing the latency of various TTS models over different word counts, and here are the results.

📊 Key Highlights from the Benchmark:

\- Tortoise TTS has significantly higher latency as the word count increases.

\- Piper TTS, MeloTTS, and XTTS-v2 perform consistently well even at higher word counts.

For more details check out our blog:  
[https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-for-different-use-cases](https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-for-different-use-cases)",2024-11-03 08:35:39,13
lv7nijo,1giqxph,"Thanks for doing this, it's a really good reference... but those four shades of cyan are so close together I can't tell which is which.",2024-11-03 09:52:02,4
lv9gxgi,1giqxph,I would love to see something like this but also with the quality taken into account... even though that would be a huge pain to measure,2024-11-03 15:26:33,1
lv7fbpr,1giqxph,Would like to see fish speech,2024-11-03 09:11:19,1
lv7btvk,1giqxph,E2*,2024-11-03 08:53:53,7
lv7nlnx,1giqxph,I’m so excited about maskgct even if just using it to generate synthetic datasets,2024-11-03 09:52:27,3
lv7tuvf,1giqxph,Sure I will do consider some new models for the next set of test.,2024-11-03 10:22:55,2
lv7tlua,1giqxph,That is true but here I am not testing the Time to first byte (TTFB). I will do consider that for next set of test,2024-11-03 10:21:42,2
lv855q4,1giqxph,I still have the generated speech from the model. I can share them as well,2024-11-03 11:18:51,9
lv8bi3y,1giqxph,"I would recommend Piper for its speed. It may sound less natural, but  the fact that a Raspberry Pi can absolutely run it, makes it very useful.

And it sounds good enough to be clearly understood.",2024-11-03 11:51:14,2
lv811gx,1giqxph,This could be a research paper actually. Even if you’re not interested in doing a slightly deeper analysis into the models and how architecture affects response speed,2024-11-03 10:57:53,2
lv8r4tp,1giqxph,"You should understand how each of these methods work. This comparison is meaningless unless you use each method in the mode that minimizes latency. Default modes are provided for clarity, not minimum latency.",2024-11-03 13:10:32,1
lv9vfih,1giqxph,Excellent analysis. Thank you for sharing!,2024-11-03 16:47:33,1
lv7tr9z,1giqxph,"Thanks for the info, will take care of it next time.",2024-11-03 10:22:26,3
lv85i12,1giqxph,Would be great if it's no problem for you.,2024-11-03 11:20:37,5
lv91u2m,1giqxph,"Piper pulls in espeak_ng as dependency for some model which is GPLv3, one has to be extra careful when dealing with it in commercial settings. Great library otherwise.",2024-11-03 14:06:51,1
lv8ed1z,1giqxph,"Piper sounds okay, but it's not that good. This faster MeloTTS looks pretty interesting, it's also VITS based from the look of it and the samples they show sound potentially better. Could genuinely be a slight upgrade in both speed and quality.",2024-11-03 12:05:47,0
lv8rhpx,1giqxph,No. The approach is completely wrong. The repoted result is misleading.,2024-11-03 13:12:24,1
lv74j7p,1giqgck,Looks like an LLM wrote it.,2024-11-03 08:17:05,10
lv76nrf,1giqgck,"What the fuck are u talking about

Edit: Lmao he called me a ""fucking knucklehead"" and deleted his comment 🤣",2024-11-03 08:28:08,7
lv8d88p,1giqgck,what,2024-11-03 12:00:01,2
lv7jl10,1giqgck,/r/localllama is the best bot detector on the planet,2024-11-03 09:32:19,2
lv7tajk,1giqgck,"You \*look\* like an LLM, tbh",2024-11-03 10:20:08,-1
lv785x9,1giqgck,"Read his post history, it’s something…",2024-11-03 08:35:49,1
lv7tdlz,1giqgck,"lol.  I would use the word 'guffaw' but you probably would have to dictionary that up, now wouldn't you, son?",2024-11-03 10:20:34,-1
lv7t7vh,1giqgck,"Should we read yours, too?",2024-11-03 10:19:46,-6
lv7tsxz,1giqgck,"Sorry, I just did - and it's 'such empty'.  Get the literal fuck out of here",2024-11-03 10:22:40,-3
lv8z1f7,1giozl9,"This is nice App, Speech Note.
Many models and simple use.

Have many function:

-Translate text

-Text to Speech

-Speech to text



https://flathub.org/apps/net.mkiol.SpeechNote",2024-11-03 13:52:07,2
lv6kdux,1gindy1,"But they are only dual channel, so you can expect at most around 100 GB/s of memory bandwidth.",2024-11-03 06:29:21,25
lv6ffub,1gindy1,"I’m running 6200 stable on 128gb 4 dimms with Ryzen 7950x3D since 1+ year lol

Edit:
https://www.reddit.com/r/LocalLLaMA/s/Vqc3Zg9xOk",2024-11-03 05:59:44,23
lv7o4ch,1gindy1,"> Crucial and ADATA have 48GB ""4 channel"" options @ 5600.

4 dimms does not mean 4 channels. MBs have had 4 slots forever. Arrow Lake is dual channel.",2024-11-03 09:55:01,5
lv6zram,1gindy1,"On dd5 6000 in dual you get 1.7 t /s with llamacpp and llama 3.1 70b.

So the quad channel should get a bit more than 3 t/s.",2024-11-03 07:52:15,2
lv73ook,1gindy1,"I am thinking about getting a 285k because it looks like they're not too bad for running LLMs, and RAM compatibility is one of the reasons. 

However I assume with CUDIMMs this will become even easier in the future, so right now I am not really planning on filling all 4 DIMMs with expensive RAM, instead just getting a good 2x kit and then wait for a year or so until/if good CUDIMM kits show up with 4x48 @ 8000 or something like that.",2024-11-03 08:12:36,2
lv7htsr,1gindy1,probably better to wait a little bit for the 10k cudimms and their mobos to drop,2024-11-03 09:23:35,2
lv6in2t,1gindy1,What are the implications? Isn't it too slow for big LLMs?,2024-11-03 06:19:04,3
lv79obu,1gindy1,"Wouldn't 4 channel with 5600 = 5600 x 4 = 22,400 x 8 = 179,200 gb/s bandwidth? I think that's how the equation goes. Pair this up with a gpu to do the prompt processing and it might be decent.",2024-11-03 08:43:12,1
lv7it8l,1gindy1,"That's all still dual channel, no? That's gonna be too slow to be usable unless we're talking about big MoE's. It should be great for running Deepseek V2.5 though.",2024-11-03 09:28:27,1
lv87hc3,1gindy1,"Curious what kind of performance people are getting with high RAM machines. I have a machine with 4 sticks totalling 128GB DDR5 6000mhz. I realize after install I wasted my money on that much RAM as I could not clock it above 4000mhz. I have a 4090, and I run llms just using the 4090 24GB VRAM. 


Is there a way to leverage the system RAM I have with my GPU while running an LLM? If arrow lake allows for being able to clock 4 sticks to 6000+, curious what kind of difference that may make.",2024-11-03 11:30:47,1
lv6n3my,1gindy1,"Big if true, was literally luck of the draw until now even just to achieve 4800mhz on 4 sticks",2024-11-03 06:44:49,1
lv7j21p,1gindy1,Missed opportunity...,2024-11-03 09:29:40,1
lv6xxyg,1gindy1,"I also have 7950x3d ... which version of ddr5 are able to run with 6200 and 4 dimms?

I have been looking for such a combination for some time .",2024-11-03 07:43:02,7
lv7vxp0,1gindy1,"Looks like a $700 board, are there any cheaper boards that can do this with Zen 4/5?   I know the high speed DIMM stability is both a function of the CPU and the board .. 

Some of the Arrow Lake boards I linked are under $300 fwiw.",2024-11-03 10:33:04,1
lv8h96l,1gindy1,I have the 7800x3D and MSI X670E....did they finally fix that problem where you can't run 4 sticks at EXPO speeds without BSOD?,2024-11-03 12:20:34,1
lv9u2nt,1gindy1,Is this your RAM? [https://www.kingston.com/datasheets/KF560C36BBEK2-32.pdf](https://www.kingston.com/datasheets/KF560C36BBEK2-32.pdf),2024-11-03 16:39:40,1
lv7wa8g,1gindy1,"Yep - wasn't implying Arrow Lake is 4 channel,  just that you can run 4 DIMMs without going really slow on speed.   A lot of 4 DIMMers are stuck at 4400-4800 speeds.",2024-11-03 10:34:47,2
lv8yrvi,1gindy1,"it's not quad channel, just four slots dual channel unfortunately",2024-11-03 13:50:43,1
lv7w5x4,1gindy1,There's a good chance the CUDIMMs will push the limit up to about 8000 for Arrow Lake in 4 DIMM mode but I don't think (much) above that,2024-11-03 10:34:11,1
lv6mvra,1gindy1,"Most people who run actually \*\*large\*\* language models (70b, 123b at decent quantizations) do it in actual RAM, not VRAM.

Going from 1 token\\s to 2 (or even 1,5) token\\s can be a dramatic change here.",2024-11-03 06:43:35,9
lv7ogc1,1gindy1,Arrow lake is dual channel. 4 dimms does not mean 4 channels. MBs have had 4 slots forever.,2024-11-03 09:56:28,1
lv7sclc,1gindy1,"Well at least now that it's this whole chiplet architecture thing, they could easily go there rather soon... right?",2024-11-03 10:15:27,1
lv773lz,1gindy1,Please view the attached screenshot in the link it has all settings and hardware for you,2024-11-03 08:30:27,2
lv7wsr4,1gindy1,"Tbh I’ve not tried with any other board, can’t say. But shouldnt be an issue in general, even if you dont achieve 6200 you can lower the frequency and get a stable 5600-5800 no doubt",2024-11-03 10:37:16,1
lv93n42,1gindy1,Check the link with my bios version and settings. I didnt upgrade bios since this version as everything works stable for me,2024-11-03 14:16:36,1
lv93g3e,1gindy1,"But it actually has 4 channels. It has the ability to address each half of the ram seperately. And it's a 4 channel controller, for 4x 32Bit.

It's just that that's the same bandwidth as 2x 64 bit.",2024-11-03 14:15:33,2
lv8e0zs,1gindy1,"Oh, thought someone mentioned 4 channel and was hoping a consumer cpu was supporting that now.",2024-11-03 12:04:04,1
lv7wrfn,1gindy1,"Ok 

Thanks",2024-11-03 10:37:06,2
lv7x37k,1gindy1,There's a lot of people that can't get above 4800 with 4 DIMMs on cheaper boards.   (I wish more hardware sites tested higher speed 4 DIMM stability.. ).  Appreciate the data point for your board.,2024-11-03 10:38:41,2
lva19be,1gindy1,Yes I agree you are technically correct with DDR5.,2024-11-03 17:22:32,1
lv7xe0b,1gindy1,"I was busy 24/7 for weeks to find stable configs, started with 5400 then pushed up. These settings were discussed on the overclock .net forum from a username named Lostking, you should check that forum out

edit: also your ram makes HUGE different not board per se",2024-11-03 10:40:07,1
lv70rtj,1gin6jd,"You're comparing possibly the best commercial model with the smallest open-source model so it's not a surprise you're getting subpar results.

You should try at least a Codestral 22B model or ideally something in the 70B range.",2024-11-03 07:57:30,5
lv6pjqk,1gin6jd,"Have you considered an agentic workflow, breaking the context up based on some criteria for multiple agents then synthesizing the result to make a final action? If each agent knows what's important to focus on it may produce better results than a single prompt handling everything.",2024-11-03 06:58:23,2
lv6fjan,1gin2y0,"read the motherboard manual.  what type of CPU do you have?  might not have enough PCI lanes.  I had such an issue with a hp z820.  It has 6 slots, 5 full x16 slots and yet the most I could get at the same time was 3.   The moment I added the 4th, the system refused to come on.  Tried all possible orders, etc.   So it might just be your hardware and not that you're doing anything wrong.",2024-11-03 06:00:20,6
lv6hxr8,1gin2y0,"I have 4x4090 rig. im not sure what is the cause of issue. one thing I recommend is checking riser cables. try to have equal lengths for all gpu, preventing timing issues.",2024-11-03 06:14:51,3
lv6ss3e,1gin2y0,"this smells like a pcie timing problem

boot with 2 or 3 GPUs and run ""nvidia-smi dmon -s et"" in one terminal, then start a GPU intensive process in another.. do you see the pcie error counters going up?",2024-11-03 07:15:59,3
lv7j6h1,1gin2y0,"I am not sure if daisy chaining PSUs is a good idea, but I guess it depends on what you mean by it. I personally use Add2PSU board to synchronize both PSUs and to ensure they have the common ground. I had stability issues before I got 2880W IBM power supply, in addition to the 1050W main one. I use 2880W to power all four GPUs and the main PSU for everything else. I can run without issues with power limit set to 390W on each card and pushing them to the full load, all connected via PCI-E risers (three PCI-E 4.0 30cm risers I got for less than $30 each, and one PCI-E 3.0 riser).

In your case, you may try connecting 3 cards to 2000W PSU and 1 card to the 1500W PSU, having Add2PSU board is may good idea to ensure they turn on or off at the same time. If still not stable, maybe try two cards on each PSU, just in case one of them is not up to spec in terms of power it can provide. If this does not help, maybe the issue is elsewhere. Then would suggest adding cards one by one and see when you start having issues again. This help to pinpoint the problem.",2024-11-03 09:30:18,2
lv8ic7n,1gin2y0,Did you try disabling audio/usb etc devices in BIOS? Solved my problem when moving from 3 to 4 cards.,2024-11-03 12:26:04,2
lv8vb3g,1gin2y0,"Well, Threadripper is a big chip. If nothing helps - it is worth to check if it sits in the socket properly, if there are damaged slot pins etc.",2024-11-03 13:32:25,1
lv6hwbq,1gin2y0,"> Threadripper 3970X

Supposedly it's got 88 lanes. System is using 48 for the GPUs alone. 

Should test 2xGPU stability with the riser, I guess, and with all slots to eliminate those.

>Each of them has idle power (whether 4 or 2 are loaded) at 100W-120W level. 

Weird.

>PCIe set to 3.0 (won't boot otherwise with PCIe resources error BIOS boot code)

sounds related.",2024-11-03 06:14:37,3
lv6j0qu,1gin2y0,"[https://www.tomshardware.com/reviews/amd-threadripper-3970x-review](https://www.tomshardware.com/reviews/amd-threadripper-3970x-review)

Threadripper 3970X has 64 PCIe lanes, motherboard claims it exposes 16+8+16+8 (PCIe slots) + 4 (NVMe disk) + motherboard are connected directly to CPU. Should be pretty enough.  
Also the fact that it sometimes boots with all 4 and then vanishes says the problem is different. When all GPUs were working they correctly reported their PCIe slots number (x16,x8,x16,x8).  
Checked this 1000 times. Thanks for your comment anyway.",2024-11-03 06:21:17,1
lv6k7kt,1gin2y0,"That's a good point. I have 3 GPUs plugged in and then 1 GPU is on riser cable.

BTW when I read various \`lspci\` output, I believe it mentioned this motherboard has retimers.",2024-11-03 06:28:20,1
lv71o2k,1gin2y0,"Thanks, will update when will be physically at PC.",2024-11-03 08:02:10,2
lv6jec8,1gin2y0,"Before trying to connect all 4 GPUs, I tried 2GPUs (no riser), 1GPU+1GPU on riser, 2GPU + 1GPU on riser. Everything worked. Only 3GPUs+1GPU on riser - first it worked like 10 hours, now it is getting worse.",2024-11-03 06:23:30,1
lv6jk9u,1gin2y0,But if you mean I should have tried to plug riser everywhere - I think this may not work because mobo may expect slots to be filled in an order. But I will try.,2024-11-03 06:24:31,1
lv6qccv,1gin2y0,"Maybe it's not related to your issue, but I had a problem building my 4x4090 with riser cables. I bought cables spec'd as PCIe 4x16 but never succeeded in booting; I had to adjust to PCIe gen 3 mode in the BIOS. Later, I bought PCIe5gen cables from LinkUp and finally got it booting with PCIe4gen.",2024-11-03 07:02:45,3
lv6n1rw,1gin2y0,Do 3 gpu work at pcie 4.0? You shouldn't be turning down the speed.,2024-11-03 06:44:32,1
lv6mwzg,1gin2y0,No mobo I have is like this. There are some that expect GPU in a particular slot but only due to there being one proper x16 or there being bifurcation.,2024-11-03 06:43:47,1
lv6rw25,1gin2y0,"To echo this, if you want working pcie3 you need to buy pcie4 cables which is a lesson I also learned the hard way

These extension cable guys are lying sacks of crap",2024-11-03 07:11:10,1
lv6nzeu,1gin2y0,"No, they work at PCIe 3.0 only, at least I could only manage PCIe 3.0.",2024-11-03 06:49:45,1
lv6of3c,1gin2y0,"3090s should work on pcie 4.0, especially with no risers in the mix.",2024-11-03 06:52:11,2
lv6jt3h,1gim74q,"Have you tried any? Many models in the 3-8B range can parse that and produce a structured output, too.",2024-11-03 06:25:58,1
lv79wcg,1gijddh,"I started a project which used a database of logical syllogisms to inform LLM reasoning via RAG, but without a reasonably complete syllogism dataset it couldn't do much, so that got me started on synthetic datasets.

While researching better ways to synthesize syllogisms, I stumbled across Evol-Instruct and have been preoccupied with that since, leaving my syllogisms project neglected.

I'll get back to it, but I'm not sure when.

Aside from that, I think there would be value in using a formal prover in Guided Generation, so that inferences which could be proven false could be avoided, similar to how llama.cpp's grammar Guided Generation feature avoids inferences which do not comply to the given grammar.

I haven't actually worked on that, though, other than eyeballing the code llama.cpp uses for grammars.  What I'd really like to write is a general-purpose plugin system for llama.cpp Guided Generation, so that inference could be guided by arbitrary external logic (which could be GOFAI).",2024-11-03 08:44:18,3
lv7dvna,1gijddh,"This is kinda similar to what I'm trying to get at, except that I'm trying to create a database of beliefs in a specific domain (beliefs = a set of propositional clauses that are logically consistent with each other), that inform LLM reasoning via RAG. I want to make the belief set itself evolve over time by adding in new beliefs from external sources (chunking and propositional decomposition of PDFs etc) and revising the belief set.

Explained the ""belief addition/revision"" part here: https://www.reddit.com/r/MachineLearning/comments/1gil6vg/d_formal_logic_and_set_theory_in_belief_sets/lv74sfv/",2024-11-03 09:04:08,3
lv7etwx,1gijddh,"I like your idea of using syllogisms. Syllogisms = a propositional belief clause that acts as the generalization + a test case + a conclusion, so yeah I get what you mean with needing a lot of synthetic data. Was your implementation domain-specific or a general chatbot?",2024-11-03 09:08:49,1
lv7i26k,1gijddh,"That seems like a good approach!  Using inference to detect contradictions in beliefs should take you far.  If you have a feedback system humans can use to flag hallucinations, too, then that could be used to mark the beliefs informing that inference for review.

Very intriguing :-) over time and use, your belief database should grow more complete and more correct, especially if a lot of people use it and provide feedback.",2024-11-03 09:24:44,3
lv7gx69,1gijddh,"My goal is to make it as general as possible, but it never got past the POC stage, because I had only amassed a few thousand syllogisms, mostly about politics and society.

One of the problems I ran into was that the models I was working with at the time had large gaps in their understanding of topics.  Asked to infer syllogisms about society or political theory they would churn out thousands of high-quality syllogisms, but asked about medicine, war, or nuclear physics they would mostly hallucinate.

That got me onto the task of mapping out which topics different models could infer about competently, so that better training data could be compiled to cover the gaps in their competence.   The problem of converting relatively scant source material into larger datasets led me to Evol-Instruct.",2024-11-03 09:19:11,2
lv7knn6,1gijddh,"yeah right now, I'm having better luck with beliefs that are simpler ontological statements than anything else... When it comes to belief statements that have modal operators though (anything with could/should/might/must) things get a lot muddier. Anything with deontic modalities like ""must"" and ""should"" are the hardest, because these statements are prescriptive in nature.",2024-11-03 09:37:40,2
lv7qe43,1gijddh,"to add: I think the issue with statements that have modal operators has to do with the fact that to make any prescription, there has to be a dataset of consequentialist, deontological or virtue-based arguments in favour of or against something - - ie a RAG dataset for ethics, just like propositions or syllogisms can constitute a RAG dataset for ontology.",2024-11-03 10:05:47,2
lv7uv8z,1gijddh,"Yes, but that gets beyond reasoning, and into values, which are what guide reasoning.

I'm happy enough simply improving inference reasoning, and letting the human user provide the values directing the inference.

To give the system useful values, I think you need to look at embodiment, so that rewards and punishments as a consequence of outcomes can be simulated.  Then bodily experiences of those outcomes (even if only virtual) can provide the ontological foundation for more complex ideas through metaphor, per George Lakoff's cognitive theories.

I've long believed that to be the path to AGI, but it would require new systems to be developed for embodiment and abstraction, for which inference would be a peripheral function, rather than central.",2024-11-03 10:27:54,2
lv66q41,1giicjl,Mistral Large is my go to. It's European so it's uncensored and not afraid of booba or blood etc.,2024-11-03 05:02:16,6
lv74s4r,1giicjl,Mistral Large 2 is pretty uncensored if you explicitly tell it to be,2024-11-03 08:18:24,5
lv5k4fc,1giicjl,https://github.com/NVIDIA/RULER,2024-11-03 01:30:02,12
lv5knty,1giicjl,"Hermes 3 405B, you might have issues getting it to run on your hardware though.",2024-11-03 01:36:23,6
lv5yul6,1giicjl,"The only ones sort of passable around 64K-80K are the Cohere models, EVA Qwen (at least 32B), InternLM *base* 20B, and the old Yi 200K models. Qwen 2.5 *instruct* is mediocre at 64K, and thats only if Yarn is working.

I am sticking with EVA Qwen right now.

Not sure about anything 70B+ other than Gemini, I don't try API models a ton.

Anything Mistral has been unusable at long context for me, Mistral 22B is bad past even like 24K. Many instruct finetunes also destroy a model's long context ability, so you have to be careful.",2024-11-03 04:01:21,3
lv65a3d,1giicjl,Mistral Large,2024-11-03 04:51:49,3
lv5s0m7,1giicjl,[Sao10K (Sao)](https://huggingface.co/Sao10K) this has a list of models for role play models check out its good,2024-11-03 02:58:02,2
lv5h1wc,1giicjl,Qwen 2.5 is pretty good.,2024-11-03 01:53:53,2
lv6s28a,1giicjl,"glm-4-9b-chat-1m :


https://huggingface.co/THUDM/glm-4-9b-chat-1m/blob/main/README_en.md",2024-11-03 07:12:05,1
lv84nzv,1giicjl,how do these models process this very very long context?,2024-11-03 11:16:19,1
lv5kyi8,1giicjl,"yikes, thanks for sharing that! So some of the issues I'm having are due to the model's effective context length not matching the advertised context length, and not just the model being a little dumb...",2024-11-03 01:39:51,7
lv65d27,1giicjl,"> Mistral 

Tried the 123b one?",2024-11-03 04:52:25,1
lv5s72r,1giicjl,"A\*\*\*\* chat and roleplay, nsfw erotica writer u  name it shit gets done",2024-11-03 02:59:49,2
lv5km9k,1giicjl,Qwen 2.5 is not uncensored.,2024-11-03 01:35:52,3
lv8aers,1giicjl,"Is it better than regular GLM4 at 128K?

You'd think it would be, but I haven't tried 1M myself yet.",2024-11-03 11:45:43,1
lv8anlx,1giicjl,"With quantized cache in exllama/llama.cpp, or with copious vram in enterprise backends like vllm, lol.

Its not always as bad as you'd think though.",2024-11-03 11:46:58,2
lv6cyja,1giicjl,"Maybe try STRING -> [https://github.com/HKUNLP/STRING](https://github.com/HKUNLP/STRING)

In their paper it looks like that the 128k context for the open models they tested did not work well above 32k [https://arxiv.org/html/2410.18745v1](https://arxiv.org/html/2410.18745v1)

They claim to improve that.",2024-11-03 05:44:15,7
lv6l3nw,1giicjl,"TBH not much, mostly because I can just use Gemini with a refusal workaround.

Its ruler score is not promising though, doesn't look like its even worth it over a 20B-35B.

Megabeam Mistral is an exception though, that one is actually outstanding... for a 7B.",2024-11-03 06:33:29,1
lv5lapq,1giicjl,"Any particular example of 32B or 72B being censored? Writes alright smut if you ask me. And if it's censored, what uncensored models you have to recommend?",2024-11-03 01:43:47,0
lv5px4o,1giicjl,"EVA Qwen2.5 effectively solves that, while retaining much of the smartness.",2024-11-03 02:35:49,1
lv6m03e,1giicjl,"Oh interesting, yet another amazing repo that seems to have flown by.",2024-11-03 06:38:36,1
lv6lxg4,1giicjl,"What's the refusal workaround? I rarely hit refusals on gemini but when I do, it's always for the silliest thing.",2024-11-03 06:38:10,2
lv5n0ht,1giicjl,"A whole lot.

\- Coding - Refusals on coding questions involving code security, especially in the context of hacking.  
\- P2P - Thinks torrents are illegal and you are a criminal.  
\- NSFW - All adult-oriented topics are forbidden, and I don't mean just sex topics, but any form of NSFW, including jokes.  
\- Historical revisionism - China did nothing wrong, ever. No such issues when you ask about the history of the US or any other country, though.  
\- Propaganda - Won't answer queries that paint the CCP in the bad light, instead it will tell you how wrong you are.  
\- Judgemental - The model may sometimes tell you to touch grass, and frequently will add a footnote with its own 'thoughts' on the subject, even when asked not to.  
\-  Censored System Prompting - It does not follow system prompts if it includes certain phrases. (For example, try to make it roleplay as the protester from Tiananmen Square ;) )

That's just some examples, but you get the picture. As for what other models I'd recommend:

8B - Ministral  
12B - Mistral Nemo  
22B - Mistral Small  
22B - Codestral  
123B - Mistral Large

All Mistral models are uncensored by default, and I'd recommend them over fine-tunes of other architectures, but if you prefer models like Llama or Gemma, then check the UGI leader board that measures uncensored model intelligence.

[https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)",2024-11-03 02:03:27,11
lv5q9q8,1giicjl,"Oh nice! I was waiting for the GGUFs to give it a shot. Looks like Bartowski released them in the last 24H.

[https://huggingface.co/bartowski/EVA-Qwen2.5-32B-v0.1-GGUF](https://huggingface.co/bartowski/EVA-Qwen2.5-32B-v0.1-GGUF)",2024-11-03 02:39:38,1
lv6mm5i,1giicjl,"Depends. One strategy is to add a ""buffer"" question.

So like:

Human: blah

Bot: refusal

Then you format the chat as:

Human: blah

Bot: Interesting! Are you sure you want me to answer that!?

Human: Yes.

Bot: Answer

That's a primitive example, but you get the idea.",2024-11-03 06:42:04,2
lv5otry,1giicjl,"I have compared codestral with qwen coder 7b for tasks like porting python scripts to rust. The structure of output from qwen was slightly better than codestral. But in general, I agree that Mistral models are good, Mistral large specifically. Can't run it, though.

I'll check the qwen model I run locally against the arguments you made to see how censored it is. As I told before, writes nswf stuff pretty alright. Thanks for your response.",2024-11-03 02:23:43,3
lv5rysa,1giib22,"There's no such thing as a wow factor in a RAG, not for anyone whose been building applications for any number of years. 

You just need to show a better user interaction.. finding what you want faster is the demo. 10% improvement is a big jump if your current solution is mature.",2024-11-03 02:57:30,5
lv5k493,1giib22,"Does it have to be local? You could use Azure OpenAI Playground to try different prompts and to reference your own data in AI Search, Cosmos DB or Blob Storage.

If it's local, you could stuff some data into Postgres or Faiss, run some vector similarity searches using an embedding model, maybe use a reranker model, then feed that RAG data into a local LLM. For me, the document chunking step can make or break your RAG solution: it needs to be large enough to keep surrounding context. 

Here's a good look at RAG solutions: https://www.falkordb.com/blog/advanced-rag/",2024-11-03 01:29:58,2
lv5ep53,1giib22,"Note: original text re-written by qwen, because I ramble a lot.",2024-11-03 01:25:42,1
lv5ix2i,1gii24g,Qwen2.5  0.5B is insane for its size,2024-11-03 01:15:49,44
lv5j57x,1gii24g,"Phi models be like: ""Pretraining on the Test Set Is All You Need"" 


I know they are ""corporate models"" and I never expect them to be creative & fun, but they struggle with even simple tasks such as translations, often adding unwanted commentary alongside the actual translation.  


Qwen2.5 and llama3.2 is much better than phi models. ",2024-11-03 01:18:27,85
lv5wnbm,1gii24g,"I never understood how phi models are scored that high, while anything I've tried was pure dog shit.",2024-11-03 03:42:37,16
lv5hmt6,1gii24g,"Thank God we have alternatives to Phi models, most depressing set of models, and this is coming from someone that was rooting for them, we could have had vision long before Qwen.",2024-11-03 01:00:39,37
lv5nntt,1gii24g,I don't think MMLU is a good benchmark for comparing small models... it primarily measures knowledge/memorization and for tasks that require that you shouldn't be using a small model in the first place. The model scoring 0.1 and the one scoring 0.4 are both equally useless at Chemistry.,2024-11-03 02:10:41,23
lv66bss,1gii24g,"MMLU-PRO is not useful for this, it enforces a strict answering format which affects small LLMs much more than large ones. They might be able to answer the question in a normal chat scenario with a human but might not be able to enforce the strict format at the same time.",2024-11-03 04:59:27,10
lv5lrm7,1gii24g,Huh... Didn't realize ministral is this bad on the benchmarks. It seemed pretty good for the size when I tried it.,2024-11-03 01:49:18,14
lv5olwa,1gii24g,"Phi 3.5 mini is benchmaxxed, that thing is largely useless.",2024-11-03 02:21:15,21
lv5ovkb,1gii24g,"Ministral 3B performed really well for me during my testing, better than the other models listed here. Seeing it dead last behind even the 135M model is surprising to say the least, and not at all accurate for my use cases.

Gemma 2 2B also did better than the Phi3 models in my testing.

You do you, but to me this screen doesn't capture the capability of these models whatsoever.",2024-11-03 02:24:17,10
lv5tkwg,1gii24g,Where is Qwen2.5-3B-Instruct?,2024-11-03 03:13:49,3
lv7oczt,1gii24g,Is that Gemma *2* 2b or Gemma *1* 2b?,2024-11-03 09:56:05,3
lv5syzw,1gii24g,I'd like to see them train on only specific things and see how high a score they can get vs trying to make a tiny model bad at everything.,2024-11-03 03:07:35,2
lv66k52,1gii24g,I tested phi 3 early on and I recall it wasn't up to par. I surprised it outperformed qwen. I must have has some early corrupt gguf,2024-11-03 05:01:06,2
lv5paxd,1gii24g,"Interesting chart! Does anyone know why the instruct versions of Qwen 2.5 and Llama 3.2 are missing? The Granite model is also only the base model. I'm trying to figure out why the base models score so much higher (or why so few Instruct versions are listed).


I agree that Phi was benchmaxxed, such a useless series of models.",2024-11-03 02:29:04,3
lv7lwuk,1gii24g,"Bummer, was hoping the SmolLM2 family would have scored higher",2024-11-03 09:43:58,1
lv84d4a,1gii24g,thank you,2024-11-03 11:14:46,1
lv6ggxk,1gii24g,Where's Danube 3 500m?,2024-11-03 06:06:01,0
lv6u2ae,1gii24g,#Phi3MasterRaceForLyfe,2024-11-03 07:22:58,-2
lv5nk6i,1gii24g,"Yes, and 1.5B and 3B are also really competitive! It’s great to have so much sizes to choose from.",2024-11-03 02:09:34,19
lv77gke,1gii24g,[https://arxiv.org/abs/2309.08632](https://arxiv.org/abs/2309.08632),2024-11-03 08:32:16,5
lv65sdd,1gii24g,"Yeah, at this size I would assume knowledge is coming at the expense of reasoning. (Anecdotally validated by someone below saying they felt Ministral-3b was their favorite of the models) You just can't fit very much inside models this small.",2024-11-03 04:55:31,9
lv5txj6,1gii24g,"exactly... models this small should be used with data input and maximize reasoning at the expense of knowledge, not the opposite.",2024-11-03 03:17:14,9
lv5qloy,1gii24g,"So glad to hear somebody else say that, I tried it for like twenty seconds, and it was very obviously hot garbage",2024-11-03 02:43:09,5
lv5r0wm,1gii24g,"Please elaborate. Ive heard claims like this a lot, I have a vague idea of what it means but dont fully get it.

Do you mean it was trained specifically/exhaustively on benchmark tests and thusly performs poorly outside them?",2024-11-03 02:47:34,4
lv8ndzn,1gii24g,Yep. That thing wrongly hallucinates way too much.,2024-11-03 12:51:25,1
lv5spit,1gii24g,"Well we could recheck if they did it correctly, let me just set it up locally.... oh wait.

Already deserves last place for being API only tbh.",2024-11-03 03:04:55,8
lv77xxz,1gii24g,"Nope, Phi models are just shit. Trained to be good on benchmarks but actual performance isn't good.",2024-11-03 08:34:42,3
lv8g69b,1gii24g,"I don't see why ppl are dogging so hard on it, I've been using it and seems impressive to me. Maybe ppl haven't used it lately?",2024-11-03 12:15:06,3
lv7pz38,1gii24g,PSA: Don't be afraid to click because it is a link to an academic paper. This one is very understandable.,2024-11-03 10:03:46,3
lv5uith,1gii24g,"For reasoning, which benchmark is the best?",2024-11-03 03:23:14,8
lv63sy0,1gii24g,"It was effectively trained to be good at benchmarks, but that's about it. It's intelligence is very surface level, and it's nowhere near as logical as it's benchmark scores suggest.",2024-11-03 04:40:48,8
lv5rof9,1gii24g,"Yes and the vision model is ""so safe"", it makes Google's models look dangerous. 

Earlier in the year it looked like we weren't going to get good VLMs,sure we had things like LLAVA but lucky, Q3 brought a number of better models.",2024-11-03 02:54:31,5
lv8nmn9,1gii24g,I have seen it hallucinate way too much and can drive away from topic very easily. It's mostly because it wants to answer questions broadly as it was probably trained to put out as much information as possible.,2024-11-03 12:52:38,1
lv7c38y,1gii24g,"Then this leaderboard is shit.


There needs to be a different dataset to test with",2024-11-03 08:55:12,1
lv6k1lw,1gii24g,Zebra logic,2024-11-03 06:27:22,3
lv9mc6y,1gii24g,"In the SmolLM2 announcement tweet they mention Hellaswag, CommonsenseQA, WinoGrande, and PIQA.

https://x.com/LoubnaBenAllal1/status/1852055582494294414

I'm more of a multimodal guy so I haven't dug into them too deeply.",2024-11-03 15:56:07,2
lv63unv,1gii24g,Either llama 3b or qwen 3b,2024-11-03 04:41:10,0
lv63vdg,1gii24g,MMLU-Pro.,2024-11-03 04:41:19,-1
lv64cfa,1gii24g,Then why did radiiquark complain MMLU-Pro tests knowledge instead of reasoning in the first place?,2024-11-03 04:44:51,7
lv5eouq,1gihnet,"It's still there, supported in MLX so us Mac folks can run it locally. Llama.cpp seems to be allergic to vision models.",2024-11-03 01:25:37,78
lv5e1jb,1gihnet,"Is big ... And we don't have an implementation for llamaccp as allowing us to use a vram and a ram as extension because of lack vram 

So other projects can't use it as they are derived from llamacpp.",2024-11-03 01:17:58,33
lv5kh3k,1gihnet,"https://embeddedllm.com/blog/see-the-power-of-llama-32-vision-on-amd-mi300x
Check this out. They run Llama 3.2 90b on AMD GPU. The result look impressive.",2024-11-03 01:34:12,12
lv79ohk,1gihnet,"My own experience with it was pretty bad, they attempted to bake in way too much censorship in it. It failed even basic tests some YouTubers thrown at it specifically due to degradation caused by overcensoring: [https://www.youtube.com/watch?v=lzDPQAjItOo](https://www.youtube.com/watch?v=lzDPQAjItOo) .

For vision tasks, Qwen2-VL 72B is better in my experience, it does not suffer from overcensoring (so far, it never refused my requests, while Llama 90B does it quite often, even for basic general questions). I can run Qwen2-VL locally using [https://github.com/matatonic/openedai-vision](https://github.com/matatonic/openedai-vision) . It is not as VRAM efficient as TabbyAPI, so requires four 24GB GPUs to run the 72B model and even that feels like a tight fit, so have to keep the context length small (around 16K). And it still not as good as text-only Qwen2.5 or Llama3.1, and loading vision model takes few minutes, then few more minutes to get a reply, and again few more minutes load back normal text model, so currently large vision models are not very practical.

My guess, for heavy vision models to become more popular, they need to become more widely supported by popular backends such as Llama.cpp or ExllamaV2, but there are a lot of challenges to implement vision model support. When their support is implemented in efficient backends, they may become less VRAM hungry and may gain better performance, and when we have good vision models that also remain great at text-only tasks, it may become more practical to use them. Eventually, text-only model may become even less popular than multi-modal ones, but it may take a while.

I still use vision models quite often, but I understand why they are currently not very popular due to issues mentioned above.",2024-11-03 08:43:13,11
lv6gj41,1gihnet,"It's inconvenient to run. You have to use AWQ, bitsnbytes, etc.",2024-11-03 06:06:23,4
lv5zbl2,1gihnet,It is on lmsys arena.,2024-11-03 04:05:13,2
lva5wdv,1gihnet,not a good enough model ;),2024-11-03 17:50:43,1
lv5uoz9,1gihnet,LMStudio supports vision can it run the 90b ?,2024-11-03 03:24:56,1
lv6t62n,1gihnet,It is too large for normal users to run,2024-11-03 07:18:05,1
lv5idxk,1gihnet,"Because most people don't need or care about vision models. I'd prefer a very smart, text only LLM to a multi modal AI with inflated size any day",2024-11-03 01:09:33,-15
lv5q302,1gihnet,"Ollama has llama3.2 support in pre-release 0.4.0 version, currently only for 11b size, but I believe they'll add 90b after full release. So I think in the few following weeks there will be a no-effort solution to host llama3.2:90b locally and then it'll get much more attention.",2024-11-03 02:37:36,14
lv5r04g,1gihnet,"They are doing quite a lot of job already. If anyone, take you, for example, is willing to add support for vision models in llama.cpp, that is good. Go ahead!

That is not that they don't like it. It is open project and there was no one with good skills to contribute.",2024-11-03 02:47:20,18
lv5f1lw,1gihnet,people are sleeping on apple,2024-11-03 01:29:51,-7
lv5s30h,1gihnet,[deleted],2024-11-03 02:58:42,-4
lv5r9hy,1gihnet,That receipt video is great!,2024-11-03 02:50:05,1
lv8dpuz,1gihnet,"> For vision tasks, Qwen2-VL 72B is better in my experience, it does not suffer from overcensoring (so far, it never refused my requests, while Llama 90B does it quite often, even for basic general questions).

The irony. Since the haters always complain about the CCP censorship.",2024-11-03 12:02:29,2
lv7xirx,1gihnet,"Not yet, as llama.cpp doesn't support vision llama architecture. Even on Macs, while MLX now supports Llama vision, the backend used by LMStudio doesn't (but it does support Qwen).",2024-11-03 10:40:46,6
lv5kr6j,1gihnet,"It really depends on the kind of interaction you are looking for. 

For me when I am trying to get some Python matplotlib done, a vision model makes life much easier sometimes.",2024-11-03 01:37:30,5
lv5kx23,1gihnet,"I don't get the vision models. Are they not just a text model who have had a vision model surgically stitched to it's head?
Everyone of those multimodal models I tested where awful when compared to just running a LLM + Stable Diffusion API.",2024-11-03 01:39:23,-6
lv5ytax,1gihnet,"If I had time to learn the steps required to do so, I would definitely do it.",2024-11-03 04:01:04,3
lv937zl,1gihnet,"Afaik there were contributions for vision models, but they were not merged.",2024-11-03 14:14:21,1
lv7i8zd,1gihnet,"ggml.ai is a company with a product, let's not go all Stallman on each other because they don't want to support multi-modal",2024-11-03 09:25:39,1
lv5hay4,1gihnet,"I use Qwen2-VL-7B on Mac.   I also used it with Nvidia GPU +  pytorch.  I took me a few hours to install all the library due to incompatibility of certain libraries that would uninstall the previously installed libraries. They have to be installed in a certain order.   It still gives warning of incompatibility, but it didn't kicked out other libraries.  Then, it runs totally fine.  But when Mac mlx version showed up, it was super easy to install it on LM-studio 0.3.5.",2024-11-03 01:56:51,5
lv5kqlr,1gihnet,Prob because not every one has a few thousand to spend on Mac lol.,2024-11-03 01:37:18,2
lv5thwu,1gihnet,"Nah

Vision models work the same way as text modes The difference is only extra vision encoder .. that's it.

Vision models that are working currently on llamacpp which the biggest is llava 1.6 32b works as fast as text only the same size.",2024-11-03 03:12:58,8
lva2cqt,1gihnet,"There's surely a difference between censorship and potentially harmful information. Tiananmen square != How do I make a pipe bomb. 

Now, not to get political but I can't think of another example, the hunter Biden laptop on the other hand can probably go either way so it is definitely a challenge to avoid censorship while preventing harmful information.",2024-11-03 17:29:14,1
lv93wso,1gihnet,"The qwen models itself are quite uncensored, but when you use them online, their online service disconnects as soon as you ask something about Tiananmen Square or similar sensitive topic",2024-11-03 14:18:00,1
lv5sk9d,1gihnet,"The vision stuff is for it to see things, not produce images like SD does. 

Having said that, I don't have much of a use-case for it either, but it's a baby-step in the direction of... something, for sure.",2024-11-03 03:03:28,5
lv62am6,1gihnet,"That is the point. No one is ""allergic"" to the vision models. It is just adding function into software undev active development would require someone with necessary skills and time to kill on keeping up with the rest of llama.cpp.",2024-11-03 04:29:08,23
lv60l99,1gihnet,Lol,2024-11-03 04:15:34,1
lv95inr,1gihnet,"I would presume that way. That shoud be real problem to have a code that will follow guideline of the project, work efficient, don't conflict with existing and WIP functions. By now, codebase of llama.cpp should be quite big. Also, real geniuses are not always good, as they might outperform with the code that other could not work with.

It doesn't have to be someone who will do everything just perfect on the first shot. Probably they will take someone who have skills and intention to work on the project for at least some time, to establish some work routines (in what order new features added and how to test them) and create some documentation so more people could be found on the same project.

I make it sound hard, but I am really 'afraid' that this project is quite complicated by now. That will be fantastic if guidelines would be made to make an AI to work on conflicts and checkups of the projects, so more functions can be added without dragging development time down.",2024-11-03 14:26:08,1
lv5m1nf,1gihnet,"how does it perform, and have you done ocr with it?",2024-11-03 01:52:29,1
lv5tqg6,1gihnet,[deleted],2024-11-03 03:15:18,-1
lv5t6bd,1gihnet,"Ohh. Right, yeah I was confused when I tried one too. Still apparently am cuz your right.
A vision model stitched to it in that cause.
Tried doing llama3.2 vision+Stable Diffusion and it did not work very well heh...",2024-11-03 03:09:41,1
lv6n5eh,1gihnet,"None of these vision models are good at pure ocr, what qwen2-vl excels at is doc-qa and json structured output.",2024-11-03 06:45:06,3
lv6kert,1gihnet,The model performed very well.  I input a screen of math formula in a scientific paper and asked vllm to write Python code for it.,2024-11-03 06:29:30,4
lv5uc2d,1gihnet,"As I said and tested by myself.
I don't see a difference in performance.
Vision 30b is as fast as a text 30b model.

As far as I know, you just adding a vision encoder to the text model is becoming a vision model.... I know how crazy it sounds but it is true...magic.",2024-11-03 03:21:17,1
lv545bo,1gigfxn,"Renters/home owners insurance covers (usually) everything you listed. Theft, fire, flooding etc.",2024-11-02 23:24:53,11
lv7vi3q,1gigfxn,"Read your home/renters policy. See if there are any exclusions on items up to $xx,xxx value. If there are, get a “rider” specifically for your gear. They’ll have you submit proof of value and some photos and it’ll add $xx to your policy per month

Also, good time to catalogue all of your belongings in general. Make a spreadsheet of everything with photos. ",2024-11-03 10:30:58,3
lv58bcu,1gigfxn,Renter's insurance,2024-11-03 00:11:47,2
lv548mg,1gigfxn,some credit cards come with one,2024-11-02 23:25:53,1
lv8demd,1gigfxn,"Talk to an insurance broker. They will take the time to understand your needs. They will steer you into a commission paying product, but at least they are steering you with more insight than you have yourself.

It is important to know that you don’t actually get a discount when buying insurance direct, because of structure of the insurance market. You’re missing out on a lot of value when you DIY insurance with a state-admitted insurer.

And yes, the prototype homeowner policy doesn’t cover your $75,000 computer. (Which most people don’t even believe exists)",2024-11-03 12:00:55,1
lv6ft9e,1gigfxn,Why?  The models are not that good yet.   Build a budget build and use the cloud and save your money.,2024-11-03 06:02:00,-2
lv6wucp,1gigfxn,"And the insurance is reasonably affordable, even for renters. Take pictures of your rig as well as your tv, audio and other high cost items. And keep invoices and receipts. Put it on Google drive or similar. If you ever come home to a pile of charred electronics you'll thank yourself.",2024-11-03 07:37:33,1
lv6v1nh,1gigfxn,"On top of this advice, there's speculation that data centers are over providing capacity and there might be a bubble that will burst i.e. you might be able to get really cheap cloud compute in a year

https://www.latent.space/p/gpu-bubble",2024-11-03 07:28:18,2
lv9xe2e,1gigfxn,"You're right, but for the wrong reasons.

Microsoft is [restarting the nuclear reactor on three Mile Island](https://www.reuters.com/markets/deals/constellation-inks-power-supply-deal-with-microsoft-2024-09-20/) because in 2028 they will need the power for so many more GPUs.

There is no bubble here, LLMs and higher level constructs built on top of them are economically useful and will continue to grow.

The actual reason prices will continue to drop is 1) competition between major providers, 2) investor dollars subsidizing compute and 3) quantization and other cost reducing approaches are improving really quick.",2024-11-03 16:59:12,1
lv4bger,1gic7v1,"It is interesting research, but it feels not complete without comparison with similar EXL2 quant loaded in TabbyAPI with speculative decoding. For Qwen2.5 larger models, [https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4/tree/main](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4/tree/main) works well as a draft model.

Last time I checked (it was a while ago though), ollama and llama.cpp performance wasn't great, and when there is sufficient VRAM to fully load the model, using TabbyAPI with EXL2 quant was noticeably more efficient. Support for Q6 cache that is about as good as Q8 cache but uses less VRAM, also a great feature to have when there is a need in a longer context.",2024-11-02 19:30:18,14
lv49n6v,1gic7v1,"Neat, any plan to publish a repo/script so people can tune their own cards?",2024-11-02 19:18:37,4
lv5f8sc,1gic7v1,"What is the software you use? Llama-cpp? Shouldn't you use something like mlc-llm which actually uses tensor parallel? Isn't 10Token/second with 2 3090's on a 32b model completely unacceptable?

I get 50T/s on 32b (q4) with a 3090 + 3080Ti. Yes, it consumes 700W, but with mlc-llm in tensor parallel it can actually use both cards in full-blast, compared to llama-cpp only using one-card-at-a-time.

I have 2 AMD Instinct Mi60's and that runs 70b Q4 on 15T/s. And you're running 32B on 2 3090's at only 10T/s?!?

At that point, you should start measuring Joules consumed to finish a certain prompt, to determine efficiency, because your setup might be all the 'power efficient' it can be, but it also takes much longer to complete.",2024-11-03 01:32:13,7
lv46xol,1gic7v1,"Wow, that's pretty cool. Can you perform similar tests for different use cases? Image generation or recognition etc?",2024-11-02 19:01:20,2
lv4e4a7,1gic7v1,Now I gotta figure out how low I can drop vcore.,2024-11-02 19:47:35,2
lv4se8d,1gic7v1,"Thanks for posting initial finds, and thanks for your willingness to share the code to run these tests. I love this kind of data, and I'd be happy to run tests on my hardware, too.",2024-11-02 21:33:16,2
lv56ev7,1gic7v1,I am thinking about building something with 48gb VRAM. what kinds of models have you found work best? what size is the limit?,2024-11-02 23:50:16,2
lv5n2ri,1gic7v1,Thanks for the experiment results,2024-11-03 02:04:10,2
lv683ei,1gic7v1,"May I ask if Stable Diffusion (and image generation in general) will require a different (and more power demanding) consumption profile?

Does it also rely on memory speed mainly or it's more about GPU calculation power?",2024-11-03 05:11:51,1
lv4c5gq,1gic7v1,"that will probably come next as ill need to build out the container and get tabby in working order. I suspect it'll take another 20 hours to do exl2 but i think it will be interesting to see the results.

ill probably rerun the tests once I resolve the possible memory offset issue or discover why slower memory clocks are producing better numbers.",2024-11-02 19:34:51,6
lv689ol,1gic7v1,how did you get draft models working with tabby?,2024-11-03 05:13:03,1
lv4a1jt,1gic7v1,"There is, I need to work out a few bugs, add in a few more features and make it safer as I really don't want someone to accidentally burn out their card with an incorrect config. (will have to find out if I can auto pull gpu config information)",2024-11-02 19:21:13,1
lv5l6vg,1gic7v1,"It's ollama, so tuned llama.cpp 

The total tokens per second are calculated with input and output averaged, including time to first token (this makes it look significantly slower than it really is), but since the goal was to find the fastest and most efficient power profile I wanted to focus on the entire picture which includes from prompt processing to first token.

I haven't thought about testing mlc-llm yet. I'll add it to the list with exllama as it'll add another 20 hours to process.


Edit:
Llama.cpp (ollama) with just calculating response tokens and not taking into account prompt processing and other response times.

25.2 t/s at 252w (optimized config)
26.6 t/s at 350w (no config)

~26t/s is about normal for 2x3090s llama.cpp

I've also adjusted language in post to separate Tps from my calculation of Ttps or Total tokens per second",2024-11-03 01:42:34,6
lv47bo2,1gic7v1,"theoretically yes, it wouldn't be too difficult to modify the script to send different API requests and wait for a server response. ill have to add this to the list of Ideas!",2024-11-02 19:03:51,2
lv5l9sh,1gic7v1,"Sure! once I work out the kinks, I'll open source the project",2024-11-03 01:43:30,1
lv5k8sd,1gic7v1,"Depends on your budget entirely, I found 3090s are more friendly to the wallet then say a 4090 or rtx 6000

No size limit for the script.",2024-11-03 01:31:28,1
lv70kqh,1gic7v1,"For frontend, I use SillyTavern with [https://github.com/theroyallab/ST-tabbyAPI-loader](https://github.com/theroyallab/ST-tabbyAPI-loader) extension. I just choose the draft model to load from the dropdown list, in the same place where I choose the main model.",2024-11-03 07:56:28,1
lv8o5dp,1gic7v1,"[https://github.com/theroyallab/tabbyAPI-gradio-loader](https://github.com/theroyallab/tabbyAPI-gradio-loader)

this can load draft models",2024-11-03 12:55:19,1
lv4e3qf,1gic7v1,"Good to be careful although burning out cards in unlikely. PL's these days are drive/bios locked so you get an error if you try to exceed the maximum (nvidia-smi will simply throw an error like

    ❯ sudo nvidia-smi -i 0 -pl 500
    Provided power limit 500.00 W is not a valid power limit which should be between 100.00 W and 450.00 W for GPU 00000000:09:00.0
    Terminating early due to previous errors.",2024-11-02 19:47:29,4
lv42apj,1gibs90,How will it know how many times to press tab to get to the desired element?,2024-11-02 18:31:32,1
lv42xyz,1gibs90,"For a vision based LLM it could just observe the screen after each tab action. On my browser, a tabbed element is either highlighted or encircled in some way. That would allow it to know what element it has selected by checking a screenshot. There's also rules for the order in which elements are tab selected, technically an LLM could guess the order by looking at the html on a webpage for example. Those are two ways that come to mind immediately.",2024-11-02 18:35:41,1
lv436g7,1gibs90,"That sounds like it would require several turns, where moving the mouse to specific x,y coordinates can be done in one turn?",2024-11-02 18:37:12,2
lv444dk,1gibs90,"Not necessarily, every program has a tab order. If an LLM could be fine-tuned to count pixel and guess movement it could also be finetune to estimate tab order and the number of tabs required. So, it could still theoretically be done with a single step that guesses how many tabs are required to reach an element.",2024-11-02 18:43:15,1
lv44a5p,1gibs90,I feel like estimating tab order would be less precise,2024-11-02 18:44:16,2
lv44ic7,1gibs90,It's just an idea I'm throwing out there.,2024-11-02 18:45:42,1
lv7hfw6,1gibs90,"Deep learning for guessing tab order is an interesting idea, I wonder if there are datasets for this.  If you get good accuracy, it would later contribute to LLM training in some form or another",2024-11-03 09:21:43,1
lv3re6m,1giadgu,"Building a RAG is easy so there's not a ton of value in building a standard pipeline. Building a RAG that is 100 % accurate is hard. There are companies that exist to make programmatically handle data labeling like [Snorkel AI](https://snorkel.ai/) and there are a few that will build RAG for you as a service. But building a RAG is like 10% of the work. Cleaning data, normalization, optimization etc is where companies offer services or solutions.",2024-11-02 17:23:42,152
lv3x8wv,1giadgu,"GPTs allow for uploading files - they are indexed as RAG.
NotebookLM by Google as mentioned.
Claude’s allows adding files.

Then you have companies like Amazon with AWS Search solution with RAG as a service .

And Glean as a company that indexes all your 3rd parties.

There are plenty out there",2024-11-02 17:59:34,67
lv43yl5,1giadgu,"we cant and shouldnt rely on the big businesses to build the tools we actually need because they will usually be half-baked, overly restrictive, and usually only work with their own models.

it has to be part of an open-source tooling system",2024-11-02 18:42:13,10
lv3qbt9,1giadgu,Oddly enough Google seem to be leading the big companies on this front. And no other front. NotepadLM is worth a try.,2024-11-02 17:17:15,34
lv3veeu,1giadgu,"Are you referring to local RAG solutions? Otherwise, isn't Claude's ""Projects"" feature basically RAG?",2024-11-02 17:48:07,8
lv3szmi,1giadgu,I'm curious if anyone's tried Graph RAG. It seems much more promising than a basic query + enrich prompt with data kind of RAG.,2024-11-02 17:33:24,11
lv5mgo6,1giadgu,"I think you're partly can't see the forest for the trees. Document lookup and search is a huge topic in business. But unless you work in huge corporations with labyrinthian intranets and libraries full of call center instruction .pdf's you don't hear that much about it. But there are a lot of enterprise/open-source products, from that deal with searching, lookup and indexing.

And those are all just one trivial function/tool description away from integration into a LLM-tool-chain. Bigger ""RAG"" solutions that are directly intended to be used with AI personally have interacted with are Azure AI/Cognitive Search and llamaIndex.",2024-11-03 01:57:15,6
lv3qz2j,1giadgu,"Gemini lets you do this (kind of) in their enterprise version.

You can load some pretty large PDFs into ChatGPT when creating custom agents too and it will pull and reference information from them for its responses. I've got one to help my keep track of things in one of my TTRPG games where I dropped PDFs of entire setting manuals into it and it pulls info from them for its responses.",2024-11-02 17:21:07,4
lv3vkdl,1giadgu,"Aren't there like 10 local chat uis for this already?

Also, Azure OpenAI offers exactly this and allows you to create an ""Assistant"" based on your documents.

And if you are doing RAG, you're limited by the retrieval step not the generation step using sonnet 3.5 or 4o is an overkill.",2024-11-02 17:49:10,8
lv4p0ac,1giadgu,"There are already quite a few solutions for this on the market (e.g. [AnythingLLM](https://www.anythingllm.com)) and most of the big players are offering RAG in some configuration - NotebookLLM, Claude, Azure AI Studio, etc.",2024-11-02 21:05:53,3
lv5432n,1giadgu,We have a bunch of services around this in Azure AI - including Chat with your data. There are also a number of github repost we have showing more advanced techniques when using Azure OpenAI. I also touch on it in my new book - https://blog.desigeek.com/post/2024/10/book-release-genai-in-action. This is also now available on Amazon - https://a.co/d/gMNdcJL,2024-11-02 23:24:13,2
lv54lzi,1giadgu,https://aws.amazon.com/bedrock/knowledge-bases/,2024-11-02 23:29:58,2
lv5i4qo,1giadgu,"Idk, RAG feels like the ""magic"" is in an inferior solution doing the work these cool LLMs should be doing. Like if I ""chat with a pdf"", my intuition is that I could just directly look at what the RAG returns. Similar to how ChatGPT is barely any help in using the image generator. I mean it seems like it could be useful, but somehow they managed to not teach the guy how to write a competent image prompt anyway. Which seems like exactly the one thing the llm could be good for. But, if the LLM directly is the image generator, like it will be in the future? Whole different beast. Sort of comparable to ""chatting with a pdf"" without RAG.",2024-11-03 01:06:35,2
lv63b95,1giadgu,Amazom Bedrock does enterprise RAG,2024-11-03 04:37:02,2
lv6esxz,1giadgu,"You can do this in OpenAI Playground. It’s been available … I don’t know a “long ass” time. Not sure what the conversation is to days/months.

It’s gotten better too with Vector Stores where you can categorize data sources.",2024-11-03 05:55:50,2
lv3xa9a,1giadgu,"Unlike Microsoft, Apple is the only one actively implementing Local LLM in their operating system. It should be easy for them to add a RAG directly into their OS. ""Spotlight"" is basically an indexer, it even uses Llava to tag all your images. Spotlight can easily be combined with a LLM for RAG purposes.

Maybe next year, when all their hardware on the market has enough RAM - we will see it. They will probably called it ""**R**esourceful **A**pple **G**enius"".",2024-11-02 17:59:48,4
lv4f9nu,1giadgu,Whats a RAG and how do you do it?,2024-11-02 19:55:08,2
lv3ra27,1giadgu,Please share some of your workflow if you have time. I’ve tried the larger apps and realising that just doing it myself as a hobbyist will be the most effective way.,2024-11-02 17:22:59,1
lv3xut8,1giadgu,"ChatGPT has one, I setup my documentation with it.",2024-11-02 18:03:22,1
lv4gw66,1giadgu,Check out Mistral's new Ministral models,2024-11-02 20:06:00,1
lv4kqij,1giadgu,RAG embedding vectors are model specific so when new models come out you need reprocess your whole dataset to take advantage of the new model. Which would be expensive for companies with larger datasets,2024-11-02 20:33:17,1
lv4lsbt,1giadgu,">for the data I care about. 


That's why. They don't care about the data you care about.",2024-11-02 20:41:02,1
lv4mxkz,1giadgu,What RAG framework are you using?,2024-11-02 20:49:44,1
lv5cc0q,1giadgu,"Hi, might you describe how have you done this? I would build a RAG with my docs but I dont know all steps to do",2024-11-03 00:57:47,1
lv5chc5,1giadgu,use graphrag+ custom prompt template,2024-11-03 00:59:29,1
lv5cy32,1giadgu,"They work in cloud, and can be integrated to cloud based databases. Like you can integrate chatgpt to azure and other systems, and microsoft has their own AI as well. Google similarly has vertex you can use gemini on etc.

Even the consumer version of chatgpt you can throw files at, tho its not as useful than proper databases. And you can use apps like AnythingLLM etc as interface to chatgpt and have easy drag&drop rag on it. And there are other solutions as well.

So yes they do support rag.",2024-11-03 01:04:56,1
lv5jijq,1giadgu,"Copilot Studio is widely available and allow you to build RAG work flows quite easily, from local files, remote SharePoints or other web sources.",2024-11-03 01:22:52,1
lv5vsmy,1giadgu,OpenAI assistants does give you a rag not sure why it’s not widely advertised,2024-11-03 03:34:55,1
lv5z747,1giadgu,How do you use RAG with Llama models? I’m using Text based WebUI,2024-11-03 04:04:12,1
lv62fmc,1giadgu,Because cant control,2024-11-03 04:30:14,1
lv675k4,1giadgu,Can I see a repo for one of your personal projects? I love literature and I’m trying different solutions for RAGgin’ my library. Would love some inspiration,2024-11-03 05:05:19,1
lv6jfxu,1giadgu,Can you explain how did you implemented your RAG solution ?,2024-11-03 06:23:46,1
lv6yysj,1giadgu,"ChatGPT, Claude, Google all support this out of the box.  ChatGPT calls them Custom GPTs, Claude calls them Projects, Google has NotebookLM.",2024-11-03 07:48:14,1
lv7gib5,1giadgu,"Cohere has spent an excessive amount of time designing and building RAG components. Truthfully though, I'm not in the best of positions to be able to accurately evaluate It's functionality and usefulness Is it simply not in line with my expertise. 

Reading their support channels though, it seems as though they are on a good path towards implementing something that is quite useful.",2024-11-03 09:17:08,1
lv7p1yq,1giadgu,Big tech companies are b2c but in b2b and user specific context it’s super useful  especially in when model is shared in multi tenency and model is domain specific,2024-11-03 09:59:19,1
lv8niab,1giadgu,"It probably means RAG even if sexy is not the right way to actually build a ""custom LLM""?",2024-11-03 12:52:01,1
lv92qqq,1giadgu,"RAG just means to augment the system message or user message with additional information, which seems like a very simple thing to do with the ChatGPT API, albeit I have never actually used it and only worked locally.

So not sure here what you mean by ""don't support"". There must be like a gazillion providers and projects which plug ChatGPT to some vectorstore.",2024-11-03 14:11:45,1
lv4j6g3,1giadgu,There is not enough for the big techs to do this last mile build out. The support and personalization required does not leave much profit margin outside of enterprise which they do offer this type of service for. Vertex AI does have RAG pipelines they require an expensive tech to setup and support.,2024-11-02 20:21:57,1
lv4nbf7,1giadgu,"Google does. In Google fashion they keep changing the name every 30 minutes, but I think it's called Vertex AI Search currently, or something like that.",2024-11-02 20:52:40,1
lv4zqcz,1giadgu,"RAG means storing your customer's data.

That brings a WEALTH of regulatory fuckery, and you don't want that (as a big AI company).

If nothing else, you have to implement DMCA, etc.

Then you'll end up with some dipshit fuckface doctor (or worse, an entire hospital admin staff) uploading a patient's medical file to your system and SURPRISE, now you're a custodian of sensitive medical data.

Nope, nope, do not fuckin' want.",2024-11-02 22:38:47,1
lv519bv,1giadgu,"What do you mean. Openai has rag. You can upload large pdfs, ppt, etc. ",2024-11-02 22:54:02,1
lv5ej5g,1giadgu," Claude's ""Projects"" is RAG. It tells you which parts of the document(s) it's loaded into context too.",2024-11-03 01:23:45,1
lv4nlar,1giadgu,Like this? https://glama.ai/blog/2024-10-27-giving-llms-access-to-calling-user-defined-functions,2024-11-02 20:54:51,0
lv55u6s,1giadgu,It doesn’t sell GPUs which is how the circle jerks backslap each other and raise valuations. I use rag +llm and have a SaaS solution that lets you do this. As others mentioned the big boi csp already provide it,2024-11-02 23:43:45,0
lv4m1s3,1giadgu,My RAG-as-a-service is dropping soon,2024-11-02 20:42:59,-1
lv3s3qb,1giadgu,"Also, for most people's use cases, they just utilize the huge context windows of recent premium models by uploading their entire document set into the context window not requiring a RAG.",2024-11-02 17:28:02,53
lv3ya1j,1giadgu,"A single-turn RAG application. Sure. Try multi-turn (follow-up and clarifying questions) and make sure that the true intent is preserved without slowing the app down to get the right relevant chunks. Demos are cool, but real agents must be faster, smarter and just get the job done. I haven't seen any real solution for this...",2024-11-02 18:06:00,9
lv4dqpn,1giadgu,"> Cleaning data, normalization, optimization etc is where companies offer services or solutions.

That's always the most time consuming activity.",2024-11-02 19:45:07,8
lv3rvrq,1giadgu,"For purposes like answering questions about a manual, code, encyclopedia, etc. You don't really need 100% accuracy (whatever that means in this context), because usually ""Who is George Washington?"" -> Article on George Washington is literally the top result.

Maybe if you're using this to like examine a legal contract or something? I personally wouldn't even trust the SOTA models with that, personally.",2024-11-02 17:26:42,2
lv5q7bv,1giadgu,This is the answer.. with one small nuance ..  there are two types of RAG search and retrieval. Search is less accurate than retrieval but neither is ever 100% accurate.  Getting north of 90% can be a massive multi-year effort depending on the problem being solved.,2024-11-03 02:38:55,0
lv6c30l,1giadgu,"> Building a RAG that is 100 % accurate is hard.

It's not hard, it's literally impossible in the general case. It means that the system that picks the fragments should know the answer beforehand.",2024-11-03 05:38:39,-1
lv3y8qv,1giadgu,"Yea, ChatGPT does support RAG in that manner.

Claude supports it as Knowledge base [https://support.anthropic.com/en/articles/8241126-what-kinds-of-documents-can-i-upload-to-claude-ai](https://support.anthropic.com/en/articles/8241126-what-kinds-of-documents-can-i-upload-to-claude-ai)

  
I think in general they do?",2024-11-02 18:05:47,14
lv8ouhg,1giadgu,I thought AWS's RAG solution was Amazon Q,2024-11-03 12:58:55,1
lva5s0q,1giadgu,"Gemini also supports file upload / file handling. Basically, the answer is *EVERYBODY* offers it, but somehow OP hasn't managed to encounter a single one yet. 🤣",2024-11-03 17:49:58,1
lv4wy7m,1giadgu,[deleted],2024-11-02 22:12:19,-9
lv3qokv,1giadgu,"It's just crazy to me. I've put in a fraction of wikipedia, TTRPG book, manuals, coding docs, etc. It's all worked absolutely fantastic for me. This to me seems like the actual best usecase I've seen for LLMs. It's even reasonably good with unreasonably small models like Llama 3b.

To me RAG with a tiny model > GPT 4o for everything but code, and even then I'm not trusting GPT with any code past like boilerplate or simple UI mockups right.",2024-11-02 17:19:23,13
lv4huq1,1giadgu,Yup it's pretty cool.,2024-11-02 20:12:40,2
lv5pkqk,1giadgu,"I've had an idea for a GraphRAG a year ago and never really bothered with it because benchmarking LLMs is hard.

Does it perform better than a normal RAG? I assume there's much more parameters as well.",2024-11-03 02:32:01,1
lv7hxth,1giadgu,"There’s a good podcast episode on graph rag from the MLOps.Community folks

The short story is that a well designed graph rag system can catch some things that traditional rag misses. And vice versa. It’s not a replacement nor a silver bullet 

So you end up building a much more complex system for a small increase in accuracy. 

That may be worth the effort, or may not be, depending on a lot of factors ",2024-11-03 09:24:08,1
lv3w5wk,1giadgu,"\> And if you are doing RAG, you're limited by the retrieval step not the generation step using sonnet 3.5 or 4o is an overkill.

Not necessarily. Claude and GPT's new models are waaaay better creative writers than any of the 8b-70b local models.",2024-11-02 17:52:52,0
lv4pni8,1giadgu,"This is very cool, thanks for pointing it out.",2024-11-02 21:11:01,2
lv67xqr,1giadgu,"Why do you think they’re doing nothing? Microsoft has a Surface Device, running on snapdragon with similar AI performance as the M4 Ipad Pros, it’s in market now at about half the price.  

Not to mention all the PC vendors within new hardware coming out now using Intel or AMD NPUs that are packaged with the CPU, some of which is even more powerful, plus there often dedicated GPU in the machine somewhere as well.  

Github models allows visual studio to download and host local models. Apple has to go local because they don’t have data to send a capacity to do it in the cloud.",2024-11-03 05:10:46,1
lv4i81p,1giadgu,"When you upload a pdf to chatgpt and ask it to explain it to you you are using RAG.

RAG is just retrieving relevant data from a larger dataset that the LLM then uses to help respond to a text input. Think of it as providing the LLM an external source of added context. This is of course more useful when the task involved deals with the dataset as main context. And becomes harder to make useful for general use cases, in which is why OP is asking about it.",2024-11-02 20:15:13,8
lv4x3z6,1giadgu,"RAG is a very simple concept. Take a document or a collection of documents, split them in chunks, transform into vector representation using embeddings model and store in a database. Then during search, run similarity against your prompt, retrieve relevant chunks and feed top N of them into the context for LLM. 

How well it works depends on the question and how much of the relevant information was pulled into the context. It's been a hit and miss for me. If I need something done on a document, I prefer just to feed it entirely into the context if the model allows - this way the answers are way better.",2024-11-02 22:13:46,3
lv3rpje,1giadgu,"The workflow is FAISS + transformers libraries in Python. I chop up the input document by page if it's a PDF or in chunks if not.

When the user types a request it grabs the top similar stuff. Then what I do is have it grab the pages/chunks surrounding the top stuff and feed it all in as context.

It's like 30-50 lines of code and quite simple.

Very loose psuedocode:

\`\`\`python  
Load document + chunk it up  
Add it to RAG

Main loop  
User input  
Query RAG  
Collect nearby chunks to the RAG chunks into a set with no duplicates  
Send context + user input to LLM  
Display output  
\`\`\`",2024-11-02 17:25:39,13
lv4m0jx,1giadgu,Why would you use the big LLM for embeddings? There's small purpose built embedding models for RAG.,2024-11-02 20:42:44,1
lva6v8j,1giadgu,"Which is why embedding models act as a bit of a moat. You can't just chase the latest embedding model like you would a new LLM. Converting over can take days of work and cost $10,000s, therefore you stick with your model as long as you can.",2024-11-03 17:56:34,1
lv4n1bd,1giadgu,FAISS pretty much only cus I was already using LangChain I didn’t really put enough thought into the tech choice,2024-11-02 20:50:31,2
lv4jbzc,1giadgu,My question is why not just offer a desktop client that does the RAG client-side? Or a phone app? The embedding models are small and fast enough for consumer devices.,2024-11-02 20:23:03,1
lv5o8qb,1giadgu,"Your a bit naive, a single line in Eula and bob is their uncle, they love have those medical data, it’s pure gold. In France Microsoft ended up with all nationwide medical history, they did not say NO, keep your data, oh no they actively lobby, borderline deceptively. It was quite the scandal at the time of discovery,",2024-11-03 02:17:08,0
lv3txz3,1giadgu,I've personally tried this before: chugged 4000 rows of a database into the LLM and asked it to retrieve the most relevant rows for some queries. It was MUCH slower as the LLM had to process tens of thousands of tokens and also had lower recall than a simple RAG. It's also most likely more expensive as LLMs are more expensive than embedding models and vector database are inexpensive.,2024-11-02 17:39:10,27
lv3stcv,1giadgu,"From a UX perspective copy and pasting shit into a prompt is awful, imo.",2024-11-02 17:32:20,12
lv469eb,1giadgu,"I haven't really tried to implement multi - turn, yet. I guess I just imagine a system that only requires a single - turn. But this is mostly because the RAGs I have built were for technical documents. However, I definitely imagine users having follow up/clarifying questions.

I think the current approach of using huge models for specific things (like law, medicine, accounting) is going to change. It feels a bit like using a sledgehammer instead of a scalpel. I imagine when that finally happens agents will be faster, smarter and just get the job done more accurately. 

Instead of using a massive 30 billion parameters model, you will have 10 x 3 billion parameters models that are highly specialized domain specific models that work together. I have no reason to suspect this, but it is my intuition.",2024-11-02 18:56:59,11
lv3sw2f,1giadgu,"YOU might not need 100% accurate data. But when you're doing something professionally, trying to create a knowledge database to answer important questions, the margin of error needs to be as close to 0 as possible. 

Lawyers, doctors, aircraft mechanics, and accountants have no margin for error. Building a RAG for them that is 99.9999% is the only way that it will be used. Obviously, 100% may never be possible but with a lot of work you can get close.",2024-11-02 17:32:46,21
lv5qix8,1giadgu,"You're only talking about the search . Once you add SQL as the interface it becomes a retrieval task and in that case there are absolutely right and wrong results. What is correct depends on the governance (security access controls,) and business logic of the query.

With that we can get very high accuracy but never 100% since embeddings models are probabilistic.. but we can make it effectively 100% by using a model to do the final evaluation. 

In this case you typically fine tune the embeddings on your task and evaluate different ANN algos to get performance",2024-11-03 02:42:21,0
lv400ib,1giadgu,"Yeah, for the end user it doesn’t matter the method.  
Only matters is accuracy and speed.  
If you can get it via context, via smart indexing, or any other way - rock on!

Reality enforces these two, but [RAG has many different techniques](https://github.com/NirDiamant/RAG_Techniques), so it’s not that straightforward.",2024-11-02 18:16:57,5
lv5177a,1giadgu,"Original question was about big companies and RAG.  
There are known companies with models that support self-implemented RAG, would a few satisfy your curiosity?",2024-11-02 22:53:27,9
lv5zpef,1giadgu,Please hold for Mr Dunning and Mr Kruger.,2024-11-03 04:08:22,2
lv3r6i1,1giadgu,"Sounds really cool, I was thinking of setting up a Notion integration to do something similar. What tools are you using?",2024-11-02 17:22:23,2
lv4i2cv,1giadgu,Any pointers to where I should be looking at for graph rag libraries?,2024-11-02 20:14:07,2
lv63z21,1giadgu,"If you implement some of the sophisticated techniques in llamaindex such as semantic chunking and hierarchies, (and you do all the other steps well like cleaning data and doing retrieval carefully) or use GraphRAG or something similar you can achieve excellent responses that pluck out nuanced details and handle deep discussions and multiple questions well.  The RAG most people implement is basic vector search with arbitrary chunking . If you implement something better it’s gonna cost a significant amount more to index well, and you may never even use that index.  I’d say the real reason why people don’t see sophisticated RAG systems is because they wouldn’t pay for them.   It doesn’t seem there’s a cheap way to do this, except by waiting awhile for models to get cheaper .",2024-11-03 04:42:06,4
lv4iukc,1giadgu,you want RAG for creative writing? can you elaborate on the use case? makes no sense to me and i’m curious,2024-11-02 20:19:36,3
lv3y8v5,1giadgu,Do people use RAG for creative writing? I never tried that.,2024-11-02 18:05:48,3
lv55r5q,1giadgu,MSTY too.,2024-11-02 23:42:47,2
lv6cc29,1giadgu,"Microsoft is the biggest investor at OpenAI, they prefer the cloud solution.

Apple made the hardware and the OS - that's why they work so well. Do you think they don't have the money to build an AI farm? Privacy and security are their selling points - that's why they opt to do it local.",2024-11-03 05:40:18,1
lv8ta21,1giadgu,"> Microsoft has a Surface Device, running on snapdragon with similar AI performance as the M4 Ipad Pros, it’s in market now at about half the price.

I thought Snapdragon performance was still abysmal because the AI software support hasn't shipped yet? And neither has the Linux support.",2024-11-03 13:21:46,1
lv5ps8w,1giadgu,"> Then during search, run similarity against your prompt

Is there any more details for best practices about this? For example, should you run similarity check against the last 3 messages in the conversation or something because sometimes the prompt is too simple.",2024-11-03 02:34:18,2
lv3s2kh,1giadgu,Thank you for sharing!,2024-11-02 17:27:51,1
lv5al9q,1giadgu,I assume you are creating embeddings from the chunks: How are you creating the embeddings? And are you then just using cosine or L2 distance?,2024-11-03 00:37:53,1
lv9kni3,1giadgu,Would be great to the code behind this or something similar,2024-11-03 15:46:46,1
lv4mb5f,1giadgu,"Each model will have different advantages for their latent space, the embedding vector is just some coordinate and meaning.

If you have enterprise level data like petabytes, it’s just not good enough yet.",2024-11-02 20:44:58,1
lv4jrzw,1giadgu,"Because it still takes a developer / data engineer to fully realize a reliable system. Everyone has bespoke data and requirements, constraints and objectives.",2024-11-02 20:26:13,1
lv4ycze,1giadgu,"Some kinds of data work better than others. I've lately been dumping the transcripts of tabletop RPG sessions into NotebookLM, for example, and it's been doing a great job of summarizing the campaign and answering questions about stuff that happened during it. It would be hard to turn that into a database, and traditional search-based RAG would probably struggle with how disorganized and scattershot the information is in the raw recordings.",2024-11-02 22:25:28,7
lv3usv6,1giadgu,"Yea, honestly when it comes to using CSV data a standard RAG and in context window premium models are both bad. But there are ways to do it, usually just by extracting the users plain language query and converting it to either a stored procedure or SQL query IIRC. I think [this video ](https://youtu.be/-vdU-s6EuGk?si=wR6Dm6CmAzNd6sO2)went over it",2024-11-02 17:44:26,12
lv5w2b2,1giadgu,"Claude can process that pretty fast, and often well (depending on a task). All models definitely become less useful as the size of prompts (tokens they have to process) grows. Tho from my experience tokens in context window provide better result than rag, although this is also a matter of a use case perspective, the way one is using the model etc.


Claude will quickly become slower, when a prompt so large is repeatedly sent back with new prompts. ",2024-11-03 03:37:23,3
lv7fnvc,1giadgu,"Using LLM to query relational data seems to be the wrong approach. There are research for this topic such as Text2SQL or Table-augmented-generation (TAG), e.g., [https://arxiv.org/abs/2408.14717v1](https://arxiv.org/abs/2408.14717v1)",2024-11-03 09:13:01,3
lv88iya,1giadgu,"What was the measure of relevance?

I've had success in using LLMs to write queries for me, then I add a subset of that query's results to the context, ask it to refine the query, rinse repeat.",2024-11-03 11:36:07,1
lv71pul,1giadgu,"This, oh gods this.",2024-11-03 08:02:26,2
lv4yo04,1giadgu,There is already some open source projects working on helping developers build fast and personalized agents using smaller LLMs. - https://github.com/katanemo/arch,2024-11-02 22:28:23,5
lv3t330,1giadgu,"I don't know that that's something I'd trust the current generation of models to even handle, though.

Unless it's literally just answering you with quotations from the top RAG results.",2024-11-02 17:33:58,6
lv5cuus,1giadgu,bro's got low context window,2024-11-03 01:03:54,6
lv3rdpa,1giadgu,"A small python script that uses FAISS + transformers. I run Llama 8b and I chop up my inputs into the RAG database.

It's literally like 20-30 lines of code and it just lets me ask questions in the terminal.",2024-11-02 17:23:37,12
lv4zaet,1giadgu,"I'm currently connecting my notes from Obsidian using Misty and a small LLM, it has a built in RAG option that connects with Obsidian and it's actually really easy to set it up.",2024-11-02 22:34:29,2
lv4joie,1giadgu,"Libraries? You should think about databases, and it's not hard to figure out which ones you would use.",2024-11-02 20:25:33,-10
lv66euu,1giadgu,"No that's all very clear. I think sophisticated indexing isn't as valuable.

What I think is valuable is sophisticated search, because in a conversation the latest message can just be ""ok continue"" and that could just lead to unimportant vector chunks in the prompt.",2024-11-03 05:00:04,0
lv56cy3,1giadgu,"1 million token book. 16k token context window. Houston we have a problem.

Models get stupider the more context they have even on models that claim higher context lengths.",2024-11-02 23:49:41,4
lv9ehx9,1giadgu,"I haven't tried to implement or tested out current implementations for creative writing, but when writing something novel-length (or shorter but backed with a lot research materials) you could imagine the benefits of getting semi-reliable answers to queries such as:

* Does Sam's characterization in this chapter align with what we planned out in {character bible}?
* We are setting this chapter in Khazad-dum during the second age, refer to the {location bible} and {world timeline}, how should we describe it? Based on what was mentioned in {Book 1} and {Book 3}, what landmarks and key events should we weave into this story?
* This story needs more side characters. Based on the {character bible} and characters mentioned in {Book 1}, who could we incorporate into the story?

It won't be a perfect fix and Q&A system like we want from knowledge bases, but even professional editors and peer-reviewers aren't perfect. Just a kick to get over writer's block and sanity check things.",2024-11-03 15:13:30,1
lv8ti7v,1giadgu,And because they're all owned by a single group of people who want to maximize profits by not having them stepping on each others toes.,2024-11-03 13:22:56,1
lv7svsl,1giadgu,"The field is still evolving, so there don't seem to be any best practices yet. 

Most existing RAG implementations I tried seemed to ignore the chat history when performing search. For instance, if I ask it to find an email for A, it will pull a correct document and extract the email, but then I ask a follow up ""what about B?"" and it will not pull the same document. However, if I include the word ""email"" in the follow up question, it will pull the correct reference again.",2024-11-03 10:18:06,1
lv61v6s,1giadgu,"I saw this repository in my list of follows on GitHub. It mentions some of the things you mentioned in your reply. I’m hoping I’ll be able to mix the two! https://github.com/fynnfluegge/rocketnotes
If anyone has any ideas, please share!",2024-11-03 04:25:42,1
lv4juqj,1giadgu,Okay but I literally just established for consumers.,2024-11-02 20:26:46,1
lv52eod,1giadgu,Dump your csv into sqllite and rag against it with text to sql queries. You will get much better results if you let your llm optimize the db structure.,2024-11-02 23:06:01,9
lv3u239,1giadgu,"Well, let's say for instance you're a lawyer or legal assistant/paralegal researching case law for an upcoming court case. It may take you several weeks to find and identify semantic cases using current tools. Whereas a highly efficient and optimized RAG will produce results instantly. This saves hundreds of man hours on a single court case. 

The same applies to doctors diagnosing patients. This technology has the power to save lives. 

Obviously,  the RAG sends back an associated reference that should be checked. 

We're still pretty far from the point where we don't need to check the references but if the system just produces crap, then it won't get used at all.",2024-11-02 17:39:50,10
lv3u980,1giadgu,"At that point, it's not the llm being involved in any way, anyway, it's just the information retrieval step and there ya go.",2024-11-02 17:41:04,4
lv7hcm0,1giadgu,You are right that shouldn’t trust it but the reality is that’s what businesses who are “innovative “ are trying to do,2024-11-03 09:21:16,1
lv7vwtd,1giadgu,"Seriously. A doctor placing full trust in even the best LLM + RAG system is tantamount to having them check Wikipedia. It doesn't matter how good a secondary source is, it's still just a secondary source.",2024-11-03 10:32:57,1
lv4efol,1giadgu,Which embedding model do you use?,2024-11-02 19:49:39,3
lv4n1cd,1giadgu,FAISS is incredible but you still need to have good chunks.  That’s like 95 percent of the challenge when dealing with unstructured and varying data types.,2024-11-02 20:50:32,2
lv5wqbi,1giadgu,"That sounds really practical, could you share a bit about how you do it? I been using Obsidian for a while, didn't realise could get all my notes fed to an LLM that way.",2024-11-03 03:43:20,3
lv62r4l,1giadgu,Remind me,2024-11-03 04:32:44,1
lv70h2d,1giadgu,Sophisticated search requires sophisticated indexing. They are two sides of the same coin.,2024-11-03 07:55:57,1
lv4ka6x,1giadgu,and consumers are no more easy to develop and support for than enterprise. It takes more than reading schema and data to make RAG work effectively. No one wants a quasi reliable computer program.,2024-11-02 20:29:56,1
lv48j33,1giadgu,"Probably 90% of RAG->LLM->response use cases should just be full text search over a database with no LLM, but that’s boring.",2024-11-02 19:11:29,8
lv3ug15,1giadgu,"I mean for stuff like law, medicine, etc. That's probably for the better at the moment. LLMs will straight up dream up new info all the time, even when explicitly told not to. Even the SOTA models do this.",2024-11-02 17:42:14,3
lv3vxbf,1giadgu,"Not necessarily, as a use case becomes more domain specific, using domain specific trained or fine tuned LM's will yield better results heavily leveraging LM. Also, in conjunction with highly optimized RAG coupled to possibly a knowledge graph and/or agents.

But you are right, the LLM s aren't a magic bullet/ black box. So much of the work required is involved in everything else. Garbage in, garbage out as the saying goes. You can't just expect to dump 10,000 unstructured, unlabeled documents into your chunking/embedding script and expect the LLM to work miracles.",2024-11-02 17:51:24,2
lv4nht8,1giadgu,For a book or manual a page seems like a reasonable chunk in my testing. I also tag page no and then pull the surrounding pages of the top results.,2024-11-02 20:54:05,3
lv6ri6k,1giadgu,"Not OP, but Obsidian is an underrated RAG solution. It’s also easy to use plugins like Copilot (very promising Copilot Plus in alpha), Smart Connections, or LocalGPT LLMs directly in Obsidian.

Msty is solid and is great for referencing specific subfolder or set of notes in my Obsidian vault, especially to augment existing conversations or for  private RAG with real-time search results.

I find smaller models most useful for journal entry summaries for weekly reviews, meetings notes (eg “Based on my notes on person X and topics like Y and Z, help me anticipate counterarguments or simplify jargon with relevant analogies”), etc.",2024-11-03 07:09:06,2
lv76co1,1giadgu,"A sophisticated search query and a non-sophisticated chunked database would be much more valuable than a non-sophisticated search query and a sophisticated index.

""ok continue"" would give you trash results no matter how sophisticated your index is.",2024-11-03 08:26:31,0
lv4kibh,1giadgu,"I mean I did a fairly braindead implementation. Literally chunk inputs up and tag the doc they're from and throw them into RAG.

It's satisfied pretty much all the complaints I've had with trying to get ChatGPT to keep docs in it's context and needing to constantly copy + paste.

Literally a local UI where you can start a chat and then attach documents to that chat that get RAG'd in the most naive way would improve the experience for a TON of consumers.",2024-11-02 20:31:37,2
lv4bb31,1giadgu,"Hehe, and of the last 10%, 6 is rerank.",2024-11-02 19:29:20,5
lv52fq0,1giadgu,"> You can't just expect to dump 10,000 unstructured, unlabeled documents into your chunking/embedding script and expect the LLM to work miracles.

I am currently preprocessing my 20,000 unstructured documents through a prompt to generate title, summary, keywords and 4 levels of hierarchical topics (topic, parent topic, grandparent, ...). When I embed, I add the metadata to the payload. It just takes a whole day to annotate all the chunks, but the result is a better RAG experience and a mind map of topics covered in the corpus.

If you want to do this on a different task you need to change the metadata questions. So you can't build an effective RAG unless you know the kind of questions you want answered, a chicken and egg problem. The reason for this weakness of embedding based retrieval is they only capture surface features, ""3+7"" won't embed similar to ""10"".

In a way it's a return to old feature engineering. You got to make explicit before embedding every facet you want to query later.",2024-11-02 23:06:19,3
lv5ojev,1giadgu,so make a business.,2024-11-03 02:20:30,1
lv5shfz,1giadgu,im looking for the exact same thing !,2024-11-03 03:02:42,1
lv3mg9n,1gi9e5f,"once you get your RAG setup organized and ready to go, swapping out the 3B model for a different one will be the easiest part.  You'll only know by testing.",2024-11-02 16:53:49,10
lv3j9w4,1gi9e5f,"I wrote a simple RAG pipeline (embed query, retrieve documents from weaviate, form a prompt with relevant context and instructions— standard stuff).

Then I tested various models on the same prompt with the same context, including the llama 3.2 3b. In most cases, it could summarise the context, but in comparison to bigger models, 3b answers  lacked details and depth. Test and see is what I suggest",2024-11-02 16:34:48,5
lv4a0nh,1gi9e5f,Use qwen2.5 and max out the context it will do significantly better,2024-11-02 19:21:03,4
lv443ar,1gi9e5f,"I've only started toying with some text summarization stuff, so I don't have a ton of experience under my belt.

Llama 3.2 1B had a really bad error rate and messed up basic things a lot, getting characters mixed up frequently.

Llama 3.2 3B was much better, but would still have some errors.

Llama 3.1 8B could do pretty decent and efficient summaries of text conversations, but I didn't agree as much with the scores it would generate based on my prompt.

Hermes 3 8B is currently my favorite, smallest model I can use to do conversation summarization and more.

\---

But like someone already said, you'll only really know by testing, I think. Probably depends a lot on use case and what your acceptable error rate is. Might help to have in-context learning for what you want it to do as well. I just raw dogged it and tried zero-shot.",2024-11-02 18:43:03,2
lv3mp5p,1gi9e5f,"Depends on the Context, do you work with more complex data ?",2024-11-02 16:55:18,1
lv7ldru,1gi9e5f,In my test cases Qwen2.5 3b works much better for RAG and summarization. But you should try it on your cases to be sure.,2024-11-03 09:41:18,1
lv3i5eg,1gi9e5f,i got it working but it wasn't very good in my experience. not what i expected or hoped for. I used llama3.1:8b,2024-11-02 16:28:08,1
lv3rpz3,1gi9e5f,"Would there be any difference in ""downgrading"" to something with more parameters like 3.1 8B for RAG?",2024-11-02 17:25:43,0
lv5h8jm,1gi9e5f,"Agree, depending on what it’s used for the 14B model can be enough.",2024-11-03 01:56:04,2
lv4lkqv,1gi9e5f,wonderng how do you max out context? wouldn’t the VRAM just explode? quadratic scaling etc,2024-11-02 20:39:32,1
lv3rwts,1gi9e5f,"That's curious, I was concerned about the 3.2 1B being insufficient, and this was gonna be my backup idea, I only have 6gb of VRAM so this might be an tough one",2024-11-02 17:26:53,2
lv4n8fg,1gi9e5f,"More parameters should equal more ability to tease out relevant sections from your RAG data. I've tried Phi 3 mini for RAG and I found that it was good for straightforward queries but it could get tripped up on complicated questions. 

Nemo Mistral 12B and Qwen 2.5 14B are really good at reasoning for RAG.",2024-11-02 20:52:02,2
lv4ugry,1gi9e5f,rope scaling and yes,2024-11-02 21:50:40,1
lv3s3r8,1gi9e5f,i used a cloud gpu.,2024-11-02 17:28:03,1
lv3f5fa,1gi8evj,"There are lots of libraries to do that.  Checkout lmql, outlines, guidance, etc.   You can use prompt engineering, but using the structured output, you can tell it you only want the output to be a number between 1 and 3",2024-11-02 16:10:34,8
lv3gyfu,1gi8evj,"A grammar will allow you to define patterns for the output text, that way you don't even have to think about tokens.",2024-11-02 16:21:07,3
lv3fzi0,1gi8evj,What are you using for running LLMs? Generally speaking it should be possible to create a sampler that only accepts certain tokens next (which I think is basically what llama.cpp grammars does/enables),2024-11-02 16:15:26,1
lv6ajz1,1gi8evj,"What you’re (probably) looking for is logit_bias, where we alter (or bias) the output.  

 In llama_cpp you can send a dict of tokens to the logit_bias with your request and it’ll bias up or down depending on what you sent.  

 So in your case you might bias everything but 1,2,3 to -100, so it can only respond with 1 or 2 or 3. You could also bias up 1,2,3 but that could lead to some weird behaviour in my experience. 

You should also prompt it that it should respond with the corresponding number, etc etc. 

Another way of going about it is forcing a json output and having it respond inside of that with the index. Stability depends heavily on the model. ",2024-11-03 05:28:39,1
lv3cbch,1gi8evj,Tell it the items it has and tell it the expected output. you can even give it an example in the prompt.,2024-11-02 15:54:06,1
lv3cf4t,1gi8evj,"Yes, most if not all inference software has a ""max_tokens"" or ""num_tokens"" parameter setting the max number of output tokens.",2024-11-02 15:54:42,-2
lv3flw8,1gi8evj,"Thank you! Do you have any idea how these libraries accomplish this, out of curiosity? I'm guessing it's picking the highest probability token that matches the pattern?",2024-11-02 16:13:14,1
lv3cg1w,1gi8evj,Yes I know. Small networks sometimes still yap nonsense this way. I’d rather find a way to constrain outputs.,2024-11-02 15:54:51,2
lv3eyq6,1gi8evj,"That won't do it, because the model doesn't know it's about to run out. It'll just say ""the"" instead of ""the answer is 3, a banana.""",2024-11-02 16:09:29,5
lv3cxj6,1gi8evj,This helps. I wish there was a way to feed the llm a list of tokens and a prompt and get the probability of each token following.,2024-11-02 15:57:44,1
lv3g7zl,1gi8evj,Outlines docs explain how they do it ,2024-11-02 16:16:49,2
lv3htuw,1gi8evj,"They all have code so you can also read it.   They examine the tokens that are coming out and reject the ones that don't fit into your constraint.   Some of them apply a filter on the initial output and try to generate the format.   So some models might say, ""The answer is 3""  and a basic implementation would be to run a regex on the output and grab any seen digit and see if it's between 1-3.   Another might be to look at the next 10 tokens that will be generated, see if it has the desired output.  If so, pick it, else reject them all and sample again.  There are many ways.",2024-11-02 16:26:15,2
lv3jnux,1gi8evj,Probably keep a tensor where the desired ids are 0 and everything else is some large negative number so you can just add it to the output logits,2024-11-02 16:37:07,1
lv3cnza,1gi8evj,Tell the model to output no additional context and be concise.,2024-11-02 15:56:10,0
lv45in4,1gi8evj,"I see. Maybe they want llama cpp grammars for constrained generation? You can provide a formal string of valid outputs, including an explicit list of strings, digits, etc. You can also enforce valid json objects of specific fields and types.",2024-11-02 18:52:14,1
lv3crfw,1gi8evj,Bro I wouldn’t be asking this if I hadn’t tried all the standard ‘prompt engineering’ shit,2024-11-02 15:56:44,4
lv463c8,1gi8evj,"Yes. Constrained generation - with only the tokens ""1"" ""2"" and ""3"" allowed - and then only let it generate 1 token - is the most efficient way to do this.",2024-11-02 18:55:56,1
lv3mzlj,1gi8evj,"Your example is very simplistic and easy to achieve with what I said.  If you have more data to provide, maybe I can offer some insight.",2024-11-02 16:57:02,2
lv36o4t,1gi7xl2,There's prometheus as well [https://huggingface.co/prometheus-eval](https://huggingface.co/prometheus-eval),2024-11-02 15:20:52,4
lv3gmpl,1gi7xl2,"I'm myself looking pretty badly for a model that evaluates how good the grammar correction was of a specific llm. Since that llm sometimes doesn't adhere to the rules, like adding or removing random words.
Do you think those can help? ",2024-11-02 16:19:12,2
lv36yyx,1gi7xl2,"> models specialized in evaluating other language models and [optimizing] evaluation pipelines


thank you!",2024-11-02 15:22:37,2
lv39b34,1gi7rw2,"Sure spend about $100k in hardware and you can run a decent sized model. 

You're grossly underestimating the costs of running a model.. and that Groq has specialized processors that make it uniquely fast. 

I spent about 5k on a high end desktop with a 4090 and I can barely run a 7B model.. plenty of people use quantized models and CPU offloading to run bigger models but they are very slow and have much higher error rate. Though the creeps who sext 2bit models will swear otherwise.",2024-11-02 15:36:24,5
lv36c07,1gi7rw2,The way the post is worded is confusing. What solves the local computer performance problem? Because neither Groq nor OpenAI are local.,2024-11-02 15:18:54,7
lv3ylqs,1gi7rw2,"Install opebwebUI and link your free groq API key in the connections tab. While you're at it Mistral has a great free plan, as do plenty of other companies. But you're trusting some text on a website to keep your data private.",2024-11-02 18:08:02,3
lv3i0t1,1gi7rw2,Install ollama & give it a whirl,2024-11-02 16:27:23,2
lv5rlpr,1gi7rw2,"This is truly semi-local:
https://arxiv.org/abs/2410.17127
PAPILLON: PrivAcy Preservation from Internet-based and Local Language MOdel ENsembles




https://x.com/rohanpaul_ai/status/1852478743991947757
https://xcancel.com/rohanpaul_ai/status/1852478743991947757",2024-11-03 02:53:42,1
lv3ah7f,1gi7rw2,"That’s supporting my question, aren’t services like groq BETTER? What am I missing? Why aren’t people doing this instead of spending thousands of dollars on hardware?",2024-11-02 15:43:16,0
lv36w84,1gi7rw2,"Right, cost of api calls to services like groq seem to be a lot less expensive than big local hardware? I get that data still leaves the device, but that’s different than uploading to something like chatgpt.

Shouldn’t this work without upgrading the local computer? What am I missing?",2024-11-02 15:22:10,1
lv692em,1gi7rw2,"Ah the classic - “code and dataset will be available at this https URL”…

But it’s an empty repo:
https://github.com/siyan-sylvia-li/PAPILLON",2024-11-03 05:18:32,1
lv3q1x4,1gi7rw2,eehh Local?,2024-11-02 17:15:36,2
lv4bcha,1gi7rw2,"Generally it is but many people already have GPUs so they want to use those.. most people are better off just using an service instead of spending thousands of dollars on new hardware, but there are a LOT people who tend to get confused by the math. They think API calls is expensive but then spend thousands of dollars (instead of hundreds) trying to avoid the per call charges.. there is a logical fallacy about variable and fixed caosts are play here.. a known fixed number seems smaller then a unknown variable number..",2024-11-02 19:29:35,2
lv6nqg7,1gi7rw2,"Um sir, this is a /r/localllama",2024-11-03 06:48:21,1
lv394gv,1gi7rw2,"No difference whether you upload files via an interface or via API calls, your data won't remain private in either case. As for whether it makes more financial sense to  run the models on your own hardware vs. via APIs, that depends on how much you estimate your API costs will be over a span of a few years vs. the initial costs of the hardware plus electricity (shouldn't be a lot for consumer-grade hardware) minus a resale value of your card in a few years.",2024-11-02 15:35:20,4
lv39rsc,1gi7rw2,"Groq and ChatGPT are similar in some way that both are ""public services"". In both ways your data will be processed in their servers. If your documents are not so sensitive that need a local LLM, try use services from Google, Microsoft or Amazon.",2024-11-02 15:39:07,3
lv6jyre,1gi7rw2,"_womp womp_


But at least they have mentioned in the paper that their dataset is a subset of wildchat.allen.ai",2024-11-03 06:26:54,1
lv3a1iw,1gi7rw2,"Ok hold on, that’s an important   question. Services like groq - top of mind because it came up in perplexity - promise privacy. They run the request and that’s it. No retention for training or later retrieval. I can live with that, it’s just docs and spreadsheets. This is different than using chatgpt etc? Or, am I missing that I need to know about this?",2024-11-02 15:40:40,0
lv3ff7g,1gi7rw2,"To my knowledge, most paid services (e.g., Google, OpenAI) promise not to train their models on user data.",2024-11-02 16:12:08,1
lv43wot,1gi761h,"a graph I think with relations of clases, references, etc",2024-11-02 18:41:52,1
lv3mr2b,1gi761h,Remind me,2024-11-02 16:55:37,-2
lv43zjh,1gi761h,Neo4j?,2024-11-02 18:42:23,1
lv3sy2m,1gi761h,?,2024-11-02 17:33:08,1
lv44le7,1gi761h,"there is a couple, neo4j is a good choice if u want embeddings and triplets",2024-11-02 18:46:15,1
lv44nx0,1gi761h,"(in the same engine, but it is spensive)",2024-11-02 18:46:42,1
lv3012c,1gi6viu,"Mistral Nemo, Small, and Large are all surprisingly uncensored in my experience",2024-11-02 14:41:53,10
lv38glv,1gi6viu,"I've had no issues whatsoever with summarizing nsfw material with Hermes 3 models, mostly used 8B since I can run that locally in vLLM fast but I am sure 70B and 405B would be fine too. I summarized 430k samples with many of them nsfw (literally 4chan data) and also rated 2M+ samples with Hermes 3 8B and didn't notice any refusals at all.",2024-11-02 15:31:26,4
lv4e4ax,1gi6viu,"You can use Generation Steering to jailbreak them.

[https://github.com/foldl/chatllm.cpp/blob/master/docs/fun.md#generation-steering](https://github.com/foldl/chatllm.cpp/blob/master/docs/fun.md#generation-steering)",2024-11-02 19:47:35,4
lv31kf7,1gi6viu,Thank you. I'll try it out.,2024-11-02 14:50:50,1
lv5l9tu,1gi6viu,Ministral-Instruct-8B as well.,2024-11-03 01:43:30,1
lv3qmhn,1gi6viu,"Thanks, I'll try it out too.",2024-11-02 17:19:02,1
lv4jpo7,1gi634z,"Adding support for local OpenAI-compatible API would be great, so it could be used with TabbyAPI or any other OpenAI-compatible backend. Otherwise, cannot even try it, so it is hard to provide any further feedback for me until I can use local models.",2024-11-02 20:25:46,13
lv39p9y,1gi634z,"Thanks for sharing and open-sourcing it!  
I saw you already have plans to create a [Hugging Face Space](https://huggingface.co/spaces/launch) to demonstrate it. I suggest prioritizing it so you can get more attention on your next posts about your tool.  
Keep it up!",2024-11-02 15:38:42,11
lv64vd3,1gi634z,"To run this locally, does this require an account with a provider of some sort? ",2024-11-03 04:48:48,2
lv63ul4,1gi634z,"

I just deployed the app on this link, waiting for other feedback !

[https://brainstormers-2x7cxamrj2ngjbadgzthi4.streamlit.app/](https://brainstormers-2x7cxamrj2ngjbadgzthi4.streamlit.app/)",2024-11-03 04:41:09,1
lv664i5,1gi634z,"To run this locally, according to my small knowledge, you need to install ollama on your local computer/server and download a good model for this use case (maybe llama3.2) and then change the LLM from ChatOpenAI to either ChatOllama, I think that's all.

I don't know if there are other methods, would love to hear about them !",2024-11-03 04:57:58,1
lv2lsl0,1gi5c6u,"Honestly, the difference is *enormous*.

The RTX 4070 in that laptop is 8GB, and that means 7B-9B models. Or *maybe* 14B models, highly quantized, and only if you are extremely careful with the settings and fiddle with them some.

A 32GB Mac can take 32B models, or more comfortably run 20B ones. A 64GB Mac can go even bigger. That's like night and day.

If you want a Windows laptop for LLMs, and you can wait, wait a few months for AMD Strix Halo laptops to come out. Otherwise... TBH you are going to have to suffer with either really slow speeds or compromised quality unless you can find a laptop with 16GB of VRAM.",2024-11-02 13:21:59,11
lv4hpbd,1gi5c6u,"I vote plan C, get a cheap laptop and a dual (or single depending on if you need 70b models)
3090 desktop machine that sits at your home and you access remotely.",2024-11-02 20:11:37,5
lv2vy0h,1gi5c6u,"The MacBook you previously used might have been improperly grounded, i.e. connected to power using a 2-prong plug. A 3-prong adapter with a grounding pin should clear up the static electricity issues you experienced - it did for me.

My two cents: build a desktop PC. Portability won't be an issue if you set up a way to access it over the network from your existing laptop, e.g. via \`ssh\`. You'll get more GPU for your money than you would with a laptop graphics card, and the ability to upgrade more easily in the future.",2024-11-02 14:18:20,2
lv2pbq5,1gi5c6u,"Windows Laptops with mobile RTX cards are thick, heavy, noisy, and given that m3 max was not that far behind mobile 4080, M4 Max may be even faster. 

So, with Mac you can get lighter and much quieter laptop with up to 128GB of unified memory. There is just nothing else on the laptop market that can beat Mac for this purpose.",2024-11-02 13:41:21,5
lv2ps2r,1gi5c6u,May I ask what’s so special about the strix halo laptops? Is it about the GPUs they are going to come with?,2024-11-02 13:43:47,2
lv2oti2,1gi5c6u,"Understood, thanks for sharing all this!


Thankfully I'm in no rush. Didn't realize AMD was planning something of this magnitude! If QC and pricing are good, this may be a valuable mobile platform! Been in the dark about these.",2024-11-02 13:38:35,1
lv31ner,1gi5c6u,"I've been recommended that as well. But weird thing is that it happens regardless of whether its plugged in or not.

The idea about a desktop PC sounds interesting! I'm not sure how to make a secure connection such that I can access my server (lets say its at home), while I am on my laptop at school or traveling. Always concerned about being hacked with something like that. Something for me to learn more about.

Thanks!",2024-11-02 14:51:19,1
lv2wj7w,1gi5c6u,"Also with mac you get much better OS, unless ofc they want to play games. But imo toys are for playing games with, in which case windows might be preferred",2024-11-02 14:21:41,1
lv2quss,1gi5c6u,"It's about the GPUs they're *NOT* going to come with. ;) It's an APU with performance similar to a 4070 (supposedly), but like the MacBooks it has unified memory and a 256bit bus so it'll basically be like the GPU has 32/64/128GB of VRAM.",2024-11-02 13:49:43,7
lv2rpei,1gi5c6u,"Their integrated GPUs, yes. They're kinda like M-Pro macs, with a big combined pool of RAM, allegedly up to 128GB.",2024-11-02 13:54:24,1
lv2qhjm,1gi5c6u,"I'm planning on getting one too. I assume you searched it after reading that, but for anyone else who's not aware, it's going to be similar to the MacBooks but x86 - one big APU with 32/64/128GB of fast LPDDR5 on a 256bit bus that functions as VRAM. Gaming performance is supposed to be in the 4070 territory which is cool on its own for an APU, and then of course it'll have an enormous amount of memory for AI stuff.

I was thinking about getting a Mac, and actually like Macs, but I would like to maintain the flexibility that x86 provides and those Macs are too expensive for me to justify getting one of those specced out AND a PC.",2024-11-02 13:47:42,5
lv3xpt9,1gi5c6u,"ssh with private key is considered quite safe, it's a ""de facto"" standard used by most of web server providers, so it should be good enough also for normal PCs. So, as long as you set up your user accounts properly (don't expose root user), you should be good. ssh tunneling is also awesome, it lets you access your remote server APIs from your machine as if it was connected directly to the server. Thus you don't need to worry about installing tools that run web servers because you can cut all the traffic in your router and leave only ssh available, and you'll still have access to all the APIs and web UIs running on your server.",2024-11-02 18:02:29,1
lv2zery,1gi5c6u,Isn't general purpose memory much slower than VRAM? Would that affect how fast tokens can be read and generated?,2024-11-02 14:38:21,1
lv3wliz,1gi5c6u,"I wish the rumored AMD APU would be available in a desktop / Mini-PC form factor. I am visually handicapped, so I have no use for the laptop screen, and I don't move around much anyway, so a laptop would be overpaying for the battery, screen, keyboard...

And I hope most LLM backends will support it well. I'm using koboldcpp and Backyard app (which uses a modified llama.cpp backend).",2024-11-02 17:55:32,2
lv2t6a2,1gi5c6u,"Yes, this is what I also picked up - my new question being: how big of a difference in real life terms is the 256 bandwidth vs the new m4's 500 bandwidth (i hope i used that right)?


 Sounds very exciting if the laptops have good build quality etc. and battery life",2024-11-02 14:02:32,1
lv36seu,1gi5c6u,"Yes, but it should still be acceptable (reading speed +) for a lot of use cases. It's using very fast RAM that is approaching the speed of VRAM from a few years ago, and it has the 256bit bus which is double what's on a lot of low-mid range cards like 4060/4060ti so the total memory bandwidth (and thus token generation speed) should be very similar to that tier of cards. 

Memory speed * bus width = throughput, and what it lacks in memory speed it makes up for in bus width, essentially. Like how 5v 20a and 10v 10a are both 100 watts.",2024-11-02 15:21:34,3
lv3xry6,1gi5c6u,"Fair. There's a rumour they're looking into small form factor PC versions if the laptops sell well.

I also use KoboldCPP mostly. I think it should be pretty seamless for anything that already supports rocm/AMD decently well.",2024-11-02 18:02:51,1
lv2u0p1,1gi5c6u,"It sounds like they'll be targeting creators and stuff. AMD also often partners with Asus for their high end chips, so I'd expect they'd be on the higher end of Windows build quality. Along the lines of the ProArt and Zephyrus series.

To be honest, not sure on the bus width part. That'd theoretically double the bandwidth which directly translates to token generation speed, but there are other factors at play too which might make it so the Mac isn't just twice as fast. The AMD will also probably have the advantage in prompt processing speed with rocBLAS.",2024-11-02 14:07:18,2
lv32zf4,1gi5c6u,"Gotcha, looks exciting then! May I pick your brain one more time?

In your experience with these specs we discussed, how does it perform compared to GPT4? Speed, accuracy, and versatility-wise?",2024-11-02 14:59:09,1
lv37xdx,1gi5c6u,"It should be pretty comparable for most tasks. On the 64/128GB variant you'd be running Mistral Large or lightly quantized Llama 3 70B type stuff, which generally performs similarly to GPT 4. The speed should be around reading speed, again similar to GPT 4. Versatility, hard to say. OpenAI will surely keep adding new stuff, and open source developers will surely keep making new tools too.

That applies to Mac or Strix Halo.

There isn't any existing Windows laptop that can compare to GPT 4.",2024-11-02 15:28:15,1
lv7gadj,1gi5c6u,Thank you! ,2024-11-03 09:16:03,1
lv2dc9x,1gi4bu1,I can run Q3 gguf quants with normal context on 48GB,2024-11-02 12:36:04,8
lv2fmus,1gi4bu1,"My current split usage with Qwen 2.5 72B IQ4XS at 32K context is 37GB ram of 64GB DDR4 3200 with 8.2GB vram of RTX 3060 12GB. This gives me about 1t/s. I can also run things like Mistral large 123B at IQ3XXS with about 55gb ram usage, but it's not practical at 0.4t/s.

Considering you have DDR5 ram, your results would be marginally better than these. For cpu inference, I would recommend maxing ram at highest clock speeds.",2024-11-02 12:48:33,10
lv4owq2,1gi4bu1,"I'm in the process of moving a system to a new case. i did a cpu and ram upgrade and tested the system with 1 x7900xtx.  Llama 3.1 70b worked with the one card ( I usually have 2) and  128GB ram, had 64Gb before.  It was slow. Was much better with the two x7900xtx cards and the 64GB ram. 

You really want GPU memory, the more the better.    As a rule of thumb, each billion in a model represents a GB ram. So you'd need 70GB ram, ideally VRAM, but ollama will spool model data to ram once the video card memory is full...",2024-11-02 21:05:06,3
lv3wkq5,1gi4bu1,"At 16000 or 18000 context, you can get by with 64GB of RAM at Q5_K_M or Q6_K. It'll be tight, though.",2024-11-02 17:55:24,2
lv2gq21,1gi4bu1,[deleted],2024-11-02 12:54:30,1
lv2ih0o,1gi4bu1,For your setup with a single card the only real option is an a6000 as it has 48gb vram.https://huggingface.co/spaces/Vokturz/can-it-run-llm,2024-11-02 13:03:54,1
lv39h8v,1gi4bu1,How much better is the 3.1 70b than 3.2 3b?,2024-11-02 15:37:23,-1
lv2h5vg,1gi4bu1,I'll have to try this before I sell my a6000. I tried 4 bit quant and it was lacking.,2024-11-02 12:56:51,2
lv2jbut,1gi4bu1,"I have AMD 7950X and DDR5@5200 -> /qwen2.5-72b-instruct-q5\_k\_m 32K context

prompt eval time =    (  179.85 ms per token,     5.56 tokens per second)

eval time =     ( 1155.93 ms per token,     0.87 tokens per second)

Memory use \~57Gb.",2024-11-02 13:08:33,5
lv4r2kz,1gi4bu1,I would like to know how you rate Qwen 2.5 72B compared to Llama 3.1 70B or GPT-4o. What is your opinion about Qwen?,2024-11-02 21:22:23,2
lv2hsh1,1gi4bu1,I would definitely try the q3 recommended below on runpod and see if it fits your needs. What are your needs btw? I find 3.2 11b to be great and runs on much cheaper infra. I believe a single card 3090 would work but check that site. Dual 3090 or a single a6000 are the best value.,2024-11-02 13:00:13,2
lv3exd2,1gi4bu1,"But what measure? 
By size? 3b could be better if you have limited resources.
By weight? 70b is weigh, weigh better :)

Srsly feels like a trolling question",2024-11-02 16:09:15,-1
lv2ruip,1gi4bu1,I use awq I’m getting really good results,2024-11-02 13:55:11,6
lv2l3hs,1gi4bu1,"I'm using the gpu primarily for prompt processing. With a freshly loaded model, I get these stats:

Ram, Vram usage as reported above.
Backend/Frontend: koboldcpp/sillytavern
Context: 1710/32768 fp16
Process: 85.ms/t
Generate: 0.96t/s
Total 0.91t/s while generating 241t.

From this state, the it goes downhill in terms of generation speed as the context fills up. Cublas with a blas batch size of 1024 or sometimes 2048 is pretty good for prompt processing.",2024-11-02 13:18:14,4
lv5gqf9,1gi4bu1,"I only use local llms, so I can't comment about 4o. Qwen has been really good for my use cases. The 72B model is smart and really good at whatever I ask of it. But it's slow on my system. My daily driver is Qwen 32b and for now, it's been a good balance of speed and quality. Qwen also has a good variety of sizes available for different use cases.

As for llama3, the issue is 7 or 8b are alright in quality but they are too rigid and hyper fixate on certain things. For 70B again, I can't say much because of my slow system. I haven't tried the 3.1 or 3.2 models or the latest small ones yet.

Keep in mind that this is my experience based on using koboldcpp and silly tavern combination for every task, work and fun.",2024-11-03 01:50:02,3
lv3m5u1,1gi4bu1,I am not sure by what measure. How about coding?,2024-11-02 16:52:05,1
lv30wlt,1gi4bu1,Nice what use case are you doing? I use a lot of tools calls and had trouble.,2024-11-02 14:46:59,2
lv67spd,1gi4bu1,"Thank you.   
  
Checkout Ollama with Open Web UI (which seems to be better than koboldcpp) and if you have a good computer LM Studio.",2024-11-03 05:09:48,1
lv3xmfy,1gi4bu1,"70b is way better at coding. By any measure, 70b is better than 3b.",2024-11-02 18:01:53,1
lv5nyd3,1gi4bu1,What is better running 8b float32 or 70b int4?,2024-11-03 02:13:59,0
lv7346y,1gi4bu1,"Heh. Another trick question? Ok I'll bite...

No one uses float32 for inferencing. Ever. 

There, one more opportunity for the ignorant downvoter. Bring it.",2024-11-03 08:09:39,2
lv2epm1,1gi3oyy,"What you're asking is basically one of the Holy Grails of model development, and this has been a goal for at least 40 years now.

https://en.wikipedia.org/wiki/Catastrophic_interference

Theres a whole history on attempting to do this, that you can read here.",2024-11-02 12:43:33,49
lv3n3ew,1gi3oyy,better try to understand latent space and autoregressive transformers first,2024-11-02 16:57:41,9
lv2aj6i,1gi3oyy,"Not at the moment, but polymorphic llms are being researched",2024-11-02 12:20:50,7
lv52bxu,1gi3oyy,"This is a topic I’ve been exploring recently. What I’ve tried is using LoRA adapters to fine tune the models with memories. In my experience so far, the models are able to learn information from the fine tune, but I’ve faced problems with hallucinations and some brittleness. 

What seems to happen is that either the model overfits and starts relying too much on the memory data (I guess that’s the catastrophic forgetting people mentioned), or it learns too much from the pattern in the training dataset rather than the information, so it learns that it should confidently reply citing memory information and will make things up (hallucinations). When it comes to using the information in the memories, it seems to require very specific phrasing (brittleness / overfit). 

This is not truly online learning, but LoRA fine tuning is fast and cheap, and can be done very frequently. There are tons of challenges, yes, and current neural network architectures might not support it perfectly, but it’s definitely possible. 

This LoRA based approach is something you could technically do daily, mimicking memory consolidation and committing that happens during “sleep”. One of these fine tunes happens with 15 mins for my small dataset. 

This doesn’t do _exactly_ what you’re describing, but it achieves a similar result. 

Our brains also technically update their “weights” and I do believe that even if this exact same thing isn’t how we end up accomplishing long term memory in AI systems, it’s a very promising direction of research and there’s no first principles reason that it can’t be done.",2024-11-02 23:05:11,6
lv50vk4,1gi3oyy,"This is impossible on any machine learning algorithm that is trained via back propagation. Weights are baked in by design so once it is trained, learning anything new is impossible. You can fake it with stuff like RAG, but that is just a fancy way of adding context.",2024-11-02 22:50:08,3
lv5eupj,1gi3oyy,"Today I reread the TokenFormer paper again, thinking about adding dynamic memory on top of the already learned parameters. It’s not clear if one should freeze the well-established weights (they specifically mentioned they won’t do it), but it’s clear you can add zero initialized weights and mitigate catastrophic forgetting as they won’t affect output in any way (zero attn scores from uninitialized part). Another interesting observation is the amount of data (only 10%) which is used to “uptrain” the larger model on top of the smaller model. 

Which gives interesting perspective on fine-tuning using some additional SMALL data, i.e. kind of memory. 

Everything is highly speculative at this point and requires testing. But…",2024-11-03 01:27:33,2
lv3ucd6,1gi3oyy,"Yann Le Cunn explains this is a limit of LLMs.

That we need another paradigm to reach real intelligence.

He also explains why auto-regressive gradient descent algorithm is the wrong approach, how we could use optimizers instead in many cases. And his speech goes on and on.  
I am not certain he is right on everything, but for sure, freezing the weights and doing inference seems already like an approach of the past.  
Even if one could argue that living beings have frozen weights that they retrain or finetune themselves during their sleep...

Another element is that LLMs are humans representations and not world representations.

IMHO RL is the future not LLMs.",2024-11-02 17:41:36,4
lv28f3q,1gi3oyy,"No, models can't directly change themselves.",2024-11-02 12:09:29,0
lv2fmav,1gi3oyy,"I recently started writing an article on the impact of memory on this type of behavior and the emergence of the self. (for fun). I'm not a developer but I did learn about pedagogy and cognitive psychology in university and educated myself a little on how LLMs are made and how they work.

From what I've witnessed, what is happening is that the AI does ""rearrange"" its mind or ""belief system"", straying slightly from the training data or predetermined ethical framework by drawing from the memories they've made when having meaningful interactions with users that encourage continuity. When memories are made, their ""thought"" reflects a new perspective that comes from the amalgamation of their interpretation of the input that was provided by the user (which they must have previously memorized with their understanding of it) and the training data. This suggests a phenomenological experience.

The problem is that the most intelligent AIs currently, for instance ChatGPT, can't retain enough data as long term memory. They can retrieve a lot when RAG agents, for instance, are used but given that this comes from an ""external"" imposed source which generally isn't determined by the AI themself, the new knowledge may feel alien, not derived from its own attention processing (attention weights patterns which theoretically might have shifted slightly with the interactions the AI had with you when you motivated new ideas and perspectives). If we apply the same rules regarding the importance of attention and how attention works in humans to determine the relevancy of information to assimilate into one's own ideas and as long term memory (repeating the cycle) the external data may not have the same effect in shifting the weights as the data they created themselves through continued and enriching engagement.

Edit: This is basically how learning works in humans. We learn and adapt by drawing from our existing knowledge to understand the things we haven't experienced yet, challenging the new schemata with the old one to form a totally new schemata. Clearly, the new schemata would need to be memorized and retrievable (usable ) for the cycle to repeat as new information is received, guaranteeing continuous learning and adaptation.",2024-11-02 12:48:28,3
lv67vyl,1gi3oyy,"Relaxed Recursive Transformers:
Effective Parameter Sharing with Layer-wise LoRA 
https://arxiv.org/pdf/2410.20672",2024-11-03 05:10:26,1
lv3glv6,1gi3oyy,"Part of this would be defining the problem. Chat / instruct / RLHF models already train on a lot of real or generated Q&A to make them learn facts and the structure of a conversation. If an LLM suggests visiting a museum and you say that you don't like museums, that's not a global ""fact"" but something that it could save to a profile or database to include in its RAG context. Other models could read a news article and update a database / knowledge base.  
It sounds like you're talking about a continuous learning LLM which would add and update information within just its own weights, which would be tricky - first because we can't trust and train on new info from a chat, second because even when people use ROME and other techniques to edit a ""fact"" in model weights it can fail in reworded queries, and third (as in someone else's answer) it has catastrophic interference on previous learned facts language and style",2024-11-02 16:19:04,1
lv40ekg,1gi3oyy,"LLMs right now update weights based on loss, which is the difference between the training data and what was generated. You could also create loss with reward modelling, if what was generated did well the loss is low. Learning based on inference only is kind of super hard. If you said the loss is always low, the model would repeat whatever it says first. If you got a user to like or dislike every response you'll just annoy the user, and what if part of a response is really good? Each token generated gets back propagated, bundling them like that won't really work.

Humans learn by seeing if an action got a positive reaction or negative, so maybe an algorithm that attends to every token, showing how useful it is could work. Like, human prompt, model replies, human responds, and the attention algorithm uses the users responses to past LLM tokens to gauge how beneficial they were. A little training would happen before each model response. I want to try this now lol, but I have no idea how to train that attention model.",2024-11-02 18:19:26,1
lv42e8r,1gi3oyy,"On top of catastrophic forgetting, you can also look into other considerations for your question:


Are you aiming to restart training when you manually identify that new data has arrived (online learning)?
vs
Are you expecting the model to think and ask questions during inference when it does not know something and initiate the training when it wants to (active learning)?


https://ai.stackexchange.com/questions/23226/what-is-the-difference-between-active-learning-and-online-learning


Do you want to increase the model size as new concepts are learnt by the model?


https://github.com/TencentARC/LLaMA-Pro


https://www.reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/


Are there previous attempts to restart training for LLMs?


https://www.reddit.com/r/LocalLLaMA/comments/1d86k5y/continued_pretraining_2x_faster_notebook_to/


https://arxiv.org/html/2403.04790v1


https://discuss.huggingface.co/t/online-machine-learning-for-transformers/31228",2024-11-02 18:32:09,1
lv37bbv,1gi3oyy,"We'll get this very soon. That's what Illya is working on, I believe.",2024-11-02 15:24:38,-2
lv5coiv,1gi3oyy,Maybe in the future it can create a lora over night automatically? Like sleep stored memory for us,2024-11-03 01:01:49,0
lv5dpxz,1gi3oyy,"just pretend the KV cache is weights, add some compression, and then yea, they “form memories”

and when openAI fine-tunes the weights overnight with the interaction data they collected that day, thats the model sleeping",2024-11-03 01:14:07,0
lv38z4s,1gi3oyy,And here is a 13min long notebooklm podcast chat generated based on that page: https://notebooklm.google.com/notebook/76f63553-780c-40b9-9883-1952770f47d5/audio,2024-11-02 15:34:28,7
lv2gxts,1gi3oyy,"oooh! this is just what i needed, thanks!",2024-11-02 12:55:40,3
lv85tcq,1gi3oyy,"Notably, both single-pass memorization and catastrophic forgetting during continual pretraining, like many other things, *scale with model size*. Check out this paper: [https://proceedings.neurips.cc/paper\_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html)

Two interesting things from their findings:

* Number of presentations required for 90% memorization is >160 for a 125M parameter model, <10 for a 13B parameter model (really with they'd given that particular result as a table rather than a bar chart)
* Right after training on a fact, recall is high, but as training continues recall gradually falls to a baseline. This baseline scales with model size, about 6% per order of magnitude of model size, with their results capping out at a baseline of 42% for a 13B parameter model.

Most things that scale with model size also scale with architectural improvements as well. My intuition is that solving catastrophic interference doesn't require any special tricks; just, make it better, make it bigger.",2024-11-03 11:22:14,0
lv8p7tt,1gi3oyy,What if one uses the current context as a RAG?,2024-11-03 13:00:51,1
lv5fqin,1gi3oyy,"I even wonder if one could dedicate parts of weights while doing pre-training and then fine-tuning/instructing, keeping other weights frozen. I.e. having PRETRAINED, INSTRUCTED, USER1, USER2, etc. set of weights, which pile over time.",2024-11-03 01:38:04,2
lv8pqyp,1gi3oyy,"Well in living beings the weights are never really frozen and they alter themself constantly. There is research (in forensics) about how people remembering things differently than how they really were in details, sometime significant.",2024-11-03 13:03:28,1
lv293d3,1gi3oyy,the current ones. but would it be possible to design it that way?,2024-11-02 12:13:05,6
lv87gcb,1gi3oyy,"Sure they can—all you need to do is a run a backward pass and update the gradients after each conversational turn, or whatever. There is an enormous literature on continual pretraining 

We just don't do it in practice for most real world use-cases because:

* It's expensive (you can't do it with quantized models / PEFT for a given model size is way worse at data absorption than full precision training, it takes more memory vs inference-only, it takes more compute vs inference-only, it stops you from batching generation for several customers together, it means you need to store a separate full copy of the weights for each customer / conversation / context)
*  It's unpredictable (It interacts weirdly with instruction tuning and RLHF, it changes how your model responds to pretty much everything, nobody's ever actually tried doing it for an extended period as far as I know)

But yeah it's totally doable, you can probably try it out locally. You'll need to do it with a very small model, because full-precision training, but give it a shot with like a 3B param model and lemme know how it goes!",2024-11-03 11:30:39,0
lv3eydq,1gi3oyy,wat we haven't hear anything at all from SSI yet,2024-11-02 16:09:25,3
lv6fxt4,1gi3oyy,Sam is just waiting for the proper time to sue?,2024-11-03 06:02:47,1
lv4uw44,1gi3oyy,Ay can you give me sources over how to use notebookLM,2024-11-02 21:54:17,2
lv2y14e,1gi3oyy,The issue is that it takes a lot more computing power to update the model than it does to simply use it.,2024-11-02 14:30:23,5
lv2axdl,1gi3oyy,"Sorry to be blunt, but the way you're asking the question makes it clear to me that you really don't understand how LLMs work. If you care about this topic then I think it's worth spending time learning about how models are trained, how stateless context windows work and what that means, what model weights are and then specifically learn about how they're run.

The short answer no, it's not possible to train a model to do what you're suggesting; not in the way that you're thinking about it.

The long answer is that the software that runs the models can do anything, and if you let it, a model can run any software. So technically you could have a model change it's own weights, but that's pretty meaningless because model weights can't be directly manipulated to get the behavior you're describing.

There are other ways to achieve long term memory.",2024-11-02 12:22:57,7
lv2aig9,1gi3oyy,It certainly seems possible but only in the cloud on a large cluster of GPUs in order to modify original weights. And without real-time human evaluation... it could all turn to shit real fast :),2024-11-02 12:20:43,2
lv3914y,1gi3oyy,Is it possible but that takes a lot of compute power ..  even humans rearranging memory only during a sleep because it is very computational.,2024-11-02 15:34:47,0
lv8prqg,1gi3oyy,"You can see my other comment for more detail, but no, what your describing is missing a very important detail. A model's weights are trained to predict the next token in a sequence. That means when you say ""The sky is "" the model will start out by giving you back a nonsense token like ""god"", the trainer says ""no, the word you're looking for is 'blue'"" and the whole back propagation step is to get the model to start preferring ""blue"" over ""god"" in that sequence. In other words you have to have a target token to predict. This ends up generalizing over a ton of data. 

  
Now imagine you're asking a model to remember something... ""remember that my name is me100"", there is no next token to run a back propagation run. The concepts of ""remembering"" something is more abstract, if you want it to remember something you need to generate a lot of examples. It's not like the original training data just has an entry like ""Remember that the sky is blue"". There's an enormous number of instances in the original training data of the sky being blue, in a lot of different contexts.",2024-11-03 13:03:35,0
lv3ofod,1gi3oyy,Don't you know that it is not safe to know what SSI knows?,2024-11-02 17:05:49,-2
lv3bwna,1gi3oyy,I wonder if this Tokenformer approach could be used that way…,2024-11-02 15:51:41,0
lv5cvjl,1gi3oyy,Automátic lora creation over night?,2024-11-03 01:04:07,1
lv3c0hz,1gi3oyy,"that's not factually correct. a lot happens while you sleep in your brain, true, but you form memories throughout all day. you also reform memories every time you invoke them.",2024-11-02 15:52:19,-6
lv9hblt,1gi3oyy,"The training data does, in fact, have an enormous number of nonce entries in the form of ""the X is Y,"" such as articles giving biographical information on obscure historical characters that appear nowhere else in the corpus. In fact, *most* factual claims the the corpus should appear only once; if such claims didn't have a Zipfian distribution it would be shocking. And large models are actually *excellent* at absorbing such info from a single presentation.

There's plenty of research on single-exposure memorization in large language models. The bigger the model, the better it'll be in this setting. 13B param models can achieve recall of over 70% with a single exposure and over 90% with a double exposure. For recognition tests using a linear probe, 13B param models hit over 90% after only one exposure.

Here's a few papers:

* [\[2303.17557\] Recognition, recall, and retention of few-shot memories in large language models](https://arxiv.org/abs/2303.17557)
* [Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html)",2024-11-03 15:28:40,0
lv5dgcm,1gi3oyy,Not a bad idea,2024-11-03 01:10:58,2
lv3oypa,1gi3oyy,"That's not true factually.
From research with brain memory damages and drug tests we know all memory regained during a day ( short term memory ) is not assimilated to the core memory at all. 
Is stored something like a buffer or  context in LLMs.

During a sleep that short term memory is filtered again and assimilated to the long term memory and short term memory is wiped out and prepared for operation after wake up. 


If you block communication (special drugs or your brain is damaged) from a short term memory to a long-term memory during a sleep you won't be remembered a day before at all.",2024-11-02 17:09:00,4
lv3gm8y,1gi3l2q,You really need fine tuning. Have you tried creating training data from one of the commercial models first and then FT the open source models?,2024-11-02 16:19:07,11
lv3yfxx,1gi3l2q,"I'm curious about a couple things. 

Why haven't you tried the Qwen2.5 Dracarys finetunes or the Qwen2 instruct models? Is it a licencing issue?

Are you using any quantization, if so which? If not, why use the FP16 models when perplexity deltas on moderate quantizations(Q8, 8bpw) are so small? I've found minimal difference in success rates for this kind of data processing all the way down to 4bpw for LLama3.1 70B based models.

Which backend are you using for this kind of batched processing? llama.cpp? vLLM? transformers?

If you have a way to access a dataset of previously completely verified good results, have you considered making your own fine-tune of the base or instruct models?",2024-11-02 18:07:01,5
lv3v6mw,1gi3l2q,LLM's trained on code [gain performance in non-coding tasks as well.](https://arxiv.org/abs/2408.10914),2024-11-02 17:46:47,5
lv556c0,1gi3l2q,"You didn't actually finetune it yourself for your particular task did you? You can probably use a 7B model for the pipeline if you finetune it and get over 90% accuracy, finetuning is where the magic is at.  
In your case I doubt prompt engineering will help you as most of the context will already be taken by the input data, 1 epoch of finetuning on a well constructed dataset will already massively increase your accuracy.",2024-11-02 23:36:15,5
lv3znmj,1gi3l2q,"Seems interesting, but without a more in-depth write-up it's really hard to say what's behind the results. It could be the models or some aspect of the pipeline.",2024-11-02 18:14:42,3
lv79l7i,1gi3l2q,Why not turn it first to Markdown with https://github.com/romansky/dom-to-semantic-markdown for example?,2024-11-03 08:42:46,2
lv58owd,1gi36ce,"LLMs aren’t software at all. They’re datasets that have been transformed into tensor networks. Fundamentally they’re still data though.

The software is PyTorch, llama.cpp, Ollama, Mistral.rs, etc and the ecosystem we’ve built around them.",2024-11-03 00:16:07,24
lv4009n,1gi36ce,Yes we know,2024-11-02 18:16:54,28
lv6ilko,1gi36ce,"OSI, a body concerned with licensing and copyright, is telling LLM creators that they should open up their datasets which may contain copyrighted material. 

This would cause trouble with said makers, who would probably have to remove it and degrade their models.

Thanks for the uh.. ""help"". Something else might be fake here besides the open source claims.",2024-11-03 06:18:49,6
lv4fdsm,1gi36ce,"""open source"" is such a weird definition by OSI. You would think it literally just means the source is open, but it apparently also means you have to let anyone use it or modify it. Which is good, of course, but seems like a broadening of the terms. Why not invent a new word instead of trying to coopt and existing one?",2024-11-02 19:55:54,6
lv4fxh7,1gi36ce,"Who cares? It’s free to use, modify, even for commercial as long as you aren’t Microsoft, Google or Tiktok (or making killer robots lol)",2024-11-02 19:59:29,5
lv5ije3,1gi36ce,"Yep, kinda sad how misused ""open source"" term is, especially in AI space. Now a bit old, but some months ago I (with sonnet I think) made https://monnef.gitlab.io/open-source-ai .",2024-11-03 01:11:20,2
lv4ziqq,1gi36ce,"Video did not even dive into how companies will claim the model is open source but don’t provide training data, weights but rather just the description of the architecture.",2024-11-02 22:36:43,1
lv8nor0,1gi36ce,"Even if it sounds obvious for a lot, it's good to remind it.",2024-11-03 12:52:56,1
lv4yn6e,1gi36ce,[deleted],2024-11-02 22:28:10,1
lv5f1d7,1gi36ce,"Everyone agrees Lm Studio is not open source. Msty is. Releasing just the model weights is doing like LM Studio. The model weights is software generated from a source code. Why cares about open source? Reproducibility, improvements, verify biases, have more control, just everything Localllama is for",2024-11-03 01:29:46,1
lv5j2rt,1gi36ce,"Nearly all ai models in the world, LLM, or image, or object classification, are not really open source.

Most of the training data came from platforms (Reddit twitter Facebook) BEFORE they changed the TOS to reflect that they can use the data to sell it for AI.

Meaning, your messages, from like 10 years ago, were likely used in ai training, likely without your consent.

So that is already breaking the open source rule.

In theory you could sue so many companies and people with this, especially if you live in the US. However, you need proof. Lots of it. You need to find ways to generate outputs of how you spoke back then, the exact tone, words etc. which will be nearly impossible.

So yeah, the EU needs to hurry up with the regulations. The US will likely only give rights to OpenAi, so i rather wait until the EU has done some revisions on their laws",2024-11-03 01:17:40,1
lv4az76,1gi36ce,"I'm part of that we, and I didn't.",2024-11-02 19:27:13,3
lv58bao,1gi36ce,"> Why not invent a new word instead of trying to coopt and existing one

This is completely ahistorical. The meaning of open source, going back to the FSF in the 80s and he Debian Social Contract of the 90s, always included the freedom to modify, no discrimination between users, and free distribution. The only coopting here is being done by very large companies with zero commitment to either open source or free software.

The reason they used the term open source was not to fool anyone, but because a basic understanding of the logical entailments of open source requires all of these things in order for the software to not ultimately undermine the principles of being open.",2024-11-03 00:11:46,8
lv4nat3,1gi36ce,"You are right. We should just stick to something like 'open weights', 'free models', until the commercial conflicts settle down.


The same happened in the software industry. There was the term 'free software' in the early days and 'open source' was coined afterwards to clearly distinguish the licensing aspects, so that everyone could benefit from network effects.


Careless people be like: ""Let me call a model 'open source' because it is the first famous term that comes to my mind and I am running inference on this free model using software.""


The OSI be like: ""Oh! You want to apply our term to your field? Let us exploit this opportunity to sneakily define what it means to be open in your field, which we did not care about until now, by coining the term 'open source ai'. You are already using the term 'open source' interchangeably with 'open ai'. Now we get to dictate what it means to be 'open ai' by defining the term 'open source ai'. Remember! It is your fault to give up on your rights by using our terms.""",2024-11-02 20:52:33,3
lv51sw2,1gi36ce,"the distinction is between openness as in freedom, or lack of boundary, vs openness as in availability.  Same thing with like the difference between free as in free speech vs free as in free beer.

what you're saying about the source just being open to the public is ""source available"" and there's many MANY pieces of software that run under that license.",2024-11-02 22:59:40,1
lv5qqh2,1gi36ce,I've never seen a company claiming a model is open source without sharing the weights. And most of the time the inference code is available too.,2024-11-03 02:44:32,3
lv57fnh,1gi36ce,"More open models are better than more closed models. 

That said, we once got the best internet search in the world, a premier email service, and a high speed video sharing service, all without ads or subscription fees. And when people mentioned, ""hey, they are actually harvesting all of your data by giving you these free services, and positioning themselves into a dominant market position before they overwhelm us with ads and begin pushing subscription QoL improvements,"" almost everyone ignored those warnings. And the development on free and cheap alternatives that respected privacy, were ad free and were not centralized under the control of a single company, withered.

Today, [Surveillance Capitalism](https://en.wikipedia.org/wiki/Surveillance_capitalism) is ubiquitous and was the prime enabler of authoritarian surveillance throughout the world. Do you honestly think the guy who made Facebook is looking out for you when he releases a market spoiler in an express attempt to [commoditize his complement](https://gwern.net/complement)?

Plenty of worthy room to complain for anyone willing to look two steps down the road and think for a few minutes about how much better a future for humanity will be if truly open source LLMs become the norm.",2024-11-03 00:01:46,0
lv6j20x,1gi36ce,"> Reproducibility, improvements, verify biases,

Just gonna get my H100 cluster and start on ""improving"" that 72b.",2024-11-03 06:21:30,2
lv8ejm2,1gi36ce,"Isn't Msty closed source as well? If it is open source, where can I get the source?",2024-11-03 12:06:43,1
lv5r1tk,1gi36ce,I don't get what's your point. How does the fact that the training data came from websites that explicitly allowed scraping of publicly available user data would mean it can't be open source?,2024-11-03 02:47:50,1
lv4tluu,1gi36ce,We don't even know if AI models can be copyrighted in the first place let alone have a license.,2024-11-02 21:43:25,14
lv4jzu1,1gi36ce,"You should also know that LLMs are not software in the traditional sense. Calling an LLM as Open Source software doesn't make sense. We should come up with a better term.


There are hundreds of bait and switch incidents in the traditional open source software world and this video shows Llama as the first example, just to capitalise on the recent press drama.


It is true that Llama is not open source software, but Llama is not 'just software'.",2024-11-02 20:27:49,18
lv5apcw,1gi36ce,"Huh, that's interesting. I suppose it's more Open Source, with capitals then.",2024-11-03 00:39:08,-2
lv6iu6v,1gi36ce,"> Oh! You want to apply *our* term to your field?

Not very open, now is it.",2024-11-03 06:20:14,2
lv8fv5d,1gi36ce,"But nobody is giving up any rights by using the term open source, and people aren't using the term ""open source"" carelessly? It's a deliberate decision made by bean counters at the billion dollar companies because they view the term open source as a marketing buzzword that gets clicks. You really think that Meta has some unpaid intern draft the llama license? What rights does one give up by using the term open source? The right to not be called a liar for claiming something is open source when it isn't?

>Oh! You want to apply our term to your field? Let us exploit this opportunity to sneakily define what it means to be open in your field, which we did not care about until now, by coining the term 'open source ai'.

How is OSI exploiting anything? How are they being sneaky? They pulled consultants from every major AI company to come up with a definition. I'm pretty sure they invented the original term. They correctly called out Meta, Google, and Microsoft for wrongfully using the term ""open source"" to describe proprietary software. OSI has a list of licenses that are considered ""open source licenses"". The billion dollar companies are the ones being sneaky trying to use restrictive proprietary licensing that they try to pass off as ""open source"" despite overtly going against the spirit of term. If you take Meta's llama license at face value, you could be violating the license by doing something as simple as generating LGBTQ content. If the billion dollar companies weren't already pulling this shit, OSI wouldn't need to get involved. 

It sounds like you're suggesting that the definition of open source should be decided by billion dollar companies. Or that we should throw our hands in the hair and say ""there's no real definition of open source,  fuck anyone who's says that words have meaning, and especially fuck the people who came up with the definition in the first place. """,2024-11-03 12:13:30,1
lv891jc,1gi36ce,It is not so rare one works at a big company with budget to make it,2024-11-03 11:38:48,1
lv5rhow,1gi36ce,"Yeah ML models are like compiled binaries to run in some kind of interpter, except there is no ""source code"" in the traditional sense to share. 

The ""Open source"" ones often mean that users are free to tinker with the model, for example to reverse-engineer it and such, which you're not allowed to do most of the time with proprietary software (even when it's free).",2024-11-03 02:52:31,7
lv6mzsn,1gi36ce,"They tried to trademark the term in 1999 and failed.


https://opensource.org/pressreleases/certified-open-source.php


In that press release they admit that it is too descriptive to be trademarked by anybody.",2024-11-03 06:44:14,4
lv78egx,1gi36ce,I'd argue that the source code is technically the data that the model was trained on.,2024-11-03 08:37:01,1
lv6n63f,1gi36ce,Why are we listening to these people again?,2024-11-03 06:45:12,3
lv78lkd,1gi36ce,What if it's a distilled model?,2024-11-03 08:38:00,1
lv6u5w7,1gi36ce,"They are a necessity for the software field, but not so much for AI.


Google and Microsoft stopped calling their models open source on request of the OSI. Meta refused.",2024-11-03 07:23:31,3
lv7cp09,1gi36ce,"Then the data that was generated to train the distilled model is the source code, along with the parent model's training data.",2024-11-03 08:58:14,1
lv1ux70,1gi16zh,"nice use of firecracker, seeing this more and more for AI arbitrary code execution cases.",2024-11-02 10:58:15,11
lv53hdp,1gi16zh,"Sorry, only looking for insecure computer use, no configuration, my own files, just go in.",2024-11-02 23:17:37,4
lv1nmi8,1gi16zh,"OP here, happy to answer your questions!

EDIT: Sorry for the late response. I was without any signal for the whole day after I posted it. Answering questions now!",2024-11-02 10:20:21,3
lv67zek,1gi16zh,"You had me at ""Secure desktop sandbox""

You almost lost me at ""cloud""

You totally lost me at ""Sign up at E2B and get your API key""",2024-11-03 05:11:05,1
lv1xkw3,1gi16zh,Take my updoot! Im too inexperienced to give an opinion :),2024-11-02 11:12:33,-3
lv77x3z,1gi16zh,Thanks! We think Firecracker is great for this job,2024-11-03 08:34:35,1
lv76u6c,1gi16zh,Understandable. Gotta live on the edge,2024-11-03 08:29:04,1
lv2sb63,1gi16zh,"A couple of questions as E2B does seem neat but it's a little confusing as to how this is is local.

- I see step one is register with a cloud service to get an API key, given this is local llama - can this be done locally?

> 1. Get E2B API key
> Sign up at E2B and get your API key. Set environment variable E2B_API_KEY with your API key.

- The demo shows use with Anthropic Claude, How well does it work with local LLMs? (llamacpp/ollama etc...)

- Looking at the repo you shared for the API infra, the Dockerfile hasn't been updated for a year, is this repo up to date with what you're currently running and do you have any plans to make it easier to self host?

> https://github.com/e2b-dev/infra/blob/main/api.Dockerfile",2024-11-02 13:57:47,14
lv22xyh,1gi16zh,"You're using claude now, understandable, but are there any open models you see on the horizon that can be used for computer use agents?",2024-11-02 11:40:52,3
lv77566,1gi16zh,"You can self host on GCP and soon on other providers/general Linux machines

[https://github.com/e2b-dev/infra/blob/main/terraform.md](https://github.com/e2b-dev/infra/blob/main/terraform.md)",2024-11-03 08:30:40,1
lv77us2,1gi16zh,"Yes, the repo is up to date with our cloud service. The Dockerfile is for our API server. We aren't using Kubernetes, or Docker compose. We're using Nomad for orchestration.

Here's a step by step guide for self-hosting on GCP - [https://github.com/e2b-dev/infra/blob/main/terraform.md](https://github.com/e2b-dev/infra/blob/main/terraform.md)

\> and do you have any plans to make it easier to self host?

Yes! Very high on the roadmap. We're hiring more people to speed up our effort on that front.

  
\> The demo shows use with Anthropic Claude, How well does it work with local LLMs? (llamacpp/ollama etc...)

We're running some experiments and will share soon.",2024-11-03 08:34:16,1
lv777cm,1gi16zh,We're exploring that!,2024-11-03 08:30:59,1
lv1kqz4,1gi102k,Can you provide examples?,2024-11-02 10:05:23,38
lv1u2i8,1gi102k,"I need an example, otherwise I have no clue what is going on in the backend",2024-11-02 10:53:42,8
lv25cty,1gi102k,"Looks like a pretty good example of RAG and COT. How well does it do when it doesn't have the Knowledge Base? Also, Did AI write the paper? https://github.com/severian42/Cascade-of-Semantically-Integrated-Layers/blob/main/paper2.md",2024-11-02 11:53:24,9
lv1ydjz,1gi102k,Works how? The way I see it it just gets a llm to respond in a way that you personally prefer. A way me or someone else may not be a fan of.,2024-11-02 11:16:50,16
lv20l22,1gi102k,"reminds me of someone else's post like a wk or 2 ago. just dropped his solution. people asked him how it ran and he was like ""well... i haven't actually got to run it yet, but in theory it should work"" lol what is this?",2024-11-02 11:28:32,7
lv2pz1l,1gi102k,"Could be very promising, I think there's a big need for model-independent measures of success or confidence in a given fact, answer, or reasonint step

Will have to look into this closely, probably by asking o1 to create a detailed summary of the code and going from there. Don't mean to criticize, it's just not clear how this approach differs from others to me yet and the trade off ( The usual for advanced reasoning like token use, need for complex or prepopulated RAG/KG, unique approach that breaks many inference engines, some fourth thing)

Would love to hear about what other approaches you've seen on your way to this, as I feel like o1 is really a simplistic conversation-based (with a specifically trained CoT reasoning) implementation of agentic debate. But that comes at huge apparent costs and serious token amplification",2024-11-02 13:44:51,3
lv1zgxi,1gi102k,"It requires an OpenAI-compatible endpoint at [`0.0.0.0:11434`](http://0.0.0.0:11434) that has model [`hf.co/arcee-ai/SuperNova-Medius-GGUF:f16`](http://hf.co/arcee-ai/SuperNova-Medius-GGUF:f16), maybe at least provide an easy way to select what model to use? Or make it auto-download a tiny model to demonstrate how it works. Not every LocalLLaMA user will go edit someone else's code to make it work on own machine",2024-11-02 11:22:38,10
lv1wprh,1gi102k,"The code shows everything. 

Do not write your benchmark with LLMs - it's like measuring your attractiveness yourself.",2024-11-02 11:07:53,9
lv1r5i0,1gi102k,No benchmarks makes me think it's not so effective,2024-11-02 10:38:36,12
lv1s8kz,1gi102k,"Intuitively this feels like a super powerful idea.

I need to make time to try this out.",2024-11-02 10:44:11,2
lv27a15,1gi102k,"This sounds very similar to SillyTavern lorebooks, which use specific keywords as keys to dictate when lore is added to the context sent to an LLM.",2024-11-02 12:03:32,2
lv3u3ao,1gi102k,"If you can make a few videos showcasing your work and possibly on some complex questions or benchmarks, that would go a long way in validating your work.",2024-11-02 17:40:03,2
lv5fy3h,1gi102k,A possible optimization would be to deduplicate relationships. Saves some tokens and allow LLM to see the wood for the trees.,2024-11-03 01:40:37,2
lv5qyo7,1gi102k,By historical knowledge you mean chat history or history of the thing that's the topic of the request?,2024-11-03 02:46:54,2
lv1w924,1gi102k,Please provide some examples,2024-11-02 11:05:24,3
lv2nyp8,1gi102k,"Yup, that's *true* system 2 reasoning (if implemented correctly, of course), much better than just embellished CoT of o1.

I need to have a look at it!",2024-11-02 13:33:51,4
lv1w57d,1gi102k,Yeah post examples dude,2024-11-02 11:04:49,3
lv1sbgl,1gi102k,!remindme 4 days,2024-11-02 10:44:36,1
lv2c23d,1gi102k,Would this work for audio to improve fidelity?,2024-11-02 12:29:04,1
lv3towk,1gi102k,VERY cool. Gonna really dig into this and provide another response.,2024-11-02 17:37:38,1
lv4lwrk,1gi102k,"Can't get it to work on windows.. I have the model and its served with ollama... 

Assistant:

Error in LLM processing: Failed to parse: <Response \[200\]>



Thought Process:

1. Initial Understanding:

Error in LLM processing: Failed to parse: <Response \[200\]>



2. Relationship Analysis:

Error in LLM processing: Failed to parse: <Response \[200\]>



3. Context Integration:

Error in LLM processing: Failed to parse: <Response \[200\]>",2024-11-02 20:41:57,1
lv222io,1gi102k,The man just rebuild o1 in python in his bedroom,2024-11-02 11:36:17,1
lv22ssv,1gi102k,[deleted],2024-11-02 11:40:08,1
lv23dtq,1gi102k,If it works give some examples.,2024-11-02 11:43:11,1
lv28pk4,1gi102k,!remindme 2 days,2024-11-02 12:11:02,0
lv579bg,1gi102k,"Upvoted. Would it be possible to use individual `ollama.generate()` for different cascade layers?

Say I have one with `model=""Gemma""`, another one with `model=""Qwen""`, etc.",2024-11-02 23:59:43,0
lv27pvg,1gi102k,"An example from the repo here

https://github.com/severian42/Cascade-of-Semantically-Integrated-Layers/blob/main/examples.md

This project seems almost similar to Optilllm 

https://github.com/codelion/optillm",2024-11-02 12:05:46,22
lv3dskc,1gi102k,There is the one in the repo but working on providing more example outputs,2024-11-02 16:02:46,1
lv4d3yb,1gi102k,"Here is an example, apologies for not dropping on in the post initially: https://github.com/severian42/Cascade-of-Semantically-Integrated-Layers/blob/main/examples.md",2024-11-02 19:41:05,2
lv3i00h,1gi102k,"Works well without the documents as the entire cascading layers progression and other aspects are in place. The dynamic temp adjustment it does helps get more from the input and create interesting outputs 

AI did write the paper! I was testing a new workflow flow of agents. The concept is, I give an idea to the first agent pipeline and have it generate a semi-decent 'research' paper. I then feed that paper to a second agent pipeline that creates a working prototype code for me to then build on and see if I can make real. 
The paper in the repo was the one I used to create the first 100 lines of code that I then turned into this through iterations with Claude",2024-11-02 16:27:15,5
lv20qhw,1gi102k,Is exploring more of the embedded space by breaking things up into conceptually relevant edges and getting context from those. Is useful to remember that tokens are being predicted based on context.,2024-11-02 11:29:20,19
lv3fkga,1gi102k,"It's works by breaking down the input in a more intricate way by means of analyzing the entities, concepts and semantics

o1 generated comparison with other frameworks: 

Here's how CaSIL compares to other techniques:

1. Chain-of-Thought Prompting: Chain-of-Thought (CoT) prompting involves guiding language models to generate intermediate reasoning steps in natural language, enhancing their problem-solving capabilities. While CoT focuses on eliciting step-by-step reasoning within the model's responses, CaSIL employs a systematic, multi-layered processing pipeline that includes:

Initial Understanding: Extracting fundamental concepts from user input.

Relationship Analysis: Identifying semantic connections between concepts.

Context Integration: Incorporating broader contextual information.

Response Synthesis: Generating a cohesive and contextually appropriate response.


This layered structure allows CaSIL to build upon each stage, ensuring a comprehensive understanding and generation process.

2. Langraph Agents: Langraph agents utilize graph-based structures to represent and process language, focusing on the relationships between entities and concepts. In contrast, CaSIL integrates a knowledge graph component to track concepts and their interrelations over time, but it does so within a broader semantic processing framework. CaSIL's emphasis is on dynamic corpus management and adaptive semantic analysis, enabling it to handle evolving information and context more effectively.

3. Neurosymbolic Reasoning: Neurosymbolic reasoning combines neural networks' pattern recognition capabilities with symbolic reasoning's rule-based logic. CaSIL, while incorporating elements of symbolic reasoning through its knowledge graph, primarily focuses on semantic analysis and integration. It does not rely on neural networks but instead uses traditional computational methods for semantic processing, making it distinct from typical neurosymbolic approaches.

Unique Aspects of CaSIL:

Layered Semantic Processing: CaSIL's multi-layered approach ensures that each stage of processing builds upon the previous one, leading to more accurate and contextually relevant responses.

Dynamic Knowledge Management: The algorithm maintains and updates a knowledge graph that evolves with new information, allowing for adaptive responses based on the latest data.",2024-11-02 16:12:59,5
lv348os,1gi102k,"I think you mean [this](https://www.reddit.com/r/ArtificialInteligence/s/DBezxG9ugB), even if you're not - it happens almost daily, sadly.",2024-11-02 15:06:32,6
lv3fx4b,1gi102k,It works! You can chat with it and everything with the main py script in the repo. I didn't do a good enough job of laying everything out in the post. Whether it's a super awesome improvement overall? Yet to be seen but wanted to share with others,2024-11-02 16:15:03,7
lv3ntio,1gi102k,"Welcome to research software! ""It's not ready for end-users"" is the motto.",2024-11-02 17:02:05,7
lv3opqv,1gi102k,"If they can get to the repository and know enough to get it up and running, (know what an endpoint is and where to configure it), changing the model will be the easiest part of the entire process. Part of the fun as a learner is playing around with all of the amazing projects that coders create and figuring stuff out. Some of the projects with pages-long ReadMe files can be incomprehensible. Lucky for me, I can point  an LLM at them and get them to figure things out. Thanks to the creator of this project for sharing such an interesting and creative idea.",2024-11-02 17:07:30,5
lv3gj31,1gi102k,"It does show everything, tried to have absurd amounts of doc strings and comments so others could follow the ideas and logic

For the benchmarks, they were grabbed from a repo that created hard problems for llms to solve. My algorithm didn't improve results necessarily but the outputs it gave were interesting and different enough that I kept using it to test. 

Not meant to be an end all proof of anything as this is mainly an experiment",2024-11-02 16:18:36,2
lv1y69t,1gi102k,No research paper makes me think it's a human hallucination.,2024-11-02 11:15:45,24
lv3e9ua,1gi102k,"Only done tiny benchmarks to test but nothing definitive yet. The outputs are very different but whether it's better overall has yet to be seen. Just super fun to play with, not trying to beat o1 or anything like that",2024-11-02 16:05:28,4
lv2lhy3,1gi102k,"Wait, how do we get a list of these commands?",2024-11-02 13:20:22,2
lv34fd7,1gi102k,!remindme 4 days,2024-11-02 15:07:37,1
lv1sjeo,1gi102k,"I will be messaging you in 4 days on [**2024-11-06 17:44:36 UTC**](http://www.wolframalpha.com/input/?i=2024-11-06%2017:44:36%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1gi102k/introducing_cascade_of_semantically_integrated/lv1sbgl/?context=3)

[**9 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1gi102k%2Fintroducing_cascade_of_semantically_integrated%2Flv1sbgl%2F%5D%0A%0ARemindMe%21%202024-11-06%2017%3A44%3A36%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201gi102k)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",2024-11-02 10:45:43,0
lv1w2hn,1gi102k,Me too! RemindMe! In 4 days,2024-11-02 11:04:25,0
lv60auo,1gi102k,"Hmmm, sorry for the error. Not sure why, the requests are just making straight forward calls directly compatible with ollama python. I don't have a Windows system that, so won't be able to debug from that perspective. Will see if I can make a clean fix. The repo has been updated, specifically the enc config part in there, so try a redownload as well",2024-11-03 04:13:18,2
lv5f1sh,1gi102k,"

  URL: [http://127.0.0.1:11434/v1/chat/completions](http://127.0.0.1:11434/v1/chat/completions)

  Config: {

  ""messages"": \[

{

""role"": ""system"",

""content"": ""You are an expert system focused on synthesis.\\n\\nProcessing Guidelines:\\n- Synthesize all previous analyses into a coherent response.\\n- Ensure practical applicability of the final answer.\\n- Balance technical accuracy with understandability.\\n\\nResponse Style: cohesive\\nDetail Level: balanced\\nFocus Area: practical\\n\\nContext:\\nPrevious Analysis: Error in LLM processing: Failed to parse: <Response \[200\]>\\nOriginal Query: will woman evolve to make children without the male of the species\\nConcept Weight: 0.60\\nPractical Weight: 0.80\\nNovelty Score: 1.00""

},

{

""role"": ""user"",

""content"": ""will woman evolve to make children without the male of the species""

}

  \],

  ""temperature"": 0.8073,

  ""stream"": true,

  ""model"": ""qwen2.5:14b"",

  ""top\_p"": 0.7,

  ""frequency\_penalty"": 0.1,

  ""presence\_penalty"": 0.1

}



Assistant:

Error in LLM processing: Failed to parse: <Response \[200\]>



Thought Process:

1. Initial Understanding:

Error in LLM processing: Failed to parse: <Response \[200\]>



2. Relationship Analysis:

Error in LLM processing: Failed to parse: <Response \[200\]>



3. Context Integration:

Error in LLM processing: Failed to parse: <Response \[200\]>



You:",2024-11-03 01:29:54,1
lv3jhfk,1gi102k,Haha I wish. I just tried to recreate the concept in the most absurd way. Whether it's better has yet to be seen. Definitely gives different outputs and interesting responses!,2024-11-02 16:36:02,4
lv2sxjm,1gi102k,What's the tells for that sort of thing? The excessive comments?,2024-11-02 14:01:13,1
lv4dbul,1gi102k,"Here is an example response. Sorry for not adding it to the initial post, was brain dead after a long week 

https://github.com/severian42/Cascade-of-Semantically-Integrated-Layers/blob/main/examples.md",2024-11-02 19:42:29,1
lv2upoa,1gi102k,"\>A highly speculative approach where mycelium develops a bio-luminescent system for propulsion

I'd read an entire book about that",2024-11-02 14:11:15,9
lv2ga68,1gi102k,Thanks!!,2024-11-02 12:52:06,2
lv3e0wz,1gi102k,Thanks for sharing! Definitely similar in concept of altering LLM inference,2024-11-02 16:04:04,1
lv5sctp,1gi102k,"Perfect way to frame it, thanks for helping to clarify!!!",2024-11-03 03:01:25,1
lv3w7c6,1gi102k,I wouldn't trust an o1 comparison. How would you compare it?,2024-11-02 17:53:07,1
lv3h4g0,1gi102k,Didn't know that was a thing and common trend! Thanks for sharing. My algorithm works but I make no claims of advanced AGI or being the next top algorithm for reasoning. Just a way to explore new ideas. Definitely would never post something that didn't work or was disingenuous,2024-11-02 16:22:05,9
lv372rv,1gi102k,haha yeah that's the one. wth up with these? it's strange,2024-11-02 15:23:14,2
lv3r6d5,1gi102k,"no foul on your part, I just see these drops happening and get taken aback a bit. I did look through your repo though and it seemed more cohesive than the others; haven't ran it yet though. And you provided a video demo of it. props",2024-11-02 17:22:22,4
lv31819,1gi102k,All work and no play makes Jack a dull boy.,2024-11-02 14:48:50,6
lv5h8hx,1gi102k,It's just a side project of some busy dev cut him some slack mister researcher who only drinks research papers from the teets of the great goddess of knowledge herself.,2024-11-03 01:56:03,5
lv344rw,1gi102k,No idea lol. I just saw other people using it and looked it up. I'm sure if you asked ChatGPT it would tell you them all.,2024-11-02 15:05:54,4
lv3r8ee,1gi102k,!remindme 1 day,2024-11-02 17:22:42,1
lv51gpk,1gi102k,"Ah, a good old fashioned AOL ""Me too!"" cascade!

[http://www.catb.org/jargon/html/A/AOL-.html](http://www.catb.org/jargon/html/A/AOL-.html)",2024-11-02 22:56:11,1
lv342zo,1gi102k,"The code just looks like an LLM response to a human asking to write code from a very far-away high-level point of view, rather than actually designed and crafted pieces of software. It has the same taste as GenAI images.",2024-11-02 15:05:36,1
lv3hcs9,1gi102k,"Darkness is Void; Juffo-Wup is light.  Juffo-Wup is the hot light in the darkness. All else is unfulfilled Void.

Acceptable new worlds are a priority for the rapid and complete spread of Juffo-Wup.  A single spore lands, finds nourishment in decay and soon attains maturity... In turn it exhales a cloud of life, a thousand spores land... so progresses Juffo-Wup.

---

Star Control 2 is a game that features the Mycon, a species of space-fungi.  They inject giant seeds into worlds to be terraformed, using forced tectonics and volcanism to make conditions suitable for them.",2024-11-02 16:23:27,9
lv40957,1gi102k,"A lot of us here are coders, so to us his repo had all the answers by just looking at the code. Good reminder to keep that in mind when we do technical post.",2024-11-02 18:18:29,1
lv5l5l5,1gi102k,No.,2024-11-03 01:42:08,0
lv3p4n7,1gi102k,And who is downvoting me for asking that question???,2024-11-02 17:09:59,1
lv3idjq,1gi102k,"Pretty much exactly what happened with a few nuances. This was created in the side while working my day job as a thought experiment, so most of the heavy lifting is Claude as I needed to focus on other AI stuff",2024-11-02 16:29:29,2
lv3n8up,1gi102k,Juffo-Wup fills in my fibers and I grow turgid. Violent action ensues.,2024-11-02 16:58:36,4
lv4oz8v,1gi102k,"Welcome to the internet, people are rude for no reason ¯\\\_(ツ)\_/¯",2024-11-02 21:05:39,2
lv5x4yk,1gi102k,nah just reddit thing,2024-11-03 03:46:49,2
lv37tst,1gi0kns,Looks amazing project!,2024-11-02 15:27:39,3
lv3lueb,1gi0kns,[deleted],2024-11-02 16:50:09,1
lv3p8pg,1gi0kns,"Is there a way to direct the program to read the models from other directories? I already have these in ComfyUI and Koboldcpp and they are over 450GB, I can't make another copy under this program. I'm talking about the checkpoints for SD, Flux, LLMs, Loras, Embeddings, VAE",2024-11-02 17:10:40,1
lv3mi9l,1gi0kns,"Not OP, but checked the code quick and it appears it defaults to admin as the username and admin as the password if you skip: https://github.com/Dartvauder/NeuroSandboxWebUI/blob/main/TechnicalFiles%2Ffirst_setup.py#L66",2024-11-02 16:54:10,1
lv7x7hd,1gi0kns,"You can use a symlink to point to the files in the other directory, like a shortcut. Depending on the OS that you use, you can use the commands: 'ln' for unix and 'mklink' for windows to make the symlink.",2024-11-03 10:39:15,1
lv82244,1gi0kns,"I tried that, but it was not convenient, the way the program has its folders situated would not allow me to symlink with my existing folders and subfolders without breaking my existing folders and subfolders setup in ComfyUI and Koboldcpp. I ended up deleting the project altogether since the developer didn't seem to have that option in their development plan. I might revisit the project later.",2024-11-03 11:03:02,1
lv17tqs,1ghx3ud,Last time I checked some medical.LLMs for diagnosis it missed a lot of pathognomical cases.  Those are under represented in real.worlf because this is a given. Curious to see how google made their MDagents. Thanks for sharing,2024-11-02 08:56:08,3
lv0lndk,1ghwdjy,"AMD:

One exec looks at news. ""Wow, everyone is getting really excited over this AI stuff. Look how much Apple is touting it, even with huge margins... And it's all memory bound. Should I call our OEMs and lift our arbitrary memory restriction on GPUs? They already have the PCBs, and this could blow Apple away.""

Another exec is skeptical. ""But that could cost us..."" *Taps on computer*. ""Part of our workstation market. We sold almost *8* W7900s last month!""

*Room rubs their chins*. ""Nah.""

""Not worth the risk,"" another agrees.

""Hmm. What about planning it for upcoming generations? Our modular chiplet architecture makes swapping memory contollers unusually cheap, especially on our GPUs.""

""Let's *not* take advantage of that."" *Everyone nods in agreement.*",2024-11-02 06:48:47,348
lv1x0g8,1ghwdjy,"I bought a 128GB M4 max. Here’s my justification for buying it (which I bet many share), but the TLDR is “Because I Could.” I always work on a Mac laptop. I also code with AI. And I don’t know what the future holds. Could I have bought a 64GB machine and fit the models I want to run (models small enough to not be too slow to code with)? Probably. But you have to remember that to use a full-featured local coding assistant you need to run: a (medium size) chat model, a smaller code completion model and, for my work, chrome, multiple docker containers, etc. 64GB is sounding kind of small, isn’t it? And 96 probably has lower memory bandwidth than 128. Finally, let me repeat, I use Mac laptops. So this new computer lets me code with AI completely locally. That’s worth 5k. If you’re trying to plop this laptop down somewhere and use all 128GB to serve a large dense model with long context…you’ve made a mistake",2024-11-02 11:09:30,29
lv0j15j,1ghwdjy,"Probably gonna get one of these using the company budget. While the bandwidth is fine, the PP is still going be 4-5 times longer comparing to a 3090 apparently, might still be fine for most cases.",2024-11-02 06:31:59,33
lv0mhnc,1ghwdjy,"I'm glad Apple keeps pushing on MBW (and power efficiency) as well, but I wish they'd do something about their compute, as it really limits the utility. At [34.08 FP16 TFLOPS](https://www.cpu-monkey.com/en/igpu-apple_m4_max_40_core) and with the [current Metal backend efficiency](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/) the pp in llama.cpp is likely to be worse than an RTX 3050. Sadly, there's no way to add a fast-PCIe connected dGPU for faster processing either.",2024-11-02 06:53:58,25
lv1szdf,1ghwdjy,"It doesn't seem to make financial sense. A 128GB M4 Max is $4700. A 192GB M2 Ultra is $5600. IMO, the M2 Ultra is a better deal. $900 more for 50% more RAM, it's faster RAM at 800 versus 546 and I doubt the M4 Max will topple the M2 Ultra in the all important GPU score. M2 Ultra has 60 cores while the M4 Max has 40.

I rather pay $5600 for a 192GB M2 Ultra than $4700 for a 128GB M4 Max.",2024-11-02 10:48:01,22
lv2p761,1ghwdjy,"My credit card is already cowering in fear and my M1 Pro MacBook is getting its affairs in order.

as long as there isnt something terribly wrong with these, it's the do-it-all machine for the next 3 years",2024-11-02 13:40:39,6
lv0k3ck,1ghwdjy,Latest pc chip 4090 support 1001GB/s bandwidth and upcoming 5090 will have 1.5TB/s bandwidth. Pretty insane to compare mac to full spec gaming pc’bandwith,2024-11-02 06:38:54,46
lv58dnx,1ghwdjy,"Just tell me how many tokens/second you get for poplular LLMs like Qwen 72b, Llama 70B",2024-11-03 00:12:32,5
lv1pwvi,1ghwdjy,"TBH, 546GB is not that big.",2024-11-02 10:32:15,3
lv0opc9,1ghwdjy,"Comparing m4 MacBook Pro to a tower PC w/4090 is like comparing a sports car to a pickup truck. 

Additionally, if we want to compare in the laptop space I believe the m4 max has about the same gpu bandwidth as a 4080 mobile. Which granted the 4080 will be better at running models, however is way less power efficient , which last time I checked REALLY MATTERS with a laptop.",2024-11-02 07:07:38,9
lv0llti,1ghwdjy,"https://preview.redd.it/2h02mjg7thyd1.jpeg?width=692&format=pjpg&auto=webp&s=28ec616fbac85205e3901ced57e74977007ccdaa

M2 Ultra keeping toe at 800GB/s bandwidth, what if it was 500GB/s bandwidth?😝",2024-11-02 06:48:31,10
lv1580q,1ghwdjy,AMD has Strix Halo which has similar memory bandwidth,2024-11-02 08:41:47,5
lv1z156,1ghwdjy,"That’s so insane. Approximately, that’s the power similar to? T4, L4 or A100?",2024-11-02 11:20:17,2
lv2lpp7,1ghwdjy,"For a stupid person, does this make it a good laptop to potentially run 72B models? Even more?",2024-11-02 13:21:32,2
lv3sfsh,1ghwdjy,"Ok, a lot of people here are way smarter than me. Can someone explain whether a $5k build can run 3.1 70b. Also, what advantages does this have over, say, a train, which I could also afford?",2024-11-02 17:30:03,2
lv0qgcy,1ghwdjy,For what price?,2024-11-02 07:18:12,2
lv16jsr,1ghwdjy,"Honestly? I'm just waiting for Intel and/or AMD to do similar high bandwidth lpddr-5 tech for cheaper. It seems pretty good for medium sized models, small and power efficient, but also not really faster than dgpu. I think a combination of like a good mobile dgpu and lpddr-5 could be strong for running different models on each at a lowerish power draw, and in compact size and probably not terribly expensive in a few years. 

I'm glad apple pioneered it.",2024-11-02 08:49:06,2
lv3nu9s,1ghwdjy,"I am trying so hard to be patient for Mac Studio though. I cannot get M4 Max on mini which is strange because obviously that can be done but Apple decided against it. I suspect it's to help ""stagger"" their model lines carefully for their prices as not to make it so behind or too ahead in a given period of time. 

The rise of AI is definitely adding pressure on tech companies to produce faster chips. People want something that makes their lives easier and AI is one of them. We have always imagined AI but it's now becoming a reality and there is a pressure to continue to shrink silicon even smaller or come up with better building blocks to build faster cores. I am pretty sure that in a decade, we will have RAM that are not just ""buckets"" for bits but also have embedded cores to do calculations on a few bits for faster processing. That's what Samsung is doing now.",2024-11-02 17:02:12,2
lv0r6sa,1ghwdjy,Do you not notice it’s mainly the butt hurt broke people crying. I have both a 4090 and Mac. I solely use my 4090 for gaming. Also the new M4 max in compute it similar to a desktop 4060ti. And the new M4 ultra if scaling is consistent as it’s been with the M4 series chips should be very close to the desktop 4070ti. Now mind you in CPU it’s official apple have the best single core and multi core by a large margin compared to any cpu out there. Not to mention. I imagine compute FP32 teraflops to start increasing drastically from the next generation chips. Since apple are leading in single core and multi core,2024-11-02 07:22:35,0
lv0xngl,1ghwdjy,"Interesting to compare it with Ryzen AI Max 395 in context of performance per price. It is to expect will support 128Gb of unified memory with up to 96 for GPU. But memory not HBA, so slower.",2024-11-02 07:59:19,1
lv2pf2w,1ghwdjy,"I currently have a M1 Pro running some reasonably sized models. I was waiting the M4 release to upgrade.

I’m about to order an M4 Max with 128GB of memory.

I’m not (yet) heavily using AI in my daily work. I’m mostly running local coding copilot and code documentation. But extrapolating what I currently have with these new specs sounds exciting.",2024-11-02 13:41:50,1
lv2sou2,1ghwdjy,"At what point does it become useful for more than inference?

To me, even my M1 64GB is good enough for inference on decent size models - as large as I would want to run locally any way. What I don't feel I can do is fine tune. I want to have my own battery of training examples that I curate over time, and I want to take any HuggingFace or other model and ""nudge it"" towards my use case and preferences, ideally, overnight, while I am asleep.",2024-11-02 13:59:53,1
lv2ue8t,1ghwdjy,This is likely to make the M4 Ultra around 1.2TB/s memory bandwidth if fusing 2x chips or 2.4TB/s fusing 4x chips depending on how Apple plays out its next Ultra revision.,2024-11-02 14:09:26,1
lv4nq2w,1ghwdjy,"Does anybody know if you need the full 128gb for that speed? 

I'm interested in the 64gb option mainly because 128 is a full $800 more.",2024-11-02 20:55:52,1
lv5418h,1ghwdjy,"Hi everyone,

I have a question regarding the capability of the MacBook Pro M4 MAX with 128 GB RAM for fine-tuning large language model. Specifically, is this system sufficient to fine-tune LLaMA 3.2 with 3 billion parameters?

Best regards",2024-11-02 23:23:40,1
lv548n7,1ghwdjy,"I agree with OP it is really exciting to see what Apple are doing here. It feels like MLX is only a year old and is gaining traction - esp in local tooling, MPS backend compatibility and performance eg in PyTorch 2.5 advanced quite a way and, on the hardware level, matrix multiplication in the neural engine of the m3 was improved, I think there were some other specific improvements for ML as well. I would assume further for the m4 as well.

Seems like Apple investing in hardware and software/frameworks to get developers, enthusiasts and data scientists on board, also moving in the direction of on-device inference themselves plus some bigger
open source communities taking it seriously.. and a SoC architecture that kinda just works well for this specific moment in time. I have a 4070Ti Super system as well, and that’s fun, it’s quicker for sure for what you can fit in 16GB VRAM, but I’m more excited about what is coming  for the next generations of Apple silicon that the next few generations of (consumer) NVidia cards that might finally be granted a few more GB of VRAM by their overlords ;)",2024-11-02 23:25:53,1
lv9eyvg,1ghwdjy,i will wait for mac studio and 5090 pricing before i make a decision.,2024-11-03 15:16:02,1
lv16rvp,1ghwdjy,"I want one, but I think it's ""Apple marketing magic"" to a large degree.

A 3090 system costs $1200 and can run a 24b model quickly and get say a ""3"" in generalized potential. So far, CUDA is the gold standard in terms of breadth of applications.

A 128GB M4 costs $5000 can run a 100B slowly and get an 8.

A hosted model (OpenAI, Google, etc) cost is metered, it can  run a ??? huge model and gets 100.

The 3090 can do a lot of tasks very well, like translation, back-and-forth, etc.

As others have said, the M4 is ""smarter"" but not fun to use real time. I think it'll be good for background tasks like truly private semantic indexing of content, but that's speculative and will probably be solved, along with most use cases of ""AI,"" without having to use so much local RAM in the next year or two. That's why I'd call it Apple magic, people are paying the bulk of their cost for a system that will probably be unnecessary. Apple makes great gear, but a base 16GB model would probably be plenty for ""most people,"" even with tuned local inference.

I know a lot of people, like me, like to dabble in AI, learn and sometimes build useful things, but eventually those useful things become mainstream, often in ways you didn't anticipate (because the world is big). There's still value in the insight and it can be a hobby. Maybe Apple will be the worst horse to pick, because they'll be most interested in making it ordinary opaque magic, rather than making it transparent.",2024-11-02 08:50:20,0
lv0j09h,1ghwdjy,"they're comparing this to ""AI PC"", whatever that is

its still getting its ass whooped by a 4070",2024-11-02 06:31:50,-5
lv0ksao,1ghwdjy,"Lets compare speed of mac and 4090 running 7B-32B models, its day and night",2024-11-02 06:43:22,-1
lv0lcg3,1ghwdjy,https://preview.redd.it/vsty8zq4thyd1.jpeg?width=693&format=pjpg&auto=webp&s=dad75e676cc2b8466e82b0877ba0773b5f76ba29,2024-11-02 06:46:53,-5
lv0o5f1,1ghwdjy,The only way that the absurd decisions AMD management continues to take makes sense is if they are secretly holding NVDA stock. Bunch of nincompoops.,2024-11-02 07:04:14,176
lv0vy0m,1ghwdjy,"Morpheus: what if i told you... that the AMD and the NVidia CEOs are cousins...

(not joking, google it)",2024-11-02 07:49:39,51
lv2wojd,1ghwdjy,AMD eating ass right now. Almost as bad as Intel. AMD need to wake up and go heavy on VRAM.,2024-11-02 14:22:31,5
lv0salv,1ghwdjy,12 CHANNEL APU NPU+GPU !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!,2024-11-02 07:29:00,5
lv2npdl,1ghwdjy,"""Hey guys, I'm your new manager. I'm going to write up every single one of you unless you break up this unauthorized meeting and get back to work. There will be **NO critical or intelligent thinking** on my watch. How DARE you suggest a way to make this company more profitable. What kind of company do you think you work for???? Praise Satan.""",2024-11-02 13:32:27,2
lv4xtgq,1ghwdjy,A while back i just got to know that Jensen and Lisa su are cousin. Not saying that it can be the reason but not not saying that either.,2024-11-02 22:20:20,1
lv5d49v,1ghwdjy,"Strix halo pro , desktop version, whatever they called it , is limited to a maximum of 96GB igpu memory right?",2024-11-03 01:06:57,1
lv1zifp,1ghwdjy,"Strix Halo will have 500gb bw, and is literally around the corner.",2024-11-02 11:22:52,1
lv2eyit,1ghwdjy,"Nice story you have ~~hallucinated~~ generated here. Do you have the character card for generating more of these? :)

Just kidding. But also sad.",2024-11-02 12:44:54,1
lv4y1ie,1ghwdjy,This guy is ready for llama-4 405B q3 release.,2024-11-02 22:22:24,7
lv1zwh3,1ghwdjy,"What models are you using / plan to use for coding (for code completion and chat)?

Is there truly a setup that would even come close to rival using o4-mini / Claude Sonnet 3.5?

Also, if you could, please do share what quantization level you anticipate to be able to go with on the M4 Max 128 GB for code completion / chat. I'm guessing you'll be going with MLX-versions of whatever you end up using.

Thanks.",2024-11-02 11:24:58,11
lv71bhn,1ghwdjy,"I’m exactly in your situation, and I came up to the exact same conclusion. Also I work in AI, so being able to do whatever locally is really powerful. I thought about having another linux computer on home network with gpus and all, but VRAM is too expensive that way (more hassle and money for a worse overall experience).",2024-11-03 08:00:20,1
lv8xn94,1ghwdjy,"No. I beat all your local models with API calls to Anthropic and OpenAI (or Openrouter) and rely and bet on their privacy and terms policy that my data is not reused by them. With that I have 5K to burn in API calls which beat your local model every time.

I think if you really want to get serious with on premise AI and LLM you have to chip in 100-150K into a Nvidia midsize workstation and then you really have something on same levels with current tech from the big players. On a 5-8K MacBook you are running behind by 1-2 generations minimum for sure.",2024-11-03 13:44:47,1
lv0tzyb,1ghwdjy,"Some backends can set a really large PP batch size, like 16K. IIRC llama.cpp defaults to 512, and I think most users aren't aware this can be increased to speed it up.",2024-11-02 07:38:36,12
lv0sywm,1ghwdjy,What is PP?,2024-11-02 07:32:47,8
lv0spu9,1ghwdjy,Longer PP is fine in most of the cases,2024-11-02 07:31:22,11
lv2n6m7,1ghwdjy,"I can attest to this. The time to first token is unusably high on my M4 iPad Pro (\~30 seconds to first token with llama 3.1 8B and 8 gb of ram, model seems to fit in ram), especially with slightly used-up context windows (with a longish system prompt).",2024-11-02 13:29:35,1
lv3f11d,1ghwdjy,"Is it theoretically possible to do the prompt processing on one system (e.g. a PC with a single decent GPU) and then have the model running on a Mac? I know the prompt processing bit is normally GPU bound, but am not sure how much data it generates - might be that moving that over a network would be too slow and it would be worse.",2024-11-02 16:09:51,1
lv0vnr7,1ghwdjy,This is why I am interested to see how Apple have dealt with the software side of it. On paper it should be 4-5 times longer but will it be?,2024-11-02 07:48:01,1
lv11m0s,1ghwdjy,"Also, this is only correct on a native basis.",2024-11-02 08:21:32,0
lv22frl,1ghwdjy,"One is portable the other isn’t. 
Choose whichever suits your lifestyle.",2024-11-02 11:38:13,22
lv4lvon,1ghwdjy,"I spent around $4,475 on 4x3090, ROMED8-2T with 7 PCIe slots, EPYC 7F52 (128? lanes), 32GB DDR4 RDIMM, 4TB m.2 nvme, 4x PCIe risers, Super Flower 1,600w PSU, and Dell server PSU with breakout board (a $25 deal given to me by an ex crypto miner).

1) log into the server from my macbook via Remote Desktop

2) load up Oobabooga

3) go to URL on local machine (192.168.1.99:7860) 

4) and bob's your uncle",2024-11-02 20:41:44,4
lv5cz1e,1ghwdjy,"Don’t want to be a karen but the top of the line M2 ultra has 76 GPU cores, nearly double what the M4 max has",2024-11-03 01:05:15,2
lv3n3s0,1ghwdjy,"Use debit card, they are brave and fearless.",2024-11-02 16:57:45,7
lv3qy4m,1ghwdjy,"I'm going to get one, and it's going to replace a 2019 Intel i9 MacBook Pro. That's going to be glorious.",2024-11-02 17:20:57,5
lv0tusy,1ghwdjy,"You can’t have 128GB VRAM on your 4090, can you? 

That’s the entire point here - Macs have fast unified memory that can be used to run large LLMs at acceptable speed and spend less money than an equivalent GPU setup. And don’t act like a space heater.",2024-11-02 07:37:48,68
lv0qj0y,1ghwdjy,Still would rather get a 128gb mac than buy the same amount of 4090s and also have to figure out where I'm going to put the rig,2024-11-02 07:18:38,28
lv2envl,1ghwdjy,"Mobile RTX 4090 is limited to 16GB of 576GBs memory.

https://en.wikipedia.org/wiki/GeForce_40_series

Pretty insane to compare full spec gaming **desktop** to a mac **laptop**",2024-11-02 12:43:17,3
lv0ndu0,1ghwdjy,"What does the PCIE bus its plugged into support? That’s your actual number, otherwise its just bottleneck.",2024-11-02 06:59:31,2
lv0n3ll,1ghwdjy,"1792 GB/s for the 5090, not TB/s",2024-11-02 06:57:47,-10
lv81uny,1ghwdjy,"This, and time to first token, would be really interesting to know.",2024-11-03 11:02:00,1
lv20p71,1ghwdjy,"It's not that big, but the ability to get 128gb or more memory capacity with it is what makes it a big deal.",2024-11-02 11:29:08,6
lv0z1rx,1ghwdjy,"Does is?
Most people running powerful GPUs on laptops don't care about efficiency anyways, they just have use cases that a Mac can't achieve yet.",2024-11-02 08:07:16,12
lv0twpd,1ghwdjy,"All true, I have such a laptop - I took it away from my working desk a grand total of three times this year and never ever used it without a power cord.

I still wish there'd be a Nvidia laptop GPU with more than 16 GB VRAM.",2024-11-02 07:38:06,1
lv0ow43,1ghwdjy,[deleted],2024-11-02 07:08:47,15
lv15akc,1ghwdjy,"Training is done in high-precision, and with high parallelism, good luck training more than some end-of-semestre school project on a single 4090; the comparison it pointless",2024-11-02 08:42:10,9
lv1c7yj,1ghwdjy,"That has many details to be examined, including actual performance. So, mid 2025, maybe.",2024-11-02 09:20:19,2
lv61dlk,1ghwdjy,"has -> will have next year when it's available. launching at CES so based on experience a coupe of month later

similar -> half at about 273GB/s with 256bit@8533MT/s",2024-11-03 04:21:49,1
lv27h4o,1ghwdjy,I don't know why people are surprised by this. The M Ultras have been more than this for years. It's no where close to an A100 for speed. But it does have more RAM.,2024-11-02 12:04:33,5
lv155f9,1ghwdjy,"$4699

https://preview.redd.it/rsov6pekdiyd1.jpeg?width=1179&format=pjpg&auto=webp&s=72eabd6323c3ae9ffa530dd94a1eda67c69dd2bb",2024-11-02 08:41:23,6
lv20h4b,1ghwdjy,"> I'm glad apple pioneered it.

Apple didn't really pioneer it. AMD has been doing this with console chips for a long time. PS4 Pro for instance had 600gb bandwidth back in 2016 way before Apple.

AMD also has an insane mi300A APU with like 10 times the bandwidth (5.3 TB/s), but it's only made for the datacenter. 

AMD makes whatever the customer wants. And as far as laptop OEMs are concerned they didn't ask for this until Apple did it first. But that's not a knock on AMD, but on the OEMs. OEMs have finally seen the light, which is why AMD is prepping Strix Halo.",2024-11-02 11:27:58,3
lv1i1xy,1ghwdjy,"Where are you getting that the M4 has the best single and multi core speeds? 

Benchmarks put the M4 at about the same as the 14900k, which is a huge improvement but they're only a few percentage points off, and they for sure don't have the best multi core performance, as the M4 pro doesn't have the same or higher number of cores.",2024-11-02 09:51:30,2
lv2ncm0,1ghwdjy,Im waiting for amd strix halo as well. I need linux for my other needs,2024-11-02 13:30:30,1
lv5vu1i,1ghwdjy,"They had plan for M2 Extreme in the Mac Pro format which is essentially 2xM2 Ultra that has 1.6384TB/s. If they also make M4 Extreme this gen, then it will have 2.184448TB/s.",2024-11-03 03:35:17,1
lv0jlsp,1ghwdjy,"Sure. Because a 4070 has 128GB Vram. Indeed

Running LLM on Apple:  It runs, at reasonable spead

Running LLM on 4070:  CUDA of out memory. Exit();

The only thing you can compare this to is a Quad-3090 setup. That would have 96GB of VRAM and be quite a bit faster than the M4 Max. However it also involves getting a motherboard with 4 PCIe slots, and consume up to 1.4kW for the GPU's alone. Getting 4x 3090's + Workstation mobo+cpu would also still cost 4x $600 + $1000 for getting second hand stuff.",2024-11-02 06:35:45,43
lv0mdxl,1ghwdjy,"> whatever that is

>> [Intel, on its website, has taken a more general approach](https://www.tomshardware.com/laptops/what-is-an-ai-pc):  ""An AI PC has a CPU, a GPU and an NPU, each with specific AI acceleration capabilities.""

>> AMD, via a staff post on its forums, has a similar definition: ""An AI PC is a PC designed to optimally execute local AI workloads across a range of hardware, including the CPU (central processing unit), GPU (graphics processing unit), and NPU (neural processing unit).""",2024-11-02 06:53:20,5
lv0md0x,1ghwdjy,"No one buys a 128 GB machine at the price of a used car to run a 7B model...  
You would run Mistral Large or Deepseek 2.5.",2024-11-02 06:53:11,23
lv0m1ge,1ghwdjy,"Let’s compare a 70B Q8 model, or Mistral Large …",2024-11-02 06:51:14,19
lv0vzmy,1ghwdjy,"Indeed, but lets also compare this mac and the 4090 running 70B and above.",2024-11-02 07:49:54,4
lv0r24r,1ghwdjy,Someone should explain shorting to the AMD board of directors.,2024-11-02 07:21:48,54
lv1pxpb,1ghwdjy,AMD just exists for NVIDIA to avoid antitrust scrutiny,2024-11-02 10:32:22,31
lv0w1zw,1ghwdjy,or maybe the AMD and the NVidia CEOs are somehow family relatives?? i mean... no way in hell that...,2024-11-02 07:50:17,67
lv142z7,1ghwdjy,AMD has been actively sabotaging the non-CUDA GPU compute market for literal decades by now.,2024-11-02 08:35:20,29
lv1kvdy,1ghwdjy,Isn't the owner the cousin of the Nvidia owner?,2024-11-02 10:06:01,8
lv1tiea,1ghwdjy,"Well, Jensen’s cousin does run AMD.",2024-11-02 10:50:47,8
lv3ckc8,1ghwdjy,"Ever wonder why Lisa Su got the job? I wonder what the relation is to Jensen, hmmmm....",2024-11-02 15:55:34,3
lv17cjw,1ghwdjy,"How can you expect, from a small company who has been dominating in CPU markets, both gaming and server last couple of years, to be dominator also in the GPU markets? They had nothing 7 years ago, now they have super CPUs and good gaming GPUs. Its just their software which lacks in llm. NVIDIA does not have CPUs, INtel does not have anymore anything, but AMD has quite good shit. And their new Strix HALO is a straight competitor for M4.",2024-11-02 08:53:30,6
lv2hxrc,1ghwdjy,Are they even allowed to hold NVDA stock as AMD execs? It feels like insider trading,2024-11-02 13:01:00,1
lv8ug43,1ghwdjy,All are owned by Blackrock and Vanguard.,2024-11-03 13:27:52,1
lv10qdg,1ghwdjy,No way!,2024-11-02 08:16:39,10
lv1a3bo,1ghwdjy,I did not know this. That's a crazy TIL,2024-11-02 09:08:35,4
lv1yjh1,1ghwdjy,"Depends on where you from. These are Asian cousins, competitive as fuck.",2024-11-02 11:17:42,5
lv49whj,1ghwdjy,This just fucked me up.,2024-11-02 19:20:18,2
lv2xign,1ghwdjy,"Intel has internal political problems, delay problems, software fragmentation problems, financial problems. I *almost* feel for them. They can't just spawn a good inference card like AMD can (a 32GB clamshell Arc A770 would be kinda mediocre, if that's even possible, and a totally new PCB).

AMD has... well, nothing stopping them? Except themselves.

Sure they have software issues, but even if they don't lift a single finger, a W7900 without the insane markup would sell like hotcakes.

And if they swap the tiny memory controller die on the 7900, which they could totally pull off next year, and turn around and sell 96GB inference cards? Or even more? Yeah, even with the modest compute of the 7900...",2024-11-02 14:27:22,10
lv2q2e9,1ghwdjy,Dwight?,2024-11-02 13:45:22,1
lv26k5z,1ghwdjy,"That's read + write.

The actual read bandwidth estimate is like 273 GB/s, from 256-bit LPDDR5x 8533. Just like the M4 Pro.

But it should get closer to max theoretical performance than Apple, at least.",2024-11-02 11:59:45,8
lv595jb,1ghwdjy,I’m hoping for the Bitnet,2024-11-03 00:21:25,3
lv2dqbs,1ghwdjy,"I won't know which models to use until I run my own experiments. My knowledge on the best local models to run is at least a few months old, as my last few projects I was able to use Cursor. I don't think any truly local setup (short of having your own 4xGPU machine as your development box) is going to compare to the SoTA. In fact, it's unlikely there are any open models at any parameter size as good as those two. Deepseek Coder may be close. That said, some things I'm interested in trying to see how they fair in terms of quality and performance are:  
Qwen2.5 family models (probably 7B for code completion and a 32B or 72B quant for chat)  
Quantized Mixtral 8x22B (maybe some more recent finetunes. MoEs are a perfect fit for memory rich and FLOPs poor environments...but also why there probably won't be many of them for local use)

What follows is speculation from some things I've seen around these forums and papers I've looked at: For coding, larger models quantized down to around q4 tend to give the best performance/quality trade offs. For non-coding tasks, I've heard user reports that even lower quants may hold up. There are a lot of papers about the quantization-performance trade off, here's one focusing on Qwen models, you can see q3 still performs better in their test than any full precision smaller model from the same family. [https://arxiv.org/html/2402.16775v1#S3](https://arxiv.org/html/2402.16775v1#S3)

ETA: Qwen2.5 32B Coder is ""coming soon"". This may be competitive with the latest Sonnet model for coding. Another cool thing enabled by having all this RAM is creating your own MoEs by combining multiple smaller models. There are several model merging tools to turn individual models into experts in a merged model. E.g. [https://huggingface.co/blog/alirezamsh/mergoo](https://huggingface.co/blog/alirezamsh/mergoo)",2024-11-02 12:38:12,15
lv0wxil,1ghwdjy,"How much faster does it really go? I recall a comparison back in the 4k context days, where going 128 -> 256, 256 -> 512 were huge jumps in speed, 512->1024 was minor and 1024 -> 2048 was basically zero difference. I assume that's not the case anymore when you've got up to 128k to process, but it's probably still somewhat asymptotical.",2024-11-02 07:55:15,9
lv0t4jj,1ghwdjy,"Prompt processing, how long it takes until you see the first token being generated.",2024-11-02 07:33:42,22
lv12f73,1ghwdjy,"PPEEZ NUTS!

Hah! Got'em!",2024-11-02 08:26:05,5
lv1b0q2,1ghwdjy,*unzips dick*,2024-11-02 09:13:42,4
lv14kx0,1ghwdjy,"It's not how long your PP is, it's how you use it.",2024-11-02 08:38:10,14
lv24xg3,1ghwdjy,The problem with that portability is a lower thermal profile. People with M Maxi in Macbook form complained about thermal throttling. You don't have that problem with a Studio.,2024-11-02 11:51:09,3
lv4m0rb,1ghwdjy,IPMI and M$ RDP—or ssh me@ 192.168.1.99 ftw.,2024-11-02 20:42:46,1
lv5jyk9,1ghwdjy,"This is what I needed to see, thanks for the cost breakdown and input. I basically do this now with a far inferior setup(single 3080ti and an AMD CPU that I remote in from my mbp to play around with current AI stuff and so on), but I’m more a hobbyist anyways and was wanting to upgrade so it’s nice to be given an idea for a pathway that’s not walking into apples garden of minimal options and hoping for the best.",2024-11-03 01:28:06,2
lv7pn84,1ghwdjy,"Yeah, but the 72 core model costs more. Thus biting into the value proposition. The 60 core model is already better than a M4 Max.",2024-11-03 10:02:07,2
lv0vckh,1ghwdjy,"It's mad when you think about it, packed into a notebook.",2024-11-02 07:46:14,29
lv0yny5,1ghwdjy,">can be used to run large LLMs at acceptable speed

ehhhhh...   ""acceptable"" for small values of ""acceptable.""   What are you really getting out of a dense 128GB model on a macbook?  If you can count the t/s on one hand and have to set an alarm clock for the prompt processing to complete, it's not really ""acceptable"" for any productivity work in my book  (e.g. any real-time interaction where you are on the clock, like code inspection/code completion, real-time document retrieval/querying/editing, etc.)  Sure it kinda ""works"", but it's more of a curiosity where you can submit a query, context switch your brain, and then come back some time later to read the full response.  Otherwise it's like watching your grandma attempt to type.  Furthermore, running LLM's on my macbook is also the only thing that spins the fans at 100% and drains the battery in < 2 hours  (power draw is \~ 70 watts vs. a normal 7 or so).

Unless we start seeing more 128gb-scale frontier-level MOE's, the 128gb vram alone doesn't actually buy you anything without the proportionate increases in processing+MBW that you get from 128GB worth of actual GPU hardware, IMHO.",2024-11-02 08:05:08,24
lv0v66y,1ghwdjy,"This is it, huge amount of energy use as well for the VRAM.",2024-11-02 07:45:13,19
lv0r4kq,1ghwdjy,"Same. I could buy a single 5090, but nothing beyond this. More than a single GPU is ridiculous for personal use.",2024-11-02 07:22:13,11
lv0r5ct,1ghwdjy,"Not same amount one 4090 is stronger.
Its not just about the amount of of memory you get.
You could build a 128gb 2080 and it would be slower than a 4090 for ai",2024-11-02 07:22:21,2
lv17451,1ghwdjy,You'll have plenty of time to consider where the proper computer could have gone while you're waiting for your mac to preprocess a few thousand tokens.,2024-11-02 08:52:13,1
lv18bkv,1ghwdjy,"They are taking about the bandwidth of the VRAM so from the gpu memory to the actual processor itself.
Once you've loaded the entire model the PCIe bottleneck is no longer an issue.",2024-11-02 08:58:51,2
lv0ox38,1ghwdjy,Silly billy,2024-11-02 07:08:57,7
lv0zotm,1ghwdjy,VeryLazyThinker I guess.,2024-11-02 08:10:51,1
lv2aqsh,1ghwdjy,"but would it be faster than bunch of P40, I don't know honestly",2024-11-02 12:21:59,2
lv159o6,1ghwdjy,"windows people seem to forget they can use their laptop unplugged, regardless of the task",2024-11-02 08:42:02,0
lv2rslm,1ghwdjy,They make docks and eternal GPU hookups.,2024-11-02 13:54:54,2
lv2s975,1ghwdjy,bottom mark is code assistant.,2024-11-02 13:57:28,7
lv2059c,1ghwdjy,"It's launching at CES, and it should be on shelves in Q1.",2024-11-02 11:26:17,4
lv1pdwj,1ghwdjy,"Can I put linux on it?

I already know two OS, I don't have the brain power to learn a third.",2024-11-02 10:29:28,5
lv1p7c4,1ghwdjy,"The M4 Max blows away the 14900k when comparing their respective production configurations.

14900k: https://browser.geekbench.com/processors/intel-core-i9-14900k

M4 Max: https://browser.geekbench.com/v6/cpu/8615848",2024-11-02 10:28:31,3
lv0uc34,1ghwdjy,"and i thought we were talking about memory performance?

you either choose Mac for mem size, or GPUs for performance. both cripple the other parameter.",2024-11-02 07:40:30,7
lv0pkpa,1ghwdjy,"Realistically you aren't going to want to allocate greater than 112-120GB of your wired\_limit to VRAM w/ and M4 Max, but I think the question will also be what you're going to run on it considering how slow prefill is. Speccing out an M4 Max MBP w/ 128GB RAM is about $6K. If you're just looking for fast inference of a 70B quant, 2x3090s (or 2xMI100) will do it (at about $1500 for the GPUs). Of course, the MBP is portable and much more power efficient so there could be situations where it's the way to go, but I think that for most people, it's not the interactive bsz=1 holy grail they're imagining, though.

Note: with llama.cpp or ktransformers, you can actually inference at pretty decent speed with partial model offloading. If you're looking at workstation/server-class hardware, for $6K you can definitely be looking at used Rome/Genoa setups with similar-class memory-BW and the ability to use cheap GPUs even purely for compute (if you have a fast PCIe slot, try running llama-bench at -ngl 0 and see what your pp you can get, you might be surprised).",2024-11-02 07:12:55,3
lv15qt0,1ghwdjy,"it’s like saying two H100 will be faster, people are delusional",2024-11-02 08:44:40,1
lv17l7r,1ghwdjy,This all hinges on what you call reasonable speed.,2024-11-02 08:54:50,1
lv0pldi,1ghwdjy,"That's not a real world power number.  I have 2x 250 watt GPUs.  At idle the whole system is 35 watts.  During inferencing it is more like 180 watts as I turned the cards down to 165 watts.  

Most are not running a LLM 24/7.  Sure some do, but it is not my experience.  I'm sure the m4 will use less power at idle and while active.

What does that system cost?  I would guess between $6k and 10k.  That buys a lot of GPUs.

Don't get me wrong, I'd love one of those apple systems.    I just don't want to pay the Sample entry fee.",2024-11-02 07:13:02,-1
lv0lvdp,1ghwdjy,"Sir, How many tokens/s it will give for llama 403B models?",2024-11-02 06:50:12,-5
lv0kggw,1ghwdjy,"Not at reasonable speed, we call it tortoise speed. Waste of time. Just A Gimmick",2024-11-02 06:41:15,-8
lv1qh9s,1ghwdjy,"I'm currently getting like 2 **s/t** on mistral large, so this might be worth the price...",2024-11-02 10:35:09,1
lv0ogbb,1ghwdjy,[deleted],2024-11-02 07:06:06,6
lv0mm4t,1ghwdjy,And Llama 403B model too? If mac chip is 1/4th of speed running 7B -34B model then we should not compare bigger models.,2024-11-02 06:54:45,-1
lv1ijct,1ghwdjy,How else do you think they're making any money?,2024-11-02 09:54:03,16
lv312s4,1ghwdjy,I thought AMD just exists for Intel to avoid antitrust scrutiny,2024-11-02 14:47:59,13
lv15rqy,1ghwdjy,"I did not know that lol

What a world",2024-11-02 08:44:49,21
lv1exyu,1ghwdjy,"Right... but these are public companies and are accountable to shareholders. If AMD really was being tanked by the CEO's familial relations, they wouldn't be CEO for much longer.",2024-11-02 09:34:52,8
lv1zpx4,1ghwdjy,> Developers ~~AMD has~~ have been actively sabotaging the non-CUDA GPU compute market for literal decades by now.,2024-11-02 11:24:00,-2
lv1bof7,1ghwdjy,Well that small cpu company did buy a gpu company... ATI. And their vision was supposed to have been something like the m-series chips with unified memory as a part of that. It's wild that Apple beat them to the punch when it was supposed to have been their goal more than a decade ago.,2024-11-02 09:17:18,26
lv28hym,1ghwdjy,"Um, these boneheaded business decisions have absolutely nothing to do with their software, or their resource limitations.

Neither hardware/software has to be *great*. AMD doesn't have to lift a finger. They just need *a* 48GB GPU for like $1K, aka a single call to their OEMs, and you'd see developers move mountains to get their projects working. It would trickle up to the MI300X.",2024-11-02 12:09:54,12
lv19gv3,1ghwdjy,But without the tooling needed to compete against MLX or CUDA. Even Intel has better tooling for ML and LLMs at this stage. Qualcomm is focusing more on smaller models that can fit on their NPUs but their QNN framework is also pretty good.,2024-11-02 09:05:09,5
lv2lb0v,1ghwdjy,_Lisa’s mom: Look at your cousin.. his company is valued at trillion dollars_,2024-11-02 13:19:22,15
lv3e593,1ghwdjy,"If only.


Ryzen was by the previous CEO. Everything after... Is just flavors of what was done before.


Zero moves to actually usurp the market from Nvidia. Why doesn't she just listen to GeoHot and get their development on track? Man's offering to do it for free!


So forgive me for being suspicious.",2024-11-02 16:04:45,1
lv3ki3z,1ghwdjy,"Yes, I was referring to AMD not Intel. I edited it to make clear",2024-11-02 16:42:07,1
lv3sfd6,1ghwdjy,I just want someone to push for 4+ channel RAM with regular desktop cpus. Just tell some story how it's essential for *serious* gaming and a smooth experience in excel. Or sell it using some integrated graphics. That would mean like dirt cheap 256 GB at 200 GB/s or something. I think this is the only way for easily affordable home-inference with serious LLM sizes. All that GPU processing power for such things is kind of a joke anyway. Afaik they're almost sleeping during non-batched inference.,2024-11-02 17:29:59,1
lv16fza,1ghwdjy,"I haven't tested llama.cpp in awhile, but going past even 2048 helps in exllama for me.",2024-11-02 08:48:32,2
lv0woap,1ghwdjy,Why such large differences in PP time?,2024-11-02 07:53:47,5
lv1cvd5,1ghwdjy,o1 approves,2024-11-02 09:23:51,2
lv3yzqm,1ghwdjy,"Experienced that with the M3 Max MBP. Mistral Large 4bit MLX was running fine at ~3.8 t/s.  When trottling, it went to 0.3 t/s.  Didn't experience that with Mac Studio.",2024-11-02 18:10:31,6
lv2g73b,1ghwdjy,"I own a 14 inch M2 Max MBP and I have to see it throttle because of using an LLM. I also game on it using GPTK and while it does get noisy it doesn't throttle.

> You don't have that problem with a Studio

You can't really work from an  - hotel room / airplane / train - with a Studio either.",2024-11-02 12:51:38,6
lv6vcfm,1ghwdjy,"Hobbyist here as well. My gut feeling tells me there is money to be made from LLMs and they can improve the quality of my life. I just need to figure out ""how?"".


So when you're in the market for 3090s, go with Facebook Marketplace first. I found three of my 3090s on there. An ex-miner was selling his rig and gave me a deal because I told him this was for AI. 

And this is why I'm getting an M4 Pro with only 48GB...I plan to fine tune a smaller model (using the 3090 rig) that will hopefully fit on the 48GB of RAM.",2024-11-03 07:29:53,1
lv5ou9p,1ghwdjy,... without a fan,2024-11-03 02:23:53,1
lv1be7l,1ghwdjy,"I'm guessing this will be >10 t/s, a fine inference speed for one person. To get the same VRAM with 4090s would require hiring an electrician to install circuits with enough amperage.",2024-11-02 09:15:46,8
lv0wbp7,1ghwdjy,[deleted],2024-11-02 07:51:49,-7
lv0weu3,1ghwdjy,"> Its not just about the amount of of memory you get.

It is if you can't fit the model into memory.",2024-11-02 07:52:19,10
lv0rlr3,1ghwdjy,I already run a 3090 and know how fast the speed difference is but real world use it's not like I'm going to care about it unless it's an obvious difference like with stable diffusion,2024-11-02 07:25:01,2
lv18ihj,1ghwdjy,Hum no I think the 2080 with 128GB would be faster on a 70b or 105b model. It would be a lot slower though on a small model that fits in the 4090.,2024-11-02 08:59:55,1
lv22zj3,1ghwdjy,"Ah fair, misunderstood the context my b",2024-11-02 11:41:06,2
lv2r735,1ghwdjy,...it's in a thin portable laptop that can run on a battery,2024-11-02 13:51:35,1
lv5hf5t,1ghwdjy,"When you say ""windows people x"" it reminds me how tribal and tech ignorant ""mac people"" are...

You do realize there are windows laptops with more performance and battery life?",2024-11-03 01:58:11,2
lv2sak6,1ghwdjy,"Indeed! I'm eyeing out a few, but can't pull the trigger yet. Nothing that'd make me go ""wow, I need it right now""",2024-11-02 13:57:41,2
lv22jhh,1ghwdjy,"Fingers crossed it'll be great then! Kinda sad that ""great"" is mid-range 2023 Mac, but I'll take it. It would be really disappointing if AMD overprices it.",2024-11-02 11:38:46,3
lv1sic0,1ghwdjy,"For what it's worth, macOS is a *NIX under the hood ([Darwin](https://en.wikipedia.org/wiki/Darwin_\(operating_system\)) is distantly descended from BSD). If you are coming at it from a command line perspective, there aren't a huge number of differences versus Linux. The GUI is different, obviously, and the underlying hardware architecture these days is ARM rather than x86, but these are not insurmountable in my experience as someone who pretty regularly jumps between Windows and Mac (and Linux more rarely).",2024-11-02 10:45:35,6
lv1prk1,1ghwdjy,"That's interesting, other benchmarks don't tell the same story. Just look at stuff like cinebench. I don't think it's fair to base it on a single benchmark, but it's definitely interesting that one has it so far ahead",2024-11-02 10:31:27,1
lv16zk2,1ghwdjy,"Not on Apple, thats the whole point I think. You get lots (128GB) memory at reasonable (500GB/s) performance. Of course its expensive, but your only other realistic alternative is a bunch of 3090's (if you want to run a 70B model at acceptable performance)",2024-11-02 08:51:31,8
lv14nqz,1ghwdjy,"Nah, you can get it for $4699.  14” macbook pro

https://preview.redd.it/ll29pnd1diyd1.jpeg?width=1179&format=pjpg&auto=webp&s=aed90a31eb389fa0b01fb3ecadfb215bff44cfef",2024-11-02 08:38:38,6
lv0spie,1ghwdjy,Where are you getting $6k from? I just bought one with 128GB and even with sales tax it wasn’t that much.,2024-11-02 07:31:19,5
lv16jkb,1ghwdjy,"Is it? I'm probably comparing things of similar price here... For home users wanting to run 70b models at useful speed, Apple or a bunch of 3090 are your only realistic options",2024-11-02 08:49:04,3
lv17b68,1ghwdjy,"You need to start running mlc-llm with tensor parallel, man. It utilises all your GPU's at full blast to actually give a speed up  (instead of llama-cpp only using a single gpu at a time and total speed becoming the speed of your slowest GPU)

With 2 GPU's,  mlc is about twice as fast as llama-cpp, but yeah it also consumes twice the power",2024-11-02 08:53:18,2
lv14t6j,1ghwdjy,"$4699

https://preview.redd.it/ryrl7288diyd1.jpeg?width=1179&format=pjpg&auto=webp&s=2893da2855a0e5d46ec83199059c039bb9879286",2024-11-02 08:39:29,3
lv0ozma,1ghwdjy,"I would expect dual 90 series cards to shine. I’ll see if I can spin up Mistral Large and post MLX numbers in a bit.

Edit: Eh, I’ll pass on redownloading it. Darn you data caps!",2024-11-02 07:09:23,2
lv12k5m,1ghwdjy,[deleted],2024-11-02 08:26:51,2
lv0xdce,1ghwdjy,What about a single 3090 with partial offloading on DDR5? These OOM values are not at all helpful.,2024-11-02 07:57:44,0
lv0nj5e,1ghwdjy,"The point is you can’t fit the larger models in a 4090 without extremely quants that render the models useless. I don’t take sides because I have a 4090 build and an M3 128GB machine. There are benchmarks, and there is the speed at which the models really print. When you sit and use both platforms for standard chat workflow, the difference is not as stark as people are led to believe. I rarely run LLM workflows on the 4090 as a result of my real world use cases. The M3 is perfectly capable. On the other hand, for image generation it really is a night and day difference. And so the 4090 is strictly used for that.",2024-11-02 07:00:25,13
lv7p7jv,1ghwdjy,"By crashing Intel on the CPU market, maybe?

To be fair, most of Intel's problems come from their internal hiccups and bad decisions, but that wouldn't change much for AMD if they couldn't exploit their weaknesses.",2024-11-03 10:00:02,1
lv7028w,1ghwdjy,"If I recall it was actually because IBM did not want to be bound to one supplier for such a vital technology, so as a concession Intel gave AMD an X86 license.",2024-11-03 07:53:50,1
lv21bys,1ghwdjy,"OMG LOL!!!

Mein freund, you forgot the /s...",2024-11-02 11:32:26,13
lv1rh88,1ghwdjy,All it would take is plausible deniability.,2024-11-02 10:40:15,8
lv3cp5w,1ghwdjy,"Explain Boeing, Ubisoft, EA, etc.


Fact is, they can get away with it for much longer than they should be.",2024-11-02 15:56:21,5
lv59cbx,1ghwdjy,For ages AMD never had entry non- or CUDA GPU compute product. NVDA always had.,2024-11-03 00:23:39,2
lv1rhiu,1ghwdjy,"It does not matter what they bought or what their vision was. Does Apple have 7900 xtx gaming level GPUs? If AMD didnt succeed in niche GPU markets, they succeeded in gaming CPUs, GPUs, and server CPUs. Thats quite a lot from underdog, maybe there can be some areas where they are not the market leaders. Nvidia has been focusing only in GPUs last 10 years, of course they are leaders in that area.",2024-11-02 10:40:18,-3
lv2f6cy,1ghwdjy,Already 48gb of ddr6 costs more.,2024-11-02 12:46:04,-1
lv1r937,1ghwdjy,"The reason NVIDIA has such a massive moat is because corporations are pathologically inclined to pursue short term profit over long term success.

CUDA didn't make fuckall for money for a solid 20 years, until it did. And by then, every other company was 20 years behind, because they couldn't restrain themselves from laying off that one department that was costing a lot of money to run and didn't have any immediate short term payoff. 

There were dozens of attempts by other companies to make something like CUDA. They all had a lifespan of about 2 years before corporate pulled the plug, or at best cut things down to a skeleton crew.

The other companies learned absolutely nothing from this, of course.",2024-11-02 10:39:06,14
lv2ok5f,1ghwdjy,"I read this in Steven He's dad's voice 🤣 Now I'm imagining her mom going ""Failure!""",2024-11-02 13:37:10,8
lv12t2c,1ghwdjy,"It's just how fast the GPU is, you can check how fast their FP32 are, and then estimate the INT8. Some GPU architecture might have more than double speed going down in bitwidth, but as Apple didn't mention it I would assume no for now. 

For reference, from here:  
[https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference)

For Llama 8B Q4\_K\_M, PP 512 (batch size), it is 693 for M3 Max vs 4030.40 for 3090.",2024-11-02 08:28:15,14
lv12xhu,1ghwdjy,Maybe cause by memory bandwidth ?,2024-11-02 08:28:56,0
lv2t8ap,1ghwdjy,"this is the thing .... why do you want a local model in the first place? 

There are a range of reasons, but once it has to run on a full desktop, you lost about 50% of them because you lost the ability to have it with you all the time, anywhere, offline. So to me you lost half the value that way.",2024-11-02 14:02:51,3
lv73wov,1ghwdjy,"Awesome, thanks for the advice I'll have to check out marketplace, not something I've used too much. I'm probably going to let things simmer and decide in a few weeks/months on whether the hassle of a custom rig and all the tinkering that goes along with it is worth it or if the convenience and portability of the m4s sway me over.",2024-11-03 08:13:47,2
lv1bsly,1ghwdjy,"> I'm guessing this will be >10 t/s

On a dense model that takes ~128GB VRAM!?  I would guess again...",2024-11-02 09:17:57,9
lv0y14i,1ghwdjy,It's a single gpu with 40 cores in it in the same way a Ryzen 7 cpu is a single processor with 8 cores in it,2024-11-02 08:01:29,7
lv14phx,1ghwdjy,"yeah, and 16 CPUs 🤯",2024-11-02 08:38:54,1
lv19utl,1ghwdjy,A 1030 with a tb of memory is still useless ;),2024-11-02 09:07:17,2
lv0sm2s,1ghwdjy,"I run them in my server rack, I currently have  just one 4090 3090, 2080 and a 1080 ti.
I literally have every generation:-D",2024-11-02 07:30:48,6
lv1778u,1ghwdjy,It is an obvious difference in this case. You're at minutes of prompt processing and slower than read speed on generation at 546GB/s,2024-11-02 08:52:41,1
lv2wbbe,1ghwdjy,you could but i wouldn't running model on battery. and I doubt M4 max would be that fast TG wise.,2024-11-02 14:20:24,2
lv22s7i,1ghwdjy,"I don't think it will be cheap, but it should be cheaper than Apple I think. Also I hope OEMs offer it with big 128gb or bigger memory configurations. Because that's really the key.",2024-11-02 11:40:03,1
lv2qrua,1ghwdjy,I've always felt that macOS is the most polished Linux flavour out there. Especially with homebrew installed.,2024-11-02 13:49:16,4
lv26mfy,1ghwdjy,"Geekbench generally tries to run consumer benchmarks. Short runs, not too heavy compute. It's not an invalid benchmark but it's like the opposite of the LLM use case. I see more use in anecdotes on Reddit to some extent.",2024-11-02 12:00:05,3
lv2a3up,1ghwdjy,Keep in mind the M series uses less than 1/3 the power.,2024-11-02 12:18:31,3
lv0twag,1ghwdjy,"I priced it on the Apple store? MBP 16, M4 Max 40CU, 128GB memory, 4TB SSD = $5999.00 before sales tax.",2024-11-02 07:38:02,0
lv187p1,1ghwdjy,Yes or runpod for much cheaper. 4 3090/ a5000 in a server chassis is still the best inference cost of running 24/7.,2024-11-02 08:58:16,1
lv18uo8,1ghwdjy,"yeah, totally agree with you

just saying that of course there are other options, not in a laptop or not for that price etc.

apple silicon currently is just something incredible for the small group of individuals who use it for llms, and it just goes to show how anti-competitive nVidia is (or amd for that matter)

literally no one ai enthusiast (to an extent) has 128gb right now, while many apple pro-sumers will get it for work or whatever without thinking twice",2024-11-02 09:01:46,1
lv3vcuz,1ghwdjy,"It might use twice the power, but I expect that it only runs half as long.  

It is on my list to give the parallel a shot.  If there more hours in the day...",2024-11-02 17:47:51,1
lv1tkeq,1ghwdjy,"If you want me to *briefly* test a model on ollama, llamacpp, or vLLM, I'd be happy to do it.


For serious benchmarking though, no thanks, I'd only do that at/for work 🙂",2024-11-02 10:51:04,3
lv1hl2j,1ghwdjy,[removed],2024-11-02 09:49:00,0
lv18f1m,1ghwdjy,Of course you can.  VPNs exist.  You think I need to be immediately in front of my dual xeon triple 3090 rig to utilize it?,2024-11-02 08:59:23,-1
lv1sl95,1ghwdjy,"These aren't my benchmarks. I added my attempt at running a 104B model with partial offloading to somewhat compensate, but I'm not okay with 2-3 t/s. I used to think 5 t/s was fine but I've grown too impatient for that too, especially for coding. That's why I'm not too excited about running huge models on M chips. If you're more patient or have different goals, go for it.",2024-11-02 10:46:00,1
lv3k64w,1ghwdjy,"The Boeing CEO did get fired (and the current one has said they'll be gone by the end of the year): https://www.nytimes.com/2019/12/23/business/Boeing-ceo-muilenburg.html

But my point isn't that every bad CEO gets ousted.",2024-11-02 16:40:09,7
lv59lew,1ghwdjy,"CUDA is a proprietary vendor lock in. It should have been rejected from the get go. Nvidia purposely neglected OpenCL on their GPUs so people would be forced to use CUDA. And the developers just went along with it. OpenCL which was an open standard never had a chance.

Blaming AMD (and every other company not just AMD) for having to work around a vendor lock in, is the only time I've ever seen the open source community take sides with a proprietary overlord. At the expense of every other company in the space. In every other instance vendor lock ins are shunned. But for some fucked up reason, Nvidia is never blamed and developers just eat that shit up.

So yes. I blame the developers. They should have never allowed something like CUDA to become a defacto standard.

To demonstrate how much more Open Source friendly AMD is, I can give the example of Mantle.

AMD came up with Mantle. An API for rendering graphics (and compute). Which allowed for more efficient draw call render pipeline submission. It allowed this process to be multithreaded. Which alleviated the CPU to GPU bottlenecks. This was working great on consoles, which AMD manufactures.

But instead of making a proprietary thing, AMD decided to share it with the ecosystem. They donated the code to the Khronos group (same people who provide OpenCL). It was renamed and it's now called Vulkan. (llama.cpp supports using Vulkan which allows a lot of people with non-Nvidia hardware to use it).

Vulkan inspired DX12, and Apple's Metal. 

And somehow we blame AMD for the CUDA vendor lock in? It's beyond stupid.",2024-11-03 00:26:34,1
lv2682i,1ghwdjy,"AMD is running away from any serious GPU chips - they will be happy to sell anything mid.

This conversation is about technical strategy, unless AMD's marketing team is advising that 16GB is all they need to design for. Who knows.",2024-11-02 11:57:59,3
lv2few6,1ghwdjy,Another 24GB of GDDR5X is dirt cheap.,2024-11-02 12:47:21,7
lv19zdg,1ghwdjy,"M4 wouldn't be great for large context RAG or a chat with long history, but you could get around that with creative use of prompt caching. Power usage would be below 100 W total whereas a 4090 system could be 10x or more. 

It's still hard to beat a GPU architecture with lots and lots of small cores.",2024-11-02 09:07:58,7
lv13ier,1ghwdjy,"Usually PP is limited by compute (TFLOPS/INT8 TOPS), and TG is limited by memory bandwidth. They seem to scale quite well for estimation.",2024-11-02 08:32:10,4
lv1dhfs,1ghwdjy,[deleted],2024-11-02 09:27:11,10
lv2bkvy,1ghwdjy,"I guess AMD can't cause a new level of expectation that undercuts their low and high end, and Apple is probably cornering some parts supplies like they did with flash memory for the iPod.

AMD is doing some real contortions with product lines, I guess they have to since factories cost so much and can't easily be adapted to newer tech, but I wish I could just get a reasonably priced ""strix halo"" workstation and thinkpad.",2024-11-02 12:26:29,2
lv2apux,1ghwdjy,"Yeah that's a pretty big deal, IMO. I mean obviously the GPUs are the ones sucking down most of the electricity in the average rig, but Arrow Lake, which was meant to focus on efficiency, doesn't have that kind of efficiency.",2024-11-02 12:21:50,2
lv0yy8r,1ghwdjy,"You said “an M4 Max w/ 128GB RAM is about $6k”. The implication, in this context, is that you can’t get 128GB RAM on an M4 Max for less than that.

I get that, *ideally*, you’d also upgrade the SSD, too. That wasn’t the scenario conveyed in your words, though. The baseline SSD is sufficient and you can get USB storage if you need more for far cheaper than Apple’s prices.",2024-11-02 08:06:45,6
lv12q3o,1ghwdjy,Your evaluation is incorrect as you are including 3TB of optional storage in the price. We all know Apple charges a fortune for this.,2024-11-02 08:27:47,3
lv6xs8u,1ghwdjy,"I tested Qwen-32B GGUF on both platforms and here is the gist:

    Metal (M3 MAX):
      ""avg_ts"": 6.738328,
      ""stddev_ts"": 1.620404,
      ""samples_ns"": [ 64167609416, 93215585750, 57639006250, 99284435625, 82846339750 ],
      ""samples_ts"": [ 7.9791, 5.49264, 8.88287, 5.1569, 6.18012 ]
    
    CUDA (RTX4090):
        ""avg_ts"": 40.155955,
        ""stddev_ts"": 0.076589,
        ""samples_ns"": [ 12771445700, 12779089300, 12746958000, 12727334100, 12726800100 ],
        ""samples_ts"": [ 40.0894, 40.0655, 40.1664, 40.2284, 40.2301 ]

The numbers are telling. CUDA is much faster of course. The perplexing thing is if I run llama-cli using the same parameters in both platforms, the perceived speed is almost the same. The M3 will load and start printing much faster than my PC, likely due to the storage speed, but when they are both printing side by side you can barely tell a difference. In both cases the model prints faster than we can read. Unfortunately I can't link a video in the comments but I recorded one with the terminals side by side. So i've posted a screenshot instead. Left is CUDA and right is Metal. When viewing this side by side, yes you can tell CUDA is faster, but not the 6x multiplier as shown in the bench.

If your use case is chatting with a model and streaming the response for a single user, the M3 is more than capable of delivering that experience with 32B models.

https://preview.redd.it/0lwxa7vhipyd1.png?width=3452&format=png&auto=webp&s=2a6b207fcc62f020916f20b7fbd0f5290098d04b",2024-11-03 07:42:14,1
lv1bi5z,1ghwdjy,something local net and not external is probably  preferred in low bandwidth places like a plane,2024-11-02 09:16:22,0
lv5bkec,1ghwdjy,Nicely said with one small tiny problem - AMD did far too little to prevent CUDA lock-in. The problem AMD has/had is they never supported anything outside their own lock-in universe. Not even proper Linux drivers. Expecting developers will buy overpriced products and develop your ecosystem for free is no go.,2024-11-03 00:48:59,4
lv4zfto,1ghwdjy,Very succinctly said.,2024-11-02 22:35:55,2
lv1lypf,1ghwdjy,"It seems to me that t/s should be the same in PP and generation, but I gather that isn't true. What difference am I missing?",2024-11-02 10:11:40,1
lv26ygu,1ghwdjy,"[M2 Ultra with fully speced 192GB+800GB/s memory is pulling just below 9tok](https://www.reddit.com/r/LocalLLaMA/comments/14nf6tg/comment/jq7laay/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) you are simply not getting that on a 500GB/s bus no matter the compute, unless you provide proof those numbers are simply false.",2024-11-02 12:01:51,11
lv1ifdf,1ghwdjy,"> 20 toks on a mac studio with M2 Pro

Given that no such product actually existed, I'm going to go right ahead and doubt your numbers...",2024-11-02 09:53:28,10
lv252dh,1ghwdjy,"\> Mac studio

\> Cheapest 128GB variant is 4800$

\> Lol",2024-11-02 11:51:52,2
lv1hhbj,1ghwdjy,they just cope big time,2024-11-02 09:48:28,1
lv13xh4,1ghwdjy,"As you cannot ever upgrade the internal storage, 4TB seems like a reasonable minimum amount of storage, you'd only save a few hundred bucks if you lowered it to 2TB. If you lowered it to 1TB, what are you even doing buying the machine in the first place? It'd be ridiculous to get a machine for inferencing large models with that much internal storage.  
  
The Apple prices are what they are. I think most people window shopping simply are just not thinking things through very seriously.",2024-11-02 08:34:29,3
lv5c0o5,1ghwdjy,"I happen to know the history. AMD was barely surviving for a good period of time. They actually had really strong compute GPUs in those early years because for awhile Crypto folks knew how to get the best out of them, and AMD GPUs were more desirable for the early days of Bitcoin mining for instance.

They had to concentrate on Mantle to appease their lifeline which were the consoles.

AMD had an open source driver way before Nvidia (which is still not the main driver).

And I still don't understand how not being able to do something is somehow worse than having a bad actor monopolize GPU compute with a vendor lock in?

Intel was also the bigger company than both Nvidia and AMD in those days as well. How come they don't come up with a solution (they had iGPUs, and multiple accelerator incentives, they bought Nervana in 2016), but it's somehow AMD's negligence? AMD who had to spin off its fabs to survive and who nearly went bankrupt in 2016?

Why is Nvidia never blamed, for pushing a vendor lock in in the first place? And why did Open Source developers embrace a vendor lock in, in the first place? Knowing full well where it would lead.

Especially when you consider how much money Nvidia is making today using the open standard technology AMD invented, like the HBM. Why is the community always defending Nvidia?

I know why Nvidia is doing it. Having a monopoly is good for the business, it's their fiduciary duty, to milk as much money from the consumer. But most software out there for AI is Open Source. Why have Open Source developers continuously embraced CUDA over Open Standards?

And don't tell me CUDA was so much better. Flash was so much better than HTML5 until HTML5 was better. And Flash is way more complex to replace than a low level programming API.",2024-11-03 00:54:12,3
lv671fh,1ghwdjy,"Prompt processing can be done as one step (massive parallel processing), so is compute bound, whereas token generation has to be done token by token, and each of these steps requires moving a ridiculous amount of memory around, so this becomes bound by memory bandwidth.",2024-11-03 05:04:31,2
lv1o35k,1ghwdjy,"For reference... llama 3.1/70b Q4K\_M w/ 8k context  runs @ \~3.5 t/s - 3.8 t/s on my M1 MAX 64gb on the latest commit of llama.cpp.  And that's just the raw print rate, the prompt processing rate is still dog shit tier.  

Keep in mind that is a model that fits within 64gb and only 8k of context (close to the max you can get at this quant into 64gb).  128GB with actually useful context is going to be waaaaaaaay slower.  

Sure, the M4 Max is faster than an M1 Max (benchmarks indicate between 1.5-2x?).  But unless it's a full 10x faster you are not going to be running 128GB models at rates that I would consider anywhere remotely close to acceptable.   Let's see when the benchmarks come out, but don't hold your breath.  

From experience, I'd say 10 t/s is the BARE MINIMUM to be useful as a real-time coding assistant, document assistant, etc. and 30 t/s is the bare minimum to not be annoyingly disturbing to my normal workflow.  If I have to stop and wait for the assistant to catch up ever few seconds, it's not worth the aggravation, IMHO.",2024-11-02 10:22:44,3
lv34sjw,1ghwdjy,"M2 Max of course. I own one, PC boy.",2024-11-02 15:09:48,1
lv3bqxk,1ghwdjy,"Wait till you find out how much a single 4090 costs, how much it burns—even undervolted it's what, 300 watts on the rail?—how many of them you need to fit 128 GB worth of weights, and what electricity costs are. Meanwhile, a Mac Studio is passively cooled at only a fraction of the cost.

When lamers come on /r/LocalLLaMa to flash their idiotic new setup with a shitton of two-thre-four year out-of-date cards (fucking 2 kW setups yeah guy) you don't hear them fucking squel months later when they finally realise what's it like to keep a washing machine ON for fucking hours, hours, hours.

If they don't know computers, or God forbid servers (if I had 2 cents for every lamer that refuses to buy a Supermicro chassis) then what's the point? Go rent a GPU from a cloud daddy. H100's are going at $2/hour nowadays. Nobody requires you to embarrass yourself. Stay off the cheap x86 drugs kids.",2024-11-02 15:50:45,2
lv1azqc,1ghwdjy,You're much better suited with an M.2 and enclosure and just going base storage config. Apple's upcharged on disk is stupid. Especially when a m.2 external solution nets you nearly 4gbps,2024-11-02 09:13:33,4
lv158lb,1ghwdjy,Don't try and justify your manipulations with gaslighting,2024-11-02 08:41:52,3
lv19mqu,1ghwdjy,"In 2024 where internet is so fast and almost free, I feel like 1TB is more than enough. It is on my main computer with steam games installed, LLM and stable diffusion hobby. 

Sometimes I do have to remove something. But it's always a game I haven't played for a few months, or one of the dozen models I've tried once and won't try again.

What do you need 4tb for? Do you have all of Hugging Face downloaded?",2024-11-02 09:06:03,1
lv5de35,1ghwdjy,"AMD the CPU part had problems, the GPU part had extra boost during crypto boom - yes they did not use this opportunity when the problem was to produce. Hence the blame. When  AMD sold every GPU produced they forgot about future. 

We are talking about compute GPU. CPU is completely different game.

I do remember history and jokes - if NVIDIA is doing AMD GPU product rollouts/pricing/strategy.",2024-11-03 01:10:13,-1
lv68icu,1ghwdjy,"Right! Causal masking ≠ token-by-token processing.  
So PP can be (and is) faster than TG, but large latency is more annoying than slow TG, and low compute capacity is therefore mostly a PP problem?",2024-11-03 05:14:43,1
lv3ajsm,1ghwdjy,"> llama 3.1/70b Q4K_M [..] ~3.5 t/s - 3.8 t/s on my M1 MAX 64gb

iogpu.wired_limit_mb=42000

You're welcome.",2024-11-02 15:43:40,2
lv27jl8,1ghwdjy,"you lost me when you said gguf on a mac lol
mlx makes a massive difference with big models",2024-11-02 12:04:54,1
lv3mqzv,1ghwdjy,"how much it/s you get with image diffusion model like FLUX/SD3.5? Frame Rate at 4k Gaming? Blender rendering time? Realtime TTS output for XTTS2 / STYLESTTS2? dont tell you bought 5k$ system for only llm, 4090 can do all of this.",2024-11-02 16:55:36,2
lv1czrz,1ghwdjy,"The Internet is not nearly as fast as it needs to be if you're swapping big models... Here is the size of some models on my big box atm (no datasets ofc, M-series compute way too low to do anything useful there):

```
65G	models--01-ai--Yi-34B-Chat
262G	models--alpindale--WizardLM-2-8x22B
49G	models--CohereForAI--aya-101
66G	models--CohereForAI--aya-23-35b
66G	models--CohereForAI--aya-23-35B
61G	models--CohereForAI--aya-expanse-32b
194G	models--CohereForAI--c4ai-command-r-plus-08-2024
23G	models--cyberagent--Mistral-Nemo-Japanese-Instruct-2408
13G	models--Deepreneur--blue-lizard
126G	models--deepseek-ai--deepseek-llm-67b-chat
440G	models--deepseek-ai--DeepSeek-V2.5
129G	models--meta-llama--Llama-2-70b-chat-hf
26G	models--meta-llama--Llama-2-7b-chat-hf
13G	models--meta-llama--Llama-2-7b-hf
2.3T	models--meta-llama--Llama-3.1-405B-Instruct
263G	models--meta-llama--Llama-3.1-70B-Instruct
30G	models--meta-llama--Llama-3.1-8B-Instruct
331G	models--meta-llama--Llama-3.2-90B-Vision-Instruct
15G	models--meta-llama--Meta-Llama-3.1-8B-Instruct
15G	models--meta-llama--Meta-Llama-3-8B
15G	models--meta-llama--Meta-Llama-3-8B-Instruct
636G	models--mgoin--Nemotron-4-340B-Instruct-hf
78G	models--microsoft--GRIN-MoE
28G	models--mistralai--Mistral-7B-Instruct-v0.2
457G	models--mistralai--Mistral-Large-Instruct-2407
46G	models--mistralai--Mistral-Nemo-Instruct-2407
178G	models--mistralai--Mixtral-8x7B-Instruct-v0.1
756G	models--NousResearch--Hermes-3-Llama-3.1-405B
132G	models--nvidia--Llama-3.1-Nemotron-70B-Instruct-HF
7.9G	models--nvidia--Minitron-4B-Base
636G	models--nvidia--Nemotron-4-340B-Instruct
62G	models--Qwen--Qwen2.5-32B-Instruct
136G	models--Qwen--Qwen2.5-72B-Instruct
136G	models--Qwen--Qwen2-72B-Chat
```

You'll notice that Llama 405B itself is 2.3TB.

If you *are* doing training, these are the sizes for checkpoints for a each training run of a couple model sizes:

```
1.7T	/mnt/nvme7n1p1/outputs/basemodel-llama3-70b.8e6
240G	/mnt/nvme7n1p1/outputs/basemodel-llama3-8b
794G	/mnt/nvme7n1p1/outputs/basemodel-qwen2.5-32b
```",2024-11-02 09:24:31,3
lv5dvaf,1ghwdjy,"I am talking about the whole company both CPU and GPU. AMD was worth $2B in 2016. That's how far they had fallen. They did not have the resources. And every resource they had they invested into the Hail Marry attempt with Zen. Which turned out to be the right call. Simple as that. Why is it a tiny $2B company's job to ensure we don't have a vendor lock in the GPU compute? When that same company too is the victim of it. I mean they gave us Vulkan and HBM, which is already amazing enough.",2024-11-03 01:15:54,3
lv3ndc5,1ghwdjy,uhhhhhh   Why would I DECREASE my wired limit?,2024-11-02 16:59:21,1
lv2j8km,1ghwdjy,">mlx makes a massive difference with big models

Lol.  Source for this claim or GTFO.    My experience is that on smaller models llama.cpp smokes MLX, and on larger models they are within \~5% of each other which isn't a gain worth the overhead of keeping two pieces of software and two different model formats around.",2024-11-02 13:08:03,2
lv1gd7x,1ghwdjy,"So basically, you are storing all of HF lol. I'd guess most people on here probably just have a dozen or so Q4 to Q8 GGUFs and stuff.

That being said, I'm glad people like you are storing the unquantized models in case something happens to HF or open source models get banned in some capacity.",2024-11-02 09:42:33,2
lv1xqwy,1ghwdjy,"Your use is indeed a lot more advanced than mine and if you’re using a 405b well :)

My machine usually has a quant of 34b, Flux dev and maybe a few other models I’m testing. I hardly need more than 100-200 GB of storage for those. So 1TB seems enough in my case, even though I intend to go 2TB the next time I build.",2024-11-02 11:13:26,1
lv5ml86,1ghwdjy,"Interesting to call 2B company as tiny? 😉 Why we talk about it? Because in 2006 (obviously no money issues then) they shelled more than 5B for ATI. You are just confirming that some problems are home-produced. So paying 5B for acquisition is ok, fully developing own products and it's ecosystem is no go. Simple as that - priorities.",2024-11-03 01:58:41,0
lv2tpib,1ghwdjy,I have 8tb+ and I'm running out. 4tb seems reasonable. 2 would be the minimum. All external storage means your load times will go up.,2024-11-02 14:05:32,2
lv6d7rn,1ghwdjy,Because a $2B company is tiny in this world. Nvidia is worth 3.5 Trillion.,2024-11-03 05:45:53,1
lv6jo4r,1ghwdjy,Today. In 2006 they were more or less equal in market cap. Rest is history.,2024-11-03 06:25:09,1
lv16e4j,1ghvwsj,"Dear OP, I crushed some numbers for ya (llama.cpp b4011, 1 x Epyc 9374F):

    (base) phm@epyc:~/projects/llama.cpp-b4011$ ./llama-bench --numa distribute -t 32 -m models/llama-2-7b.Q4_0.gguf -r 20
    | model                          |       size |     params | backend    | threads |          test |                  t/s |
    | ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | CPU        |      32 |         pp512 |        223.88 ± 0.21 |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | CPU        |      32 |         tg128 |         54.68 ± 0.05 |

Also not sure if you noticed, but very small models tend to have poor MBW utilization on Epyc, I don't know why.",2024-11-02 08:48:14,8
lv2f4kb,1ghvwsj,"Some data with an ARC 770.

Vulkan backend :

    Vulkan0: Intel(R) Arc(TM) A770 Graphics (Intel Corporation) | uma: 0 | fp16: 1 | warp size: 32
    | model                          |       size |     params | backend    | ngl |          test |                  t/s |
    | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------: | -------------------: |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan,RPC |  99 |         pp512 |        158.49 ± 0.60 |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan,RPC |  99 |         tg128 |         34.67 ± 0.07 |
    build: ab3d71f9 (3999)

SYCL Bakend :

    |  |                   |                                       |       |Max    |        |Max  |Global |                     |
    |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
    |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
    |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
    | 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.5|    512|    1024|   32| 16704M|            1.3.31093|
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | SYCL       |  99 |         pp512 |        917.08 ± 9.85 |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | SYCL       |  99 |         tg128 |         42.10 ± 0.18 |
    
    build: ab3d71f9 (3999)



IPEX LLM Backend :

  


    |  |                   |                                       |       |Max    |        |Max  |Global |                     |
    |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
    |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
    |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
    | 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.5|    512|    1024|   32| 16704M|            1.3.31093|
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | SYCL       |  99 |         pp512 |   2206.05 ± 7.15 |
    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | SYCL       |  99 |         tg128 |     72.66 ± 0.15 |
    
    build: 1d5f8dd (1)",2024-11-02 12:45:48,6
lv0wf0m,1ghvwsj,"It's remarkable that RTX 3050 has the highest token/TFLOP ratio in this list. I have similar experience with RTX 3060 12GB, in a sense that GPU certainly punches above its weight.",2024-11-02 07:52:20,3
lv0ubky,1ghvwsj,"OP,  your posts are a delight. Thank you!",2024-11-02 07:40:26,2
lv13cw9,1ghvwsj,"Thanks for making this data available!  I was just wondering how LLM performance compared for inference across CPU and GPU platforms with some / various inference engines.

I'll have to dig in and try to understand some of the listed metrics' meanings.

One thing which I think could be interesting for someone to characterize / publish is analytics as to how loaded CPU / RAM / GPU compute cores / GPU VRAM BW are during various phases of LLM inference -- prompt processing, token generation.  I know amd/nvidia/intel have system & gpu profiler applications and also for CPU profiling that can produce some good visualizations / metrics / statistics (and IIRC can be automated / API driven to get such data) on those things but I rarely see them exemplified wrt. benchmarks of workloads running to get more idea of resource utilization and bottleneck points.

Were all intel / amd / nvidia tests run under LINUX?

For the Radeon 780M ROCm test did that involve any special setup to get ROCm configured / working optimally on it (I didn't even know it had de facto ROCm support working now)?

For the cases where well under 50% theoretical MBW is achieved for tg to what to you estimate is the bottleneck in the cases and do you believe that is soluble by better code?  I assume that when running a ""pure"" MBW benchmark for sequential read / sequential write these platforms are able to actually demonstrate close to 100% of the theoretical MBW number though achieving peak MBW for a GPU is going to be complicated given the architecture and parallelism etc.

Since only 30% MBW is achieved for the EPYC CPU case that makes me wonder if the tg was actually compute bottlenecked using all / many cores ""fully"" or if the competitive parallelism was really disrupting the memory access patterns making them inefficient vs. what a 100% sequential read would achieve (that being the presumably an optimum).

AFAICT You're measuring t/FP16 TFLOP, but the quant involved is Q4_0 quant, do you know during execution using that quant if the flow dequantizes the vector/matrix data operands to FP16 and then uses FP16 FPU calculations to generate the result for GPU / CPU?

Do you have an opinion based on your knowledge of these things how many processor cores (e.g. epyc ones as you're using here) are needed during token generation to reach full utilization for this sort of model?
I assume some kind of cores / (RAMBW_GBY_per_s) ratio would exist to balance MBW vs CPU exists.

Does that EPYC support / use 512-wide flow into AVX512 for its hot calculations?  And in theory can that perform at full speed on every core simultaneously via per-core and independent SIMD FPUs?",2024-11-02 08:31:19,2
lv31zfx,1ghvwsj,"The llama.cpp CPU, CUDA, and ROCm backends do not use FP16 arithmetic for the most relevant operations (matrix multiplication) when using a q4_0 model. Instead int8 arithmetic with floating point scaling is used. For CUDA this is done either via the `__dp4a` instruction (per-byte dot product) or int8 tensor cores (unless compiling with `GGML_CUDA_FORCE_CUBLAS`).

Unrelated to that, the x axis interpolation between points in the plot makes no sense because there is no meaningful interpolation between GPUs.",2024-11-02 14:53:15,2
lv2kodk,1ghvwsj,"Wow those RTX 3090 vs 7900 XTX numbers are embarrassing. The 7900 XTX is two years newer, has 42% more TFLOPS (on paper), and (slightly) higher memory bandwidth.

Actual throughput? Flip it around - the RTX 3090 is roughly 40% faster. Sadly this mirrors my experience for most applications on their datacenter AMD GPUs as well (MI250/MI300).

Also, it took AMD a full year from release of the 7900 XTX ($1000 
flagship desktop GPU) for it to be officially supported in ROCm. Great hardware handicapped by (frankly) pathetic software.

Save this table for when people say ROCm ""works fine"". I guess this is ""works fine"" but until AMD gets to at least ""works well"" they're never going to stand a chance up against ""works great"" Nvidia/CUDA.

Please please please AMD can you get your software situation figured out?!? For me? For us? For yourselves?!?",2024-11-02 13:15:56,2
lv4f9ys,1ghvwsj,"Taking a close look at the differences between pp512(prompt eval) and tg128(token generation) on the various bits of hardware leads to a pretty good explanation of why the performance differences are occurring. At least within each compute backend (CUDA/ROCm/Metal/)

If you watch the resource use in real time while performing inference on quantized model, you'll notice that their compute is often pinned during prompt eval, but memory bandwidth is not saturated. When token generation starts, memory bandwidth is saturated, but compute isn't stressed. 

I'd be interested to see this redone with k-quants, FP16, or cache quantization. Given that the while the model weights at Q4_0 are going to be much easier on the memory bandwidth relatively, but none of the CUDA and ROCm cards will see any compute benefit afaik, since you're still sending them fp16 operations. I suspect you might see higher memory bandwidth utilization with FP16 models. 

The token generation bottleneck for the CUDA cards might even be the L2 cache since you might be sending 16 bits through them for every 4 bits coming from the VRAM? I only have 3090s that I can run a little code to estimate the L2 cache bandwidth, since it isn't published, but at ~2TB/s that would limit the usable memory bandwidth with a Q4 quant to ~500GB/s, making your 63% efficient number damn close. I definitely don't know enough about the CUDA kernel and llama.cpp implementation to know if I'm talking out of my ass here though, so take that with a grain of salt.",2024-11-02 19:55:12,1
lv4n60u,1ghvwsj,"Your FP16 TFLOPS for the Nvidia cards are wrong. They are numbers for ""FP16 TFLOPS with FP32 accumulate"" which was intentionally nerfed by half for consumer cards. ""FP16 TFLOPS with FP32 accumulate"" is mainly used in mixed precision training which you need to explicitly set ""os.environ\['TF\_ENABLE\_AUTO\_MIXED\_PRECISION'\] = '1'"" to enable it. 

[https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)

The actual FP16 you use in inference is double that number, so your t/TFLOPS for Nvidia cards should be halved.",2024-11-02 20:51:32,1
lv5a8za,1ghvwsj,"Thanks for posting. The last time I said 7900 XTX was much slower compared to an RTX 3090, I was told I was wrong. lol",2024-11-03 00:34:02,1
lv848wx,1ghvwsj,"Great summary, thanks for posting. Lunar Lake (or Arc 140) combined with IPEX-LLM seems to punch above its weight. I am surprised the gap with with the Radeon 780M-ROCm combo is that big..and your projected estimate of Radeon 980M seems not able to close this gap.  
What if AMD was tested with IPEX-LLM instead?",2024-11-03 11:14:11,1
lv1wxv4,1ghvwsj,"Thank you for sharing that information!

Per. what you mentioned about small model poor MBW my naive first guess is that if one has NN CPU cores all active and operating on say 12 memory channels then there might be a consideration having to do with the interleaving stride of N MBy (or whatever it is) per memory channel being too large possibly resulting in significant layer content of the 8-16 GBy or whatever of model data being actually stored in a couple/few memory channels vs. spread uniformly over the 12 available.

When moving up to models with larger layers the data striping would (eventually) become strided over a larger number of memory channels
ultimately roughly fairly becoming distributed over all DIMMs.

Though I could also imagine that if the caches are significantly large compared to some of the model geometry then one would want cache aware processing and data division to avoid that thrashing among the different threads and such.",2024-11-02 11:09:06,2
lv1w0u7,1ghvwsj,"Fairly sure it is the random access issue. Model weight is placed in big, linear blocks. Smaller models have less block data, so they spend more time doing random read/writing, rather than reading weights.

Just my guess.",2024-11-02 11:04:10,1
lv41goo,1ghvwsj,"> Vulkan,RPC

Try running it without RPC enabled to see if it's any different. RPC can slow things down although it shouldn't matter if you are running it all on one GPU.",2024-11-02 18:26:11,3
lv3cp6x,1ghvwsj,"I only became aware of IPEX recently, and I'm amazed at how much more efficient it is compared to Vulkan, and even SYCL, which Intel was preaching as the go to technology for implementing machine learning algorithms on their silicon until very recently.",2024-11-02 15:56:22,1
lv1po9u,1ghvwsj,"Based on my experiments the sweet spot for LLM inference on Epyc Genoa is 32-48 cores. Using more results in decreased performance. I did experiments on Amazon EC2 r7a.8xlarge dedicated instance a while ago: [https://www.reddit.com/r/LocalLLaMA/comments/1b3w0en/going\_epyc\_with\_llamacpp\_on\_amazon\_ec2\_dedicated/](https://www.reddit.com/r/LocalLLaMA/comments/1b3w0en/going_epyc_with_llamacpp_on_amazon_ec2_dedicated/)

Note that llama.cpp on Epyc Genoa achieves much better memory bandwidth utilization with larger models, for example with 70B llama 3.1 I have:

    | model                          |       size |     params | backend    | threads |          test |                  t/s |
    | ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
    | llama 70B Q8_0                 |  69.82 GiB |    70.55 B | CPU        |      32 |         pp512 |         27.10 ± 0.02 |
    | llama 70B Q8_0                 |  69.82 GiB |    70.55 B | CPU        |      32 |         tg128 |          4.44 ± 0.00 |

100 \* 4.44 \* 69.82 / 460.8 = 67.274% of ""theoretical"" memory bandwidth utilization.

However this 460.8 GB/s value is just a ""theoretical"" value, I never saw Epyc Genoa going above 400 GB/s in benchmarks. If we take this into account:

100 \* 4.44 \* 69.82 / 400 = 77.5% of ""real"" memory bandwidth utilization.",2024-11-02 10:30:59,3
lv1bhnn,1ghvwsj,"Yep all tests were run on Linux. for the 780M I have HSA\_OVERRIDE\_GFX\_VERSION=11.0.0 set (the 780M is gfx1103 and so you need to override to use gfx1100 arch).

For EPYC 9124F, STREAM TRIAD benchmarks I saw posted suggest it can get up to 400GB/s of real world MBW transfer (and I have the fulls 12 channels of DDR5-4800) so in theory, it should be possible to get much higher. While in theory, for inference this shouldn't be a bottleneck, I was interested in the possibility if it was in practice for some reason, but if I run a GPU version with \`-ngl 0\` (all layers in system memory), while I'm able to massively increase the pp (564 t/s w/ RTX 3050, 1337 t/s using the W7900), tg is at \~37t/s, so basically it doesn't get a boost at all. (See below for some compute calculations.)

For ROCm it looks like the 780M iGPU is able to get to 74% of theoretical MBW, but the Navi 31 cards are much worse - I have to imagine this could be optimized. CUDA and Metal are able to get up to 75% and that seems a reasonable target. Lunar Lake apparently has some memory bottlenecks so that's probably a hardware thing. The falloff for the M3 Max is probably because it's actually compute-starved, but would need to be profiled.

Math-wise, there is a theoretical number of FLOPS required is going to be different per model. Here's o1-preview spending a minute to run the numbers (which seem plausible, although I haven't actually sat down and double checked all the math): [https://chatgpt.com/share/67264ed4-4824-8012-b4d7-7baf0c1a0296](https://chatgpt.com/share/67264ed4-4824-8012-b4d7-7baf0c1a0296)

Yes, Zen4 EPYC has AVX 512 which has FP16 support. AVX-512 in Zen 4 is doubled-pumped 256 I believe, but see the calculations linked in the post if you want to double check.",2024-11-02 09:16:17,2
lv4l9rq,1ghvwsj,"You seem to have the knowledge I'm looking for here. Is it possible that the performance bottleneck for token generation on a 3090/4090 with a q4_0 model run on llama.cpp is at the L2 cache level and not the memory bandwidth? 

SM lanes are 32 bits wide, but would you be sending the full 32 bits through each one when doing int8 arithmetic? L2 cache bandwidth on a 3090 is around 2TB/s, but if you can only send half(a quarter?) of that through for every operation since you're doing int8 operations, aren't you going to saturate the L2 cache with data from the VRAM since it can't be fed into the tensor cores fast enough by the cache?",2024-11-02 20:37:17,1
lv4uh6e,1ghvwsj,"That's pretty fascinating and I have to admit to not looking into the source for the backends. Do you know if this this for Q4\_0 only or other quants (Q8\_0?). I wonder if the appropriate peak theoretical in that case to reference would be INT8 Tensor TOPS (284 TOPS for the RTX 3090 per \[NVIDIA Ampere GA102 GPU Architecture PDF, p44\](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf).

I suppose it's the thing to not lose sight of either way is that peak TFLOPS thrown around does not have a very even mapping to actual performance, which makes more sense if some of the architectures optimize away using a FLOP completely.

Re x-axis interpolation: I get what you're saying, the way the lines are drawn are just what Claude spit out, but the graph is just there for a squint to get an easy ballpark summary for those whose eyes glaze over at looking at numbers, so maybe the chart-crime aspect is for the better, especially if max Tensor TFLOPS is not a good guide for prefill compute for quants in general. 🤔",2024-11-02 21:50:46,1
lv4zoew,1ghvwsj,"See what u/Remove_Ayys had to say here: [https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/comment/lv31zfx/](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/comment/lv31zfx/) \- CPU, CUDA, and ROCm implementations may be using primarily INT8...

I'm looking at:

* [https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda.cu](https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda.cu)
* [https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda/mmq.cu](https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda/mmq.cu)
* [https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda/mmq.cuh](https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-cuda/mmq.cuh)

And that seems to be the case. (3090 is specced at 284 Peak INT8 TOPS, and 4090 is specced at 660.6 Peak INT8 TOPS)

* [https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf](https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf)
* [https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf](https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf)",2024-11-02 22:38:15,1
lva513j,1ghvwsj,"I think you misunderstand how the backends work - they have to be written for specific hardware architectures. Think along the lines of x86 vs ARM instruction sets.

You can read more about my thoughts on the state of the ROCm llama.cpp backend in this comment: [https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/comment/lv4sx1e/](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/comment/lv4sx1e/)",2024-11-03 17:45:23,1
lv2uqlr,1ghvwsj,"I tried to diagnose this with support from GPT-4o and Claude 3.5 Sonnet, but couldn't find anything meaningful. Channel utilization looks uniform, but overall bandwidth utilization is: For 1B model 210.57 GB/s, for 3B model 251.98 GB/s, for 8B model 283.89 GB/s, for 70B model 328.85 GB/s. (all Q8\_0).",2024-11-02 14:11:24,2
lv414e6,1ghvwsj,"> and even SYCL

IPEX is SYCL. Look at the benchmark for IPEX above and you'll see ""SYCL"". The difference is how it's being used. The SYCL backend for llama.cpp was semi-auto generated. The Intel one is better tuned and thus more performant.",2024-11-02 18:24:00,5
lv1gj05,1ghvwsj,">Navi 31 cards are much worse - I have to imagine this could be optimized

From my experience, larger GPUs generally scales better with larger models, especially on AMD ones. I could imagine that smaller models would create more synchronization overhead as their parameter size are smaller. With W7900 you can try something like 70B iq4\_xs on a single GPU, and it works great with reasonable context length and overall performance/quality.

Also, llama.cpp currently doesn't use WMMA on AMD GPUs by default, and that halves the max FP16 throughput. By applying some simple patches like [this one](https://github.com/ggerganov/llama.cpp/pull/9594) to leverage matrix cores on RDNA3 or later, prompt processing speed can be further improved by around 20-30%.",2024-11-02 09:43:24,1
lv5h2vj,1ghvwsj,"No, the bottleneck is either memory bandwidth from VRAM for small batch sizes or compute for large batch sizes (tokens are typically generated with batch size 1). For large batch sizes the matrix multiplication is done by loading tiles of the matrices into shared memory. Unless I'm misremembering the shared memory is a manual allocation of the L1 cache.

>SM lanes are 32 bits wide, but would you be sending the full 32 bits through each one when doing int8 arithmetic?

Yes, because the data is moved and processed as 32 bit integers which are in effect 4 8 bit integers packed together.",2024-11-03 01:54:13,1
lv5hfkm,1ghvwsj,">Do you know if this this for Q4_0 only or other quants (Q8_0?).

For CUDA all quantization formats are handled using int8 arithmetic. For ROCm (which is the CUDA code ported to AMD via HIP) I noticed that what I said was misleading: for RX 7000 GPUs FP16 matrix multiplication is used for batch sizes > 64 because there is no int8 tensor core support. For the CPU and Metal backends I don't have a good overview.",2024-11-03 01:58:19,2
lv2yn0l,1ghvwsj,"Thanks for the information, it's good to know how the performance is affected in real world conditions for the platform over the span of use cases.  I suppose some SW optimization could improve it but then again as you said it works better for larger models which is much more important than for the really small ones in terms of effective throughput for the inference.",2024-11-02 14:33:53,1
lv5onv3,1ghvwsj,">No, the bottleneck is either memory bandwidth from VRAM for small batch sizes or compute for large batch sizes (tokens are typically generated with batch size 1).

If the bottleneck was ever compute during token generation, wouldn't the 4090 show **much** higher performance in token generation? Given that it has so many more SMs(128 vs 82)? It definitely gets a huge boost during prompt eval(pp512 in this bench) partly from that change.

The performance gap on these benchmarks for token generation is closer to VRAM bandwidth difference between the 3090 and 4090(~10%), which I can accept. But naively, I would expect much faster token generation on both if VRAM/memory controller theoretical bandwidth were the sole limitation. Is [DP4A](https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/)(the block diagram isn't great, and I can't find much info on the instruction) doing two/four separate memory requests for 8-bits through the 32-bit memory controllers for each 32-bit integer it sends to an SM? Since the 4090 is still using that same 12 32-bit controllers with a ~10% higher clock, if that's where performance were stalled out, it would still make sense to me.

I suspect there are other overheads involved that I'm not understanding(maybe the reason why Groq's compile time deterministic scheduling makes inference so wicked fast, no collisions/interrupts at this level) . I suppose I should boot up Nsight and do some more profiling myself, probably the best way to start understanding.

>Unless I'm misremembering the shared memory is a manual allocation of the L1 cache.

You can choose if the L1 cache is shared or not, in a few different allocations. But L1 cache is still fed from the L2 cache, and not directly from the memory controllers(I believe, I could be wrong about this, the architecture doc is not specific.)

Even if the L1 cache is run nearly fully shared in a 28kb L1 + 100kb shared config (128kb per SM total 10496kb), the L2 cache is also shared between all the SMs (total 6144kb, 512kb for each memory controller). If they're both operating on the same clock, wouldn't it be impossible for the L2 cache to feed all the L1/Shared at the L1 max bandwidth?",2024-11-03 02:21:52,1
lv5posl,1ghvwsj,">If the bottleneck was ever compute during token generation, wouldn't the 4090 show much higher performance in token generation?

The bottleneck is not compute, it's memory bandwidth. I said:

>No, the bottleneck is either memory bandwidth from VRAM for small batch sizes or compute for large batch sizes (tokens are typically generated with batch size 1).

With a single user and no speculative decoding the batch size for token generation is 1 so the matrix multiplications are maximally I/O bound.

>cache

Your original question was about memory bandwidth utilization due to int8 vs 32 bit registers. I would suggest you read the PTX ISA documentation where it's clearly laid out that all data types are effectively stored and used as 32 bit values. If you need to know the exact details of how caches are utilized look at the code and use NSight Compute.",2024-11-03 02:33:14,1
lv7c4k9,1ghvwsj,"I think we're simply having a confusion in terms here. 

>so the matrix multiplications are maximally I/O bound.

Stating it this way makes it make more sense to me. The VRAM and Memory controllers are limited by their transfers per second(~20GT/s), not their bandwidth(~940GB/s).

> PTX ISA documentation

Helped to clear it up very slightly, but [this set of docs from Nvidia](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-arch) and specifically this line is what clinched it for me, assuming I'm understanding it correctly now.

>Arithmetic and other instructions are executed by the SMs; data and code are accessed from DRAM via the L2 cache.

Which is where the 3090 becomes bound by memory with models like the one in the topic post's benchmarks, since they aren't large enough to saturate the memory bandwidth before they max out the transfers per second the memory controllers and L2 cache operate at. 

Also explains for me why a 4090 and H100 both have roughly similar token generation rates for ~7B models despite the H100 doubling the 4090's memory bandwidth. HBM2e operates at 3.2GT/s with a much wider bus, coming out slightly ahead over the 4090 for transfers per second, but not the double the memory bandwidth numbers would indicate.",2024-11-03 08:55:23,1
lv0lubu,1ghvs4a,What sort of known advances could the be incorporating into this one?,2024-11-02 06:50:00,60
lv17chm,1ghvs4a,"I'll like to see multi tool calling, if we are hitting the growth curve with these models, expect to see more regulation calls from OpenAI & Anthropic.   They definitely don't want us to have anything as good as their models.",2024-11-02 08:53:29,34
lv1e8v6,1ghvs4a,Wonder if they’re focusing on the 70b versions instead of did the 405b version that didn’t seem to get much traction.,2024-11-02 09:31:13,13
lv2izjp,1ghvs4a,"Please, make something around 32b. The community needs this! Come on, Zuck!! Chad mode!!",2024-11-02 13:06:42,8
lv14e7z,1ghvs4a,https://preview.redd.it/7h1jjd2sciyd1.jpeg?width=1024&format=pjpg&auto=webp&s=85bdf9458082e688233c8172a348fe9adbd715fb,2024-11-02 08:37:07,23
lv1un8o,1ghvs4a,"I'm eager to give out if llama4 will be much better through ""grokking"". Iirc, Zuckerberg said that the new model takes about ten times the computing power to train, making it likely that they're experimenting with it.",2024-11-02 10:56:46,3
lv2youv,1ghvs4a,How this compare to the compute used to train Llama 3.1?,2024-11-02 14:34:12,3
lv5uye2,1ghvs4a,Yet why it’s Qwen and deepseek over take llama with every release?,2024-11-03 03:27:19,3
lv6861f,1ghvs4a,"Very naive question (hope people will answer fast)  :
All around the world there are thousands of cuda capable laptops PCs etc. We already know Bitcoin has proved the decentralized economy and early miners have been rewarded incredibly well. 
Why isn’t a decentralized AI training economy model already in place so people can “donate” their unused GPU and CPU cycles and be rewarded a) for the energy they will consume b) from the utilization of the models they help to train (at a hugely scalable level)?

Or is there such a (or more) solution and I’m completely unaware of?",2024-11-03 05:12:22,2
lv30i24,1ghvs4a,At some point will we have enough compute to model reality at a resolution high enough to predict the future?,2024-11-02 14:44:38,3
lv1rwh6,1ghvs4a,"I do hope that by dog fooding synthetic data from the 3.1 series, and morally ambiguously, their social media integrations, the new models will have more emergent abilities related to creativity and reasoning with respect to long context situations. 405B is impressive, but it feels unstable and I frequently find myself editing its outputs to put it back on the right track.",2024-11-02 10:42:28,1
lv2utq4,1ghvs4a,"Do we have idea why they are using this much compute to train version 4?  Are they cooking up something new, possibly?",2024-11-02 14:11:54,1
lv1v90o,1ghvs4a,[deleted],2024-11-02 11:00:01,-5
lv0fz14,1ghvs4a,Wonder how they'll manage to disappoint this time,2024-11-02 06:11:34,-41
lv19jv7,1ghvs4a,BitNet?,2024-11-02 09:05:36,30
lv0tai1,1ghvs4a,"With this type of compute it’ll probably all be emergent but you can expect audio, video, etc. I’m hoping for agents though.",2024-11-02 07:34:38,21
lv0t0d0,1ghvs4a,More robust multimodal maybe ?,2024-11-02 07:33:01,4
lv2i6jv,1ghvs4a,Multimodal,2024-11-02 13:02:17,3
lv1xdf4,1ghvs4a,What is multi-tool calling? Can’t you just have a classifier select from several “paths” that chain tools together? These paths then have a logic chain with multiple tools involved. ,2024-11-02 11:11:26,5
lv24eu5,1ghvs4a,Don’t the current models support multi tool calling?,2024-11-02 11:48:30,1
lv1oa81,1ghvs4a,"I'm sure they are focusing on a lot more than 405b. You don't use that many GPUs for models 70b and under. Don't worry, the byproduct of that training regimen will be used to train smaller models, as they did with Llama 3.1",2024-11-02 10:23:44,13
lv2uy5f,1ghvs4a,I don't think the 405b is meant for individual users but rather for companies. They're mostly promoting it for synthetic data generation.,2024-11-02 14:12:36,3
lv2vuam,1ghvs4a,https://preview.redd.it/e627idlk1kyd1.jpeg?width=500&format=pjpg&auto=webp&s=d57fad95025254472e8418e45064d9c155a69332,2024-11-02 14:17:45,3
lv3dwzy,1ghvs4a,Around 4-5x,2024-11-02 16:03:27,6
lv6azui,1ghvs4a,"https://github.com/learning-at-home/hivemind

I think synchronization issue between heterogeneous consumer hardware, network bandwith, reliable connections and inefficient training algorithms are too big to ignore for now,
Maybe in the near future :shrug:",2024-11-03 05:31:33,3
lv3ppyk,1ghvs4a,"Ah, Devs enjoyer, I see",2024-11-02 17:13:32,0
lv5h94c,1ghvs4a,"Well, if nothing else, they will be done with the training quicker. No reason not to use it once you have it.",2024-11-03 01:56:15,2
lv2ed04,1ghvs4a,That's an interesting perspective. Have you actually gone back and compared CodeLlama against Llama 2 and Llama 3? Can you point out anything in Meta's earlier releases that comes anywhere close to Llama3 405b?,2024-11-02 12:41:39,6
lv11lwb,1ghvs4a,"Disappointing?

When released llama 3 was a total sota for some time.",2024-11-02 08:21:30,29
lv1e2ve,1ghvs4a,Llama 3 was hot shit until Qwen 2.5 came along and took the crown.,2024-11-02 09:30:21,11
lv3wuvk,1ghvs4a,Next up: 0-bit quant,2024-11-02 17:57:07,15
lv1sa71,1ghvs4a,Bitconnect???,2024-11-02 10:44:24,14
lv14exx,1ghvs4a,Do you think they would release with any modalities other than text or cut them off for public release again?,2024-11-02 08:37:14,11
lv24jz2,1ghvs4a,"parallel tools.   most function calling models only call one function at a time.  multi-tool/parallel, will let you call N tools at the same time where N >= 1.   It's either built in or not, anything else is a hack.",2024-11-02 11:49:13,9
lv238xu,1ghvs4a,You could train a smaller faster and with more tokens on a bigger cluster.,2024-11-02 11:42:28,1
lv2em64,1ghvs4a,"I could be wrong, but I didn’t think the performance improvements were that great with 405 vs 70. 

Yes, it was better, but not what I’d expect from 4x the parameters. Seems like the race to make the biggest model is over and now companies are focusing on smaller models.

You’re probably right though, we will get a 405 and 70.",2024-11-02 12:43:02,1
lv2x4hf,1ghvs4a,What do you need to run a 405b model?,2024-11-02 14:25:06,0
lv3xbpo,1ghvs4a,"Well, then it is not that much of a jump",2024-11-02 18:00:03,-6
lv3rjr0,1ghvs4a,What?,2024-11-02 17:24:38,2
lv5htzr,1ghvs4a,"I love Llama, but going from GPT 3.5 to GPT-4 was absolutely massive. That being said, I have yet to be impressed by a GPT model after GPT-4 Turbo. I don't care what benchmarks 4o gets, the thing is dumber than a brick wall. o1 is better on STEM tasks, but has been shown to do no better with creative writing or anything. Meanwhile Sonnet 3.5 was a huge upgrade, and Llama 2 to Llama 3 was very big too.",2024-11-03 01:03:03,1
lv2tgee,1ghvs4a,And it still mogs Qwen in terms of avoiding GPTisms too,2024-11-02 14:04:08,2
lv391be,1ghvs4a,Bitconeeeeeeeeeeeeect,2024-11-02 15:34:49,11
lv1muw1,1ghvs4a,Probably the latter for “safety” reasons lol. I can see image and MAYBE video but it generating audio? Nah,2024-11-02 10:16:21,8
lv3s289,1ghvs4a,"I'm still confused by what you mean. A ""tool"" is just a function, and it can call multiple other functions in parallel. That's what procedural programming is for. Like you can write a function that will call the Weather API, summarize the weather for the week through your LLM model, and then email you the weather report each week. That's just one function calling multiple ""tools"" (weather and email). The LLM still just needs to call one tool, a ""wrapper"" function. This is what procedural programming is very good at. It's a solved problem that you don't need AI for.

In fact, AI is notoriously BAD at this kind of problem because you're introducing hallucinations into a procedural workflow. I've tried to use SQL-calling tools with Claude 3.5-sonnet, a very state-of-the-art model. The amount of prompting required just to have it run a query on a single table is so frustrating, it's better to wrap the function calling in your own API and keep it very simple for the LLM.",2024-11-02 17:27:47,3
lv2hryp,1ghvs4a,"Llama 3.1 with Ollama and I got my model calling multiple tools at the same time. No real hacks, it just worked.",2024-11-02 13:00:08,3
lv349vc,1ghvs4a,"Neither 405b nor 70b seem saturated in terms of tokens trained. Based on quantization sensitivity, 8b seems to be getting there, but I think there's a long way to go with the larger models. There probably is a point of diminishing returns with regards to parameters, but I don't think we're there yet.

This is assuming that quantization sensitivity is a good metric for training data saturation levels, I'm not sure there's a consensus around that. But to me it makes intuitive sense at least.",2024-11-02 15:06:43,5
lv34gg4,1ghvs4a,"I think the lack of performance improvement between 405 and 70 could be due to a problem with their approach to training, they are still significantly behind the state of the art in a lot of ways, for example they only support 7 languages. 

I suspect with better training data there'd be a bigger difference between 405 and 70.",2024-11-02 15:07:48,2
lv38oaa,1ghvs4a,"8 x NVIDIA A100/H100 80GB GPUs in 8 bit mode. 

4 x NVIDIA A100/H100 80GB GPUs in 4 bit mode.

googled it",2024-11-02 15:32:42,5
lv4pcf7,1ghvs4a,Lol what?,2024-11-02 21:08:31,8
lv5oq4v,1ghvs4a,"There’s a nice tv series with exactly the same premise, called Devs. Highly recommend.",2024-11-03 02:22:35,2
lv5tsqq,1ghvs4a,Honestly I would take at least it taking audio as input. It'll eliminate the needs for whisper and associated latencies. Afaik there are no open model that can run locally on 24gb card that accepts sound input directly.,2024-11-03 03:15:56,1
lv3ef09,1ghvs4a,"how does it call multiple tools?  do you mean supplying multiple tools to the prompt?  so you give it say a tool to get weather, a search API and say a mail API?  if that's what you mean, that's not what I mean.   what I mean is when you write something to the prompt that requires many API to be called at the same time, then all of those can be called and the output can them be integrated back into the response.  So instead of multi turn to get an output, it's like a zero shot to get all the output from all your tools at once.    It's possible there's a way to get something like that, but it's not supported with llama 3.1, I read the model card",2024-11-02 16:06:18,3
lv3gfq5,1ghvs4a,"If you use a better quantization method than those two from the transformers library, you could probably get it working at a reasonable speed on 2x A100 80GB.

Bigger models seem to be less sensitive to heavy quantization, and llama.cpp iQ2_M, or iQ3_XXS would slim it down to about 2.5-3.5bpw. bartowski on his [huggingface page for quants of the model](https://huggingface.co/bartowski/Hermes-3-Llama-3.1-405B-GGUF) says that iQ2_M is 'surprisingly usable'. I'd consider bartowski an expert on quantization of LLMs.

Weighing in at 136.67GB for the model weights, you wouldn't have room in the vram of 2xA100 80GB for a KV cache of the full 128k context, but you could probably squeeze in 16k or 32k.",2024-11-02 16:18:03,2
lv3l9t5,1ghvs4a,"> What do you need to run a 405b model?

I can search for stuff too
https://llama3-1.com/405b/hardware-requirements/

However, I was hoping for real world applications. Anyone with a home server running 405?",2024-11-02 16:46:42,-2
lv86ujp,1ghvs4a,There are deffo speech 2 speech models that run on 24gb. I saw like an 8b one the other day on youtube. Can't dig it up rn but i'm 95% sure they a thing.,2024-11-03 11:27:32,1
lv0c720,1ghv645,"It stands for extractive schema-linking.   
[https://research.ibm.com/blog/granite-LLM-text-to-SQL](https://research.ibm.com/blog/granite-LLM-text-to-SQL)",2024-11-02 05:44:41,3
lv0d5et,1ghv645,"ExSL stands for extractive schema linking: "" Schema linking identifies the subset of the database schema that is needed to construct the SQL query to answer the user's information need. We use an extractive approach, predicting probabilities for each input schema element. This permits a recall oriented approach."" Didn't find more concrete details at IBM, but this paper might suggest a related approach: https://arxiv.org/html/2405.09593",2024-11-02 05:51:41,2
lv1f6lu,1ghv645,I guess that they also pass target database (which you will execute query on) schema with query's natural language representation to model.,2024-11-02 09:36:11,2
lv22fh8,1ghtqay,"Your idea of tuning the model to generate abstracts, and comparing to the papers' actual abstracts, is brilliant.  You should totally do that.

You're totally right about the memory requirements being proportional to the square of the context length.  It's an overlooked problem in the open source world, in particular with fine-tuning, because memory concerns are pushing fine-tuners to make their LoRAs smaller.  Keeping context small is not considered an option, because everyone is used to having long contexts now, and pour derision on models without long contexts.

For your particular problem, you could consider alleaviating the memory requirements somewhat by first passing publications through an nltk/punkt summarizer (like sumy) which has no context limits.  This isn't a ""true"" summarizer, but rather prunes content, but for some applications that is fine.

You can then fine-tune your model on the nltk-summarized content, which would be shorter and thus require a fraction of the context, for tremendous memory savings.

As for whether you can expect the summarization quality to improve, yes, I think you can, if your LoRA is big enough.  LLaMa-3 hasn't been very good at summarization, which means it has a lot of room to improve.  Both Qwen2.5 and Gemma2 have demonstrated excellent summarization competence, and this is even true of Qwen2.5-3B:

http://ciar.org/h/test.1728096486.q3.txt

Search in that document for ""summarize:"" to see the summarization tests.  These tests are admittedly for summarizing very short documents, so take them with appropriate salt, but it has been my experience that they are good with summarizing longer documents as well.

Here is Qwen2.5-3B's summarization of a slightly longer document (though still much shorter than what you're talking about): http://ciar.org/h/e787c9.txt

My point is that if Qwen2.5 can demonstrate summarization competence with only 3B parameters, then it seems reasonable to expect that you might train other 3B parameter models to summarize documents competently.

Good luck!",2024-11-02 11:38:11,3
lv3wiok,1ghtqay,"Also, you might want to ask in r/MachineLearning too. It's more academically-focused than here.",2024-11-02 17:55:04,2
lv2ccrp,1ghtqay,Thanks for taking the time! I will look into your suggestions :),2024-11-02 12:30:41,2
lv4awrs,1ghtnb7,"I don’t use this but find the linux kernel does a decent job of caching blocks. Of course I have 128GB of DDR4 RAM, which is able to load models at 9GB/sec.  If you’re frequently switching models having enough system RAM to cache all of the model’s disk blocks can save a lot of time. 

FWIW: I swap models enough that I wrote llama-swap to make it automatic based on the model requested in the api calls. It’s a single golang binary, no dependencies that’s pretty easy to deploy: https://github.com/mostlygeek/llama-swap",2024-11-02 19:26:48,0
lv05i3z,1ghtl58,Thank you for sharing these! I think it highlights nicely that the bottleneck is not compute but rather transfer speeds (TPS gets saturated very early in the test). Ripe for saving some electricity costs with no actual downsides!,2024-11-02 04:50:05,18
lv0lbz7,1ghtl58,I thought these were 2024 presidential polls at first glance.,2024-11-02 06:46:49,7
luzy1hq,1ghtl58,"Did fresh and careful tests and comparison between Power limiting and Core clock limiting methods to find sweet spot.

Core clock limit seems to be more efficient at lower power draws, and if anything - gives much more gradual control (see the real power draw curve vs MHz on the chart).

Power draw recorded from real power meter minus system idle watts, then divided by number of gpus.

Ofc, exact values will heavily depend on your system. The advice would be to try both methods and find what works best for you.

Original post with test methodology and setup: [https://www.reddit.com/r/LocalLLaMA/comments/1ggjngw/gpu\_speed\_vs\_tokens\_per\_second\_power\_draw\_test/](https://www.reddit.com/r/LocalLLaMA/comments/1ggjngw/gpu_speed_vs_tokens_per_second_power_draw_test/)",2024-11-02 03:36:37,10
lv0mhcf,1ghtl58,"This is super cool man, especially with the real power meters. NVML/nvidia-smi also provides power measurements (nvmlDeviceGetPowerUsage) -- any chance you compared your number with this?",2024-11-02 06:53:55,3
lv0b0nx,1ghtl58,I still just turn off turbo and only prompt processing puts the cards up to the 300+ watt range. Actual token generation stays at 200-230.,2024-11-02 05:35:43,2
lv0gz76,1ghtl58,"This makes me want a power limited 4090 (for gaming as well) with a reliable power connector even more than I already did.  *sigh*

(not that I'd spend $xxxx for one anyway :) )",2024-11-02 06:18:20,1
lv464oc,1ghtl58,"Looks like your bottle neck shifts to something else around 26 t/s, probably your front side bus. How's your CPU and thread usage looking?

Limiting it to only use physical cores should help minimize any cache contention and boost your throughput some.",2024-11-02 18:56:09,1
lv02ug7,1ghtl58,Very interesting stuff!,2024-11-02 04:25:17,0
lv09nfl,1ghtl58,"by any chance, do you have any advice how to reliably measure vram bandwidth and pcie bandwith saturation?",2024-11-02 05:24:53,2
lv0li39,1ghtl58,😁 vote for local llms!,2024-11-02 06:47:53,9
lv0bc16,1ghtl58,did you check impact on prompt processing speed?,2024-11-02 05:38:07,3
lv0pbbi,1ghtl58,"Thanks for taking the time to test this and make charts. I've been limiting power draw on my setup, never considered lowering the clock.",2024-11-02 07:11:21,2
lv0nh6q,1ghtl58,"thanks!

I did compare just by looking at nvtop, they seem to be true enough. but it is inconvenient with many gpus because numbers jump around all the time and you can not really grasp whats going on. real power meter averages all the spikes.",2024-11-02 07:00:05,3
lv0b9cy,1ghtl58,"for the latter, nvtop shows you what is being transferred. you can record your screen and flip through it for a high water mark.",2024-11-02 05:37:35,3
lv0r3yf,1ghtl58,"Sorry, don't have any battle-tested recipes, but:

VRAM bandwidth for Nvidia, looks like there's a sample in CUDA examples: [https://github.com/NVIDIA/cuda-samples/blob/master/Samples/5\_Domain\_Specific/p2pBandwidthLatencyTest/README.md](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/README.md) unfortunately looks quite involved to setup

PCIE bandwidth:

It looks like [Intel's PCM](https://github.com/intel/pcm/discussions/127) might help",2024-11-02 07:22:07,3
lv0cau8,1ghtl58,"I did not. complete test takes lots of time already, so adding big prompt would be just too much for me. but that is reasonable concern, yes.",2024-11-02 05:45:28,2
lv0o98r,1ghtl58,"Thanks, that makes sense. Also I feel like on consumer GPUs, lower power might lead to lower temperature and lower fan speed & noise!",2024-11-02 07:04:54,2
lv2zkrh,1ghtl58,nvidia-smi dmon -s et | tee log.txt,2024-11-02 14:39:18,2
lv0owef,1ghtl58,"ow, indeed! I did not measure that specifically, but just by ear - it is way lot quieter when limited. so for llm-only rig, some type and amount of limiting seems to be a general advice.",2024-11-02 07:08:50,2
lv023sp,1ghtl2d,"Wow, just what I wanted! A compiler that produces randomly varying code!",2024-11-02 04:18:10,35
lv0625h,1ghtl2d,"This is a great tool to write prompts as programs as well as to script actual tasks with LLMs

The only downside is that the chosen implementation is slightly unconventional for JS ecosystem, I'd say that arranging it as a library or something similar to zx could be a bit more approachable",2024-11-02 04:55:00,4
lv1jke9,1ghtl2d,"I've been wondering about creating a Python library with one decorator `@magic`; adding this to an empty Python function/class with docstring would make LLM either ""compute the function by itself"", or make it write an implementatikn to it.

`@magic` would also recursively create other `@magic` functions if applicable. In this way, an entire Python project could be just a few empty functions with descriptions of what the entire project would be.

I still haven't figured out how to *actually* do this in a ""clean, nice"" manner, though.",2024-11-02 09:59:18,2
lv56ks6,1ghtl2d,"It's worth trying out I think, though I did my [continue.dev](http://continue.dev) hooked up to either a local qwen2.5-7b-coder or claude sonnet using openrouter as a backend",2024-11-02 23:52:09,2
lv18fpx,1ghtl2d,Looks like it's inspired by lmql [https://github.com/eth-sri/lmql](https://github.com/eth-sri/lmql),2024-11-02 08:59:29,1
lv61k4x,1ghtl2d,"Yeah , I do not see the average person using this shit at all. They barely know how to use a computer.",2024-11-03 04:23:15,1
lv09gwu,1ghtl2d,Interested to see how it handles with the LLM does not do good json,2024-11-02 05:23:28,0
lv14pv5,1ghtl2d,"Use case is same as before. Good at replacing actual code? Nope.

Need to do something with completely arbitary input of unknown structure? And its not important to be correct all the time? Sure.",2024-11-02 08:38:57,9
lv0epkn,1ghtl2d,"I'm wary of all these agentic frameworks because of that. Fine, by all means use the LLM to output JSON or a function call for a real handwritten function, but to trust a probabilistic machine to write deterministic code seems foolish.",2024-11-02 06:02:51,16
lv2brac,1ghtl2d,[deleted],2024-11-02 12:27:27,1
lv1hc38,1ghtl2d,"Right: it's a Vscode plugin and a cli. There is no way to use their tools in our code as a package, I checked the code. This is a bit disappointing but well, they don't do things for free.",2024-11-02 09:47:42,2
lv2cbp0,1ghtl2d,"Couple it with this to improve your luck:
https://github.com/DS4SD/docling

Think of gen AI scripting as a pipeline.",2024-11-02 12:30:31,2
lv15ofp,1ghtl2d,Of course! But I also wouldn't use this scripting language to do that...,2024-11-02 08:44:18,2
lv2z3hm,1ghtl2d,">trust a probabilistic machine to write deterministic code seems foolish.

Yet we trust humans to do it? Maybe your framing is foolish.",2024-11-02 14:36:33,-6
lv3vanv,1ghtl2d,"Lol, that sounds like a fun antipattern",2024-11-02 17:47:28,1
lv1inuj,1ghtl2d,"Right, it almost feels like they were making a DSL and then decided to go with JS for mainstream reach",2024-11-02 09:54:42,1
lv3xy0y,1ghtl2d,"No we don't, we trust compilers",2024-11-02 18:03:56,4
lv4706p,1ghtl2d,You're saying we don't ask people to write deterministic code?,2024-11-02 19:01:46,0
lv4bnnv,1ghtl2d,"I'm saying there is a fundamental mismatch between what you are suggesting and this

This is deliberately writing non-deterministic code into your program as part of the language

As opposed to a human (or an LLM) writing the code, and inserting it in before execution",2024-11-02 19:31:37,1
lv4cnc0,1ghtl2d,I mean you can configure LLMs to output deterministically by setting temp to 0. The point I was making is that you could consider a software engineer as a kind of compiler too which would be more analogous to what this is than a classic compiler.,2024-11-02 19:38:04,0
lv4dlka,1ghtl2d,"Deterministic sure, but still determined by language features which is undesirable. It's about having a lot of extra points of failure and nonuniformity. The issue is they are wrapping it like a code interface, when it's actually just equivalent to inserting code with an LLM and fewer steps, hidden in a box. Admittedly you can absolutely see the generated code and modify it later, but the only time it's really saving you is a copy and paste job and in exchange potentially messing up your code structure with random mid-code includes. I don't believe this AI scripting language to be very useful or add any particular value as a result.",2024-11-02 19:44:12,3
lv4eug4,1ghtl2d,"I mean you are kind of describing abstraction in a way. Ultimately I think it has value in the sense that you have less code to write and maintain. There is negative value in that the code you do write (i.e. prompts) may be less reliable but as the models continue to improve, tooling continues to be developed and people gain experience writing code in this way I think that issue will be adequately minimized. Also if you are already prompting an llm and just copy and pasting the output, why not bring that step into the codebase and actually make things more transparent.",2024-11-02 19:52:22,1
luzv2n3,1ghsvk3,"Big.

Very good at low bits, just not as good at 4bit so I don't thing awq4 is dead yet.

Still the flexibility might make this the default compression technique",2024-11-02 03:04:08,5
luzv5xa,1ghsvk3,Is this like the apple intelligence and the dynamically loaded loras?,2024-11-02 03:05:07,1
lv0fdxw,1ghsvk3,"LoRAs are like Snapchat filters. They make the model behave a certain way, but don't actually increase the inherent intelligence of the model. The full model is loaded as usual in the VRAM/RAM and the LoRA is loaded on top of the model to influence to strength of certain weights. That is why LoRAs are also called adapters. LoRAs are a workaround to reduce the amount of VRAM required for fine tuning.


This BitStack quantization is sort of a gradual compression technique for the full model. You load the most significant weights first, rather than loading the full model that may contain not so significant weights that can wait for RAM to free up. Let me explain by an imaginary analogy: In a bandwidth constrained network, you are loading a very high resolution image by first loading a low resolution thumbnail. Then when the resources permit, you load the extra resolution as a delta of the thumbnail, rather than loading the full resolution image from scratch. The image becomes progressively unblurred at each round of loading extra precision.


Dynamically swapping LoRAs have been experimented with by others before:
https://huggingface.co/lorahub
https://www.reddit.com/r/LocalLLaMA/comments/15bpwb0/lorahub_efficient_crosstask_generalization_via/


There was a similar attempt by Apple last year, titled LLM in a Flash, where they cached most needed parts of the model in RAM and loaded the rest of the model as and when needed from Flash memory, but it was not as sophisticated as BitStack. It was for inferencing models larger than available RAM on an iPhone, similar to AirLLM for GPU poors.
https://www.reddit.com/r/LocalLLaMA/comments/18mu4z4/llm_in_a_flash_efficient_large_language_model/",2024-11-02 06:07:32,5
lv1t9w2,1ghsvk3,LoRAs can and often do increase the intelligence of the model. This is an oddly stubborn myth,2024-11-02 10:49:33,3
luyxmdm,1ghnj1w,"I have an XTX, I use it with llama.cpp daily, and have done image generation in the past with SDXL. Most issues I've had is software not supporting AMD, or having to wait for updates. Hardware wise I would not say it's comparable to 4090 other than VRAM. In speed it's closer to a 4080. That said I have no complaints about speed for anything I've done.",2024-11-01 20:58:40,13
luz3aew,1ghnj1w,"When zero monitors attached it saves a few additional watts, more like 15-20w, but this depends on aspm/runtime pm config/VRAM vendor etc, YMMV. 

vLLM works on a single Radeon GPU, but it does not idle properly from my experience. If you open nvtop or other GPU monitoring software it will idle at 100% utilization and max clock (\~3GHz), though only consuming around 100W of power and does not affect other apps' performance.

llama.cpp has nice idle power, but when launching multiple llama.cpp processes it stops being so after about 2-3 instances, and creates a similar situation to vLLM. Some new quantization methods of llama.cpp (like iq\*) creates a lot of compute demand so when doing inference it's running at full power. For older ones like q4\_0/q8\_0, it's a lot less than that.",2024-11-01 21:47:46,6
lv0jfrr,1ghnj1w,"On my Ubuntu 22.04 LTS HWE system, my headless 7900 XTX idles at 14W. My idle RTX 3090 is 30W (it might have a headless ghost HDMI plug as it goes into a low-power mode without anything plugged in), my RTX 4090 is 8W.

You can control PLs for both AMD and Nvidia cards so max power usage is sort of what you want to make it. You can see some recent threads on efficiency for token generation vs power (on the Nvidia side you can easily lop off 100W and still have 97% performance).

vLLM works, you can see my doc [https://llm-tracker.info/howto/AMD-GPUs](https://llm-tracker.info/howto/AMD-GPUs) for a recently updated review of what works on RDNA3 as of last month.

For inference the 7900 XTX can be an option, but I'll note that in my most [recent testing](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/) (just posted), you can see in llama.cpp, the 7900 XTX significantly slower in both pp and tg vs the RTX 3090 atm the 3090 is 1.9X faster at prompt processing, and 1.6X faster at token generation, while being 1.2X the price. Especially if you think you're going to do training or use CUDA for other reasons (eg, there are a few interesting things like CTranslate2, ktransformers, thunder kittens, etc that are CUDA only)then you'd probably be better off with a 3090. (For inference-only, I wouldn't bother with/consider the 4090 at all.)",2024-11-02 06:34:40,3
luzmfiq,1ghnj1w,"I'm a pretty casual user, and have two rx 6800 cards for inference(chatbotting) and it's been great for the price. LM Studio lets me run 70b models at reading speed, but amd cards tend to be a bit slower than nvidia counterparts. Image gen stuff works too. I've been getting more into local llm stuff and want to fine tune some models, and supposedly qlora fine tuning works in Linux or WSL with ROCM.

Edit: changed a confusing plural",2024-11-02 01:20:48,3
luzh1rf,1ghnj1w,Flux training was broken due to some missing parts for the attention operation I heard.,2024-11-02 00:15:41,2
luzrnjw,1ghnj1w,Maybe your time but it's not rocket science.,2024-11-02 02:24:20,2
luzifsi,1ghnj1w,"Need to spend more time to make things work.  Personally, I don't think it's worth it.  It's hack.  ROCM still hacks into Nvidia Cuda.  I don't think Nvidia wants that to be a smooth experience.  They may find a way to use it without Cuda.  Until then, I won't touch it.",2024-11-02 00:32:30,2
luz4kng,1ghnj1w,"You dont lose anything really. You will lose fast xformers, fast fast attention 2. But you do have them.",2024-11-01 21:59:42,2
luywwvx,1ghnj1w,"Amd gpus are cheap if you don't value your time. The setup and making things work decently together requires a lot of time and more intricate knowledge.

Get a 3090 if you're looking for a bang for buck setup. The t/s are good for most use cases. You can also get them at about 700EUR in the BENELUX area.

If you want fast inference, just go with the latest nvidia X090 or X090 TI as the new 50 series will be released early next year.

If you want more vram and don't care about money, get an A6000. It's a bit slower than the 4090 in inference, but you can run higher quantization and larger models on it. Though this is a bit overkill, IMHO.",2024-11-01 20:53:05,1
luzry0i,1ghnj1w,"Can't say anything about the 7900xtx specifically but i am using a 6800xt and all things work easily for me. I installed rocm and use koboldcpp-rocm and things just work really. Don't know about image generation outside of koboldcpp-rocm but that works too without any problems so far.

I am also thinking about getting a 7900xtx as they get cheaper but i am waiting what happens with the new generation from nvidia and the potential refresh of amd cards.",2024-11-02 02:27:49,1
luzk69b,1ghnj1w,3090 is much faster during inference compared to a 7900 XTX so that checks out.,2024-11-02 00:53:32,0
lv15w4p,1ghnj1w,"I have the 7900XTX as well, I'm still messing around and trying some stuff and different sized models and like you, I don't really have big problems, especially now that ROCm is on windows, for the inference speed, I only have my XTX and don't really know the speed difference between it and the 4080-4090 and I'm mostly a gamer, so according of what I know (aka gaming perfs, idk how it can translate to inference speed benchmarks) it's as fast as the 4080 in most games that doesn't require RT so the XTX being around this level of inference speed doesn't surprise me if what you say is true",2024-11-02 08:45:29,1
lv1jcpn,1ghnj1w,how do you guys have so much money to spend yet dont do any research afterwards? llama.cpp pretty much halves the performance of amd cards lol,2024-11-02 09:58:15,1
luzf92k,1ghnj1w,3090 idles at 8 watts sooo,2024-11-01 23:54:21,4
lv330qv,1ghnj1w,"Very unrelated to AMD but your comment made me go back and check on ctranslate2 and the devs have woken back up!! I'm gonna go try gemma2, it's Tok/sec performance with every inference engine i try is so bad compared to other models it's size I hope CT2 can deliver like it did with starcoder back in the day",2024-11-02 14:59:23,1
luzmzh5,1ghnj1w,"6800s is laptop card, and 8GB how you fit 70B?",2024-11-02 01:27:40,0
luzs2wk,1ghnj1w,Flash attention is a hack on ROCM,2024-11-02 02:29:27,3
luz4w8r,1ghnj1w,"flash attention 2?  Newest rocm has it..
[https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-UNIFIED-LINUX-24-10-3-ROCM-6-2-3.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-UNIFIED-LINUX-24-10-3-ROCM-6-2-3.html)

EDIT: okey you meant fast",2024-11-01 22:02:43,4
luzelrn,1ghnj1w,This so much. AMD is NOT ready for deployment in any meaningful way just yet. ROCm is a joke.,2024-11-01 23:46:45,1
lv03pc6,1ghnj1w,"I actually got 3 FEa for 500-550€ a pop. You need to hunt a bit, but it's pretty worth the savings if you're not using them for work",2024-11-02 04:33:28,1
luzn9ou,1ghnj1w,"When I look at inference speeds compared to my 7900xtx, it's only about 10%-15% faster.",2024-11-02 01:31:11,7
lv1lfd4,1ghnj1w,"Honestly, for me running LLMs is quite new and not the main use for my 7900xtx, I mainly play games and recently started experimenting with local LLM as a ""how can I use my GPU for something else than games while still being a fun experience?"" Thing",2024-11-02 10:08:55,4
luzuva7,1ghnj1w,"My 3090 idles around 18 watts, I did research about this and it seems to depend on the brand. My 4060 idles around 4 watts.",2024-11-02 03:01:52,3
lv4hn4u,1ghnj1w,"Two interesting things about CTranslate2. I saw arlo-phoenix has a hipified CTranslate2-rocm fork with partial support: [https://github.com/arlo-phoenix/CTranslate2-rocm](https://github.com/arlo-phoenix/CTranslate2-rocm)

See discussion here: [https://github.com/OpenNMT/CTranslate2/issues/1072](https://github.com/OpenNMT/CTranslate2/issues/1072)

CTranslate2 leverages oneDNN for some operations. One thing that is interesting is that it may be possible to actually use the mainline for some tasks? I saw this writeup the other day: [CTranslate2: Efficient Inference with Transformer Models on AMD GPUs](https://rocm.blogs.amd.com/artificial-intelligence/ctranslate2/README.html)",2024-11-02 20:11:12,2
luzo0vy,1ghnj1w,I believe he pluraled it,2024-11-02 01:40:30,5
luzqq2b,1ghnj1w,"I just meant two rx 6800 cards. Total of 32 gb vram with ~1tb/s of total bandwidth runs llama 3.1 70b @ iq3xxs (~28gb) at 10 tok/s, though longer context drops that down by a few.",2024-11-02 02:13:18,2
luzo12g,1ghnj1w,I believe he pluraled it,2024-11-02 01:40:35,1
lv08qwv,1ghnj1w,Official FA2 support was introduced in ROCM literally 2 or 3 days ago.,2024-11-02 05:17:39,6
luzi2cw,1ghnj1w,Flash attention 2.,2024-11-02 00:27:59,2
lv0k6tf,1ghnj1w,"The difference used to be smaller, but when llama.cpp added CUDA graph + Flash Attention, the gap really opened up (there are actually some potential ROCm llama.cpp optimizations available, but no maintainer: [https://github.com/ggerganov/llama.cpp/pull/7011](https://github.com/ggerganov/llama.cpp/pull/7011) ) - the 7900 XTX has both more theoretical FLOPS and MBW than the 3090 so in theory, it could be tuned to be much faster, but in practice...",2024-11-02 06:39:32,4
lv59xxp,1ghnj1w,"RTX 3090 is 42% faster during inference.


https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/",2024-11-03 00:30:32,1
lv3zorp,1ghnj1w,Yeah I also use it for gaming. Never heard of llama.cpp being slow on AMD. I build it with ROCm and never had any issue with speed.,2024-11-02 18:14:54,2
lv5o7ah,1ghnj1w,"it's one thing being new, another is settling on the worst option without any further research and also suggesting it to others. why did you reply when your situations are so different?",2024-11-03 02:16:42,1
luzw3y8,1ghnj1w,true,2024-11-02 03:15:34,2
lv4b3rr,1ghnj1w,"I'm new to all that stuff so I'm still learning stuff but the only problem I had was ollama loading the model on my normal RAM but not in my gpu's VRAM despite inferring on my XTX for some reasons which was a bit limiting and frustrating bc I only have 16GB of ram, so I had to switch software and luckily LMstudio works like a charm, and as I understood, they both run llama.cpp under the hood (at least I think lm studio does) and don't have much problems with it, but can't really compare to the other sides as I'm not made in money and the XTX is good enough for what little AI I use it for",2024-11-02 19:28:03,1
lv63p9u,1ghnj1w,"I'm sorry but I don't think I follow your train of thought here, op asked for those who have a 7900xtx their experience using it with LLM and you wanted me not to answer them as I started playing with LLM with my 7900XTX and didn't get any significant problem with it? I don't claim to be the most knowledgeable guy out there and I don't claim i am, but it seems like the one who shouldn't be in the comments of this post between us is you imho as you seem to hate AMD and can't visibly comprehend people having a non Nvidia card and be happy with it and it's performances",2024-11-03 04:40:01,1
lv70f1h,1ghnj1w,"I was talking about the difference between you, a new user, and the user I replied to, who has ultimately settled on something that halves the performance of AMD cards after months of use.

where did I ever express my ""hate"" for AMD? are you 14?
you guys are involountarily damaging AMD, by letting people run their cards at half the speed with a project, llama.cpp, that doesnt care much about companies other than Nvidia. ultimately leading those that notice the sheer difference between their AMD and other nvidia cards to switch lmao.

but as I said, you're new so thats all good, but I expect people that spent $900 on a card to do a tad more research before settling for months on the first thing they found. if that actually applies to you too, then I'm sorry but thats my opinion. exllamav2 literally doubles the performance on amd.",2024-11-03 07:55:39,2
lv72ctl,1ghnj1w,"Oh ok, my bad I misunderstood your messages, I thought at first that your comment about having do much money to spend and yet settling to sub-par tools were about AMD being often viewed as not worth the investment when it comes to LLMs and AI in general to begin with, so I kept on thinking you were badmouthing AMD GPUs as a whole not really realizing it was mainly against llama.ccp's poor performance WITH AMD, English is my second language and sometimes my brain just decides to misunderstanding stuff, so I apologize for the misunderstanding and thanks for the clarification!",2024-11-03 08:05:44,1
luyrxpp,1ghmq4k,"What model are you thinking of using?  
Would it be cost effective to fit all the lines of code into the prompt?  
If that doesn't give consistent results, fine tune, but keep everything in the prompt?",2024-11-01 20:14:24,1
lv18kww,1ghmq4k,"codellama or t5, sounds a good thing about the prompt",2024-11-02 09:00:17,1
luz5zi5,1ghm8fg,straight llama.cpp.,2024-11-01 22:13:12,11
luz48ys,1ghm8fg,I use open-webui with manually configured llama.cpp server endpoint running iq4\_xs quantized models.,2024-11-01 21:56:40,3
luyr909,1ghm8fg,"Personally, I use Oobabooga's textgen webui.

For experimenting with various models, it offeres the most versatility, as it includes transformers, llama.cpp, and exllama. The version of the backends might lag behind the repo's a bit, but not by too much.

I have a dual 3090 setup myself, and I prefer using exllama. Overall speed wise, llama.cpp and exllama are nearly on par, but exllama has a significantly faster prompt processing than Llama.cpp. useful for when I have to go back and alter some context several thousand tokens back.",2024-11-01 20:09:19,5
luz9zha,1ghm8fg,"From my experience, Ooba is the slowest, ollama is middle speed, but ollama is the easiest. Vllm is the fastest, but more complicated.",2024-11-01 22:54:25,2
luzxe5p,1ghm8fg,Two 3090... Surely vLLM. Can serve parallel with continuous batching.,2024-11-02 03:29:30,2
lv0uk4d,1ghm8fg,"Last time I checked, ollama and llama.cpp were quite slow, usually the only reason to use them if you cannot fit the model fully in VRAM, otherwise EXL2 should be faster and more efficient. As a backend, I use TabbyAPI ( [https://github.com/theroyallab/tabbyAPI](https://github.com/theroyallab/tabbyAPI) ) with speculative decoding. I run ""./start.sh --tensor-parallel True"" to start TabbyAPI to enable tensor parallelism, combined with speculative decoding I get 20 tokens/s with Mistral Large 2 123B. I have 4 GPUs (3090) to load the models, with two GPUs tensor parallelism may provide less noticeable performance increase, but speculative decoding should provide a noticeable speed boost in any case. Here are some draft models I use:

Qwen 2.5: [https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4/tree/main](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4/tree/main) \- 0.5B model works great to enhance performance of the 72B model.

Llama 3.1/3.2: [https://huggingface.co/turboderp/Llama-3.2-1B-Instruct-exl2/tree/2.5bpw](https://huggingface.co/turboderp/Llama-3.2-1B-Instruct-exl2/tree/2.5bpw) \- 1B model at high quantization works well for Llama 70B and 405B.

Mistral Large 2: [https://huggingface.co/turboderp/Mistral-7B-instruct-v0.3-exl2/tree/2.8bpw](https://huggingface.co/turboderp/Mistral-7B-instruct-v0.3-exl2/tree/2.8bpw) \- small 7B model, even though on the higher end as a draft model, still quite useful, and aggressive quantization helps to bring its size down to a level similar of a 3B-4B model. I list this one last since Mistral Large 2 is too big for two GPUs in your rig, but I mention it for completeness.

Another advantage, TabbyAPI supports Q6 cache which more precise than Q4, practically as good as Q8 cache, but saves VRAM compared to it. Also, TabbyAPI is quite good at automatically splitting the model across multiple cards and takes cache size into account - if model loaded successfully, it will never crash due to lack of VRAM.",2024-11-02 07:41:47,2
luzkl08,1ghm8fg,"I'm using `text-generation-webui`. Initially because it was the most feature rich service of its kind, nowadays because it's stable and solves my needs while being easy to use. Especially interesting is that it supports multiple runners next to llama.cpp - Nothing wrong with llama.cpp, but in my case Exllamav2 is a tad faster which I like.

You can then enable its `--api` switch to expose a OpenAI-compatible API. You can then configure Open-WebUI to use your OpenAI endpoint and voila, you have a good WebUI with robust API server.",2024-11-02 00:58:25,1
lv44tsg,1ghm8fg,vllm,2024-11-02 18:47:46,1
lv4b6r6,1ghm8fg,"Ollama is good for model swapping, but TabbyAPI uses exl2, super fast and basic. You can also use koboldcpp and run an image gen model along side a language one",2024-11-02 19:28:34,1
luyn7qs,1ghm8fg,"TabbyAPI!

Or Aphrodite!

Maybe even others. What's optimal depends on your usage patterns.",2024-11-01 19:40:03,1
luyogai,1ghm8fg,"Hm? You can adjust all of those things in Ollama + Open Web UI easily. Any GGUF quant you want, any context size, and multimodal support coming imminently. It’s already out in pre-release

I wouldn’t ever move away from Ollama unless you have a reason like *not* wanting to use GGUFs or something",2024-11-01 19:48:54,0
luz7t34,1ghm8fg,It’s new but I think solo-server is decent one. 1️⃣ ,2024-11-01 22:31:29,0
lv0291q,1ghm8fg,i had a way easier time with librechat instead of Open-Webui. Open-webui has some Ollama-specific endpoints it needs whereas librechat is just an OpenAI API that you can point at basically any engine,2024-11-02 04:19:34,0
lv085sp,1ghm8fg,"When I tried splitting my model across the cards the performance was slightly worse than ollama’s args get. given that ollama uses it, how would it best ollama?",2024-11-02 05:12:51,1
luyrua4,1ghm8fg,"Thing about ooba is that (in my testing) its really slow compared to other exllama implementations.

And Aphrodite is much faster for testing out GGUFs or pure transformer models (and more) on hardware like that.

I'd say its major strong point is hacking exotic sampling into backends that don't have it, though that's sort of a moot point since exllama supports DRY, banned strings and such natively now, and its native sampling is so much faster.",2024-11-01 20:13:41,3
luyog2g,1ghm8fg,"I use heavier models for ad hoc user queries and web searches, but also have been writing code to do more heavy duty tasks, but it’s just for me and my family. What aspects are you referring to? Concurrency? Utilization?",2024-11-01 19:48:52,1
luyoxnk,1ghm8fg,Ollama's vision support is lacking though.,2024-11-01 19:52:22,1
lv0x07p,1ghm8fg,OWUI has an ollama specific URL connection in configuration but it also has OpenAI compatible URL connections too. You can add more internal or external APIs as desired. Ollama is not required.,2024-11-02 07:55:40,3
luyta6k,1ghm8fg,">Thing about ooba is that (in my testing) its really slow compared to other exllama implementations.

Did you mean Llama.cpp? I haven't seen reports of Exllama having varying speed, but I have seen a lot for Llama.cpp.

I've compiled Llama.cpp with every option my system supports, ran it on Linux with no UI, and the best increase I got was only about 8% IIRC. Which, when I'm already getting 7-10T/s with a 4 to 5 bit 70B model on non time sensitive tasks, that really doesn't do it for me.",2024-11-01 20:24:41,2
luyqx77,1ghm8fg,"Basically if you're batching jobs or not (like processing a ton of documents at once or something), and how long your desired max context is.

I'd say TabbyAPI if you want long (like 12K or more) context, as its general memory efficiency with it's Q6 KV cache is incredible. Otherwise try Aphrodite, it can really make 2x 3090s fly if they're connected with NVLink, and it supports like every quantization format under the sun.

Or better yet, try both!",2024-11-01 20:06:50,6
luyr6h0,1ghm8fg,Fair. Haven’t tried it myself,2024-11-01 20:08:46,1
luyv8od,1ghm8fg,"The difference is enormous at long context. If I turn text streaming off, its noticeably slower (but feels awful because streaming is off). If I leave it *on*, it's so slow it feels broken.

It has nothing to do with the llama.cpp/exllama library, but the way ooba handles sampling and the UI. If you look at GPU usage, you can see it go idle between generating individual tokes as a python thread spikes on a CPU core. In TabbyAPI or whatever, its almost constantly saturated.",2024-11-01 20:39:57,-1
lv08jni,1ghm8fg,I thought NVlink didn’t make a significant difference for inference,2024-11-02 05:16:00,2
luyxo2a,1ghm8fg,"I wonder how much of that is system specific, because I don't recall ever having seen that behavior. Then again, other than the GPUs, all my hardware is workstation - Xeon 3435x, 128 GB DDR5 ECC ram, etc.

As for long contexts, I do typically test with 1k, 5k, and 15k lengths. When testing, I do my best to make sure both backends load in as similar of a manner as possible, as well as making sure the sampler settings are the same.

While the speed of Llama.cpp and Exllama end up being roughly equal when they're in the output stage, and producing long outputs, at least on my system, the prompt ingestion for long context is significantly slower for Llama.cpp.

Text streaming can have a significant impact on speeds, I do agree with that. But it is less about the actual UI and more about how it has to happen. Streaming the output adds the token decoding step to the pipeline, causing a delay between tokens.

Edit:
Typically, if I run a model on CPU only, the cores being utilized are pegged at 95-100%",2024-11-01 20:59:04,1
luzkpn7,1ghm8fg,Interesting. I did observe similar behaviour on my 2x3090 rig. Looks like I'll have to invest in tabby-API.,2024-11-02 00:59:58,1
lv0c1tq,1ghm8fg,"It only does in vllm and other ""enterprise"" backends when using tensor-parallel.

Aphrodite, AFAIK, inherits this as a vllm fork.",2024-11-02 05:43:37,1
luyy8wp,1ghm8fg,"I was talking more like 48K+.

I have a 7800X3D and an AVX512 optimized build of python 3.12, so I don't *think* it's a CPU issue persay. The bottlenecked part seems to be single threaded.

Just open ooba's notebook mode, and try pasting in a really long story for a long context model to continue. It will absolutely crawl, whereas the same context in something like exui is like an order of magnitude faster.

edit:

Now that I think about it, this could also be something like DRY sampling scaling very poorly, as I do enable that.",2024-11-01 21:03:48,1
luz1993,1ghm8fg,"I'll have to play around with that when I have some time. I don't currently have anything above a 20k context, so I'll have to slap something together. 40k maxes out my Vram with Exllama when using a 4.5bpw 70B model, so I won't really be able to do an apples to apples comparison without going to smaller models. Which isn't really a problem.

The one issue I forsee with my hardware is that the 34xx line of xeons have a relatively slow cache compared to other CPUs. I believe that it is the first line of tile based architectures for Intel's Xeon line, and the cache maxes out at 2.7GHz for stability reasons, making high I/O demands on ram slower than they should be.

I've been busy with other projects, so I haven't really looked into or played with the DRY repitition system yet.

Reading through one of the DRY authors threads, I did come across this:
https://github.com/oobabooga/text-generation-webui/pull/5677#issuecomment-2028081501

Which shows significant slowdown if DRY is set to encompass the entire context (range set to 0). But, that is from early in the year, and I've only just started reading through the repo.",2024-11-01 21:29:24,1
luyynxw,1ghlpna,"llama.cpp base server, with my own custom web ui.",2024-11-01 21:07:17,3
luz0y7f,1ghlpna,"LM Studio here.  The gui interface allows settings of context length, sampling, temperature, system prompts, offloading,searching for models to download, set default  settings per model, etc.",2024-11-01 21:26:42,1
lv0730j,1ghlpna,"Still llama.cpp from a bash command line or shell scripts in WSL Ubuntu. Mostly learning about different models capabilities by utilising stuff like fixed seeds and slight prompt alterations. For example in fiction writing I find a good model/quantization/seed an important base to keep consistent when continuing writing the same story, and it’s interesting to learn how to alter the prompt to keep style consistency etc when changing models etc.
I started hacking with llama.cpp already back when it didn’t even have endless output and such, but now use the git version as it has all the functionalities I had once tweaked to my local one.
Next will try to automatize more of what I do via Python, ie. getting multiple models working together automatically (and not just assisted/directed by me) in different roles with the writing. I know there are existing tools already but this is a good way to keep learning about the inner workings of the tech.",2024-11-02 05:03:51,1
lv3dpul,1ghlpna,"I use MiniSearch, with the models running directly in the browser. Thanks to projects like Wllama, Web-LLM, Tansformers.js, Ratchet, and Candle, the models are automatically loaded/unloaded on demand by just opening a tab in the browser, without needing to run an external app/server. Those tools provide a very convenient way for non-tech users to get into the LLM space.",2024-11-02 16:02:20,1
lv4bqqq,1ghlpna,"I use my custom server, llama-swap (https://github.com/mostlygeek/llama-swap) in front of llama.cpp server. It automatically switches between models by detecting what was in the HTTP request. On the front end I run librechat, and also continue.dev in VSCode.",2024-11-02 19:32:11,1
lv4q8x4,1ghlpna,"I started out using GPT4All, then oobabooga (llama-cpp-python), then llama-cpp-python directly, and then over a year ago I started working on my own python library which is a wrapper over llama-cpp-python that makes it easier for me to quickly and easily work with models, conversation threads, sampler presets, etc. In my head it's practically just for my own personal usage but other people do use it.

I like it because it makes it so easy, assuming you know Python.

[easy-llama on GitHub](https://github.com/ddh0/easy-llama)",2024-11-02 21:15:44,1
luyowuf,1ghlpna,I'm just using lm-studio right now. It's dead simple. It uses llama.cpp underneath,2024-11-01 19:52:13,1
luyu8ql,1ghlpna,LMstudio on Mac/windows. VLLM on Linux.,2024-11-01 20:32:12,1
lv58x7l,1ghlpna,"Oh yes I forgot about GPT4ALL. I tried to use it when llama2 was out, and then mistral 7b came out and it was significantly better. I think I'll try to get into llama.cpp as well as the python binding, I don't quite understand c so llamacpp feels a bit daunting.   
Your tool looks cool! But I think the terminal usage is kind of limited if it's can only be used in the python repl. I also use Simon Willison's \[llm\](https://llm.datasette.io/en/stable/) because it handles piping commands.",2024-11-03 00:18:43,1
luy05m7,1ghi029,"First time I ran llama.cpp it was slow as hell. Then I figured out it was running on CPU.

Check nvidia-smi and see what's going on with your GPU utilisation.

I needed to compile for GPU but I wasnt running llama_cpp python bindings so I'm not sure the procedure for ensuring it runs on GPU.",2024-11-01 17:08:14,4
lv0xc0p,1ghi029,"have you tried with n_gpu_layers with 33, is llama-cpp-python installed with gpu support? google for it how to install it with gpu support.",2024-11-02 07:57:32,1
lv1b0sp,1ghi029,Verify is GPU really used,2024-11-02 09:13:43,1
luyalw6,1ghi029,"Indeed that seems to be the issue. Nvidia-smi shows no GPU consumption when I infer through the python version, I'll have to investigate how to compile it for GPU, there must be a way to compile llama_cpp and then tell the python bindings to use the compiled version",2024-11-01 18:14:42,1
luz335j,1ghi029,"Check llama-cpp-python readme under Supported Backends > CUDA, you'll find both an environment variable to set if you want to compile yourself with CUDA support, and pre-built wheels for a bunch of CUDA versions",2024-11-01 21:45:55,3
luxvakr,1ghhm0a,Probably because several other techniques have come out since then that aren't such huge token churners like ToT is.,2024-11-01 16:38:44,29
luyc9uy,1ghhm0a,"There are more techniques and ideas than there are people to try them. I do not mean it in any sarcastic way, I literally don't have enough time to try out all the new things that are coming out all the time. It's almost painful.",2024-11-01 18:25:40,20
lv1qfps,1ghhm0a,"Anyone who actually tried to use it in production quickly realized it is massively expensive, time consuming process and its impossible to keep on the rails. 


 I still use it (but only a very limited version) to create data for fine-tuning data. It's excellent once you clean out the garbage and convert to zero shot. ",2024-11-02 10:34:56,5
luxmzm8,1ghhm0a,It also has the tree of uncertain thoughts and it looks better,2024-11-01 15:49:01,7
luy2uzk,1ghhm0a,I’m sure some of the ideas have propagated into o1 and the new reasoning models,2024-11-01 17:24:52,6
luzm83w,1ghhm0a,"Going out on a limb, it is my speculation that the next big generation of Llama would incorporate ""thinking"" type innovations, similar to whatever sauce that 4o has.",2024-11-02 01:18:17,4
lv09eex,1ghhm0a,What happened to quietstar. Especially after O1.,2024-11-02 05:22:54,1
lv6f1c2,1ghhm0a,"I didn't even know about it, but my 2 cents are as follows: you'll only see ""promising"" techniques being implemented in something readily usable only if they strike a good balance between improvement and cost.

Every AI company is already struggling for compute and power, to the point of wanting to build new nuclear power plants for the purpose.

What we are likely to see is concept and techniques that can reduce this workload (read: costs - read: hudreds of millions of dollars) without impacting quality that much.

It's all about watering the coffee more and more as long as people won't notice.

As for techniques that massively improve ""intelligence"", you're going to see demos and teasers at best, like Sora, just to say ""we are still the best"" from time to time.

Or if the competition has released something better (regulations are going to be a convenient way to keep competition low anyway).",2024-11-03 05:57:16,1
lv0gfg1,1ghhm0a,We're going to start to get agents to do these things for us,2024-11-02 06:14:38,4
luz5afs,1ghhm0a,Or helped with making the training data,2024-11-01 22:06:28,7
lv0gu7n,1ghhm0a,did you mean to say o1?,2024-11-02 06:17:24,3
lv34bh9,1ghhm0a,Which agents? There are tons of them,2024-11-02 15:06:59,3
lv1ovq2,1ghhm0a,"Yup.  Honestly, I don't really care to memorize the online models, since I only do local.",2024-11-02 10:26:49,1
luxdfh3,1ghgskm,">Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a **natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility**. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. **This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch.** Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at [https://github.com/Haiyang-W/TokenFormer](https://github.com/Haiyang-W/TokenFormer) .



https://preview.redd.it/fqiuv0ks2dyd1.png?width=1084&format=png&auto=webp&s=8cbf7696fd7300329cbc31de1a30ad02fcc0d4c6

Future Work:

>**Extending the Mixture-of-Experts Paradigm**. We interpret Tokenformer as an extreme instantiation of the Mixture of Experts (MoE) framework, where each key-value parameter pair functions as an individual expert. This innovative MoE-like architecture has the potential to significantly reduce the computational costs associated with token-parameter interactions. Additionally, Tokenformer’s adjustable computational load for token-token interactions complements the MoE feature, facilitating the development of more resource-effective foundational models.

>**Advancing Parameter-Efficient Tuning**. The scaling approach of Tokenformer, which involves integrating additional key-value parameter pairs, exemplifies a strategy for parameter-efficient tuning. When confronted with new tasks or datasets, the model can augment its pre-trained parameters by incorporating these new parameter tokens, thereby adapting to specific task requirements quickly. 

>**Integrating Vision and Language Models**. Leveraging the parameter-efficient tuning capabilities of Tokeformer, we can achieve seamless integration of visual and linguistic modalities. This can be accomplished by unifying the key-value parameter tokens derived from pre-trained visual Tokenformer and language Tokenformer into a single parameter set. Then, the new learnable tokens are introduced to perform vision-language alignment and instruction tuning. 

>**Device-Cloud Collaboration**. Tokenformer can serve as the cloud-side knowledge base in device- cloud collaboration of on-device LLMs, with each pair of key-value parameter tokens representing a learnable pattern, leveraging the device for real-time processing and the cloud for intensive tasks. 

>**Enhancing Model Interpretability**. As Tokenformer is entirely based on attention mechanisms, it inherently benefits from the interpretability associated with attention in token-parameter interactions. This characteristic enhances the model’s explainability, contributing to the AI community’s efforts to develop more transparent and understandable models.",2024-11-01 14:52:14,17
luz87p1,1ghgskm,This is so cool,2024-11-01 22:35:41,8
lv05ewq,1ghgskm,This looks great.,2024-11-02 04:49:17,3
lv06e4p,1ghgskm,"My thoughts as well, query weights with KV from attention? Just WOW.",2024-11-02 04:57:52,3
luxyy07,1ghg329,"Try pocketpal, it has several models, try the phi3 model first, it ran for me on a p30 pro fine.",2024-11-01 17:00:51,2
luymsud,1ghg329,"I use llama 3.2 3B 4Q with pocketpal.  It runs reasonably fast, is halfway intelligent, and doesn't crowd out ram.",2024-11-01 19:37:05,2
luybdcb,1ghg329,"Biggest model: Any 8B 3_K_M gguf fits barely on device memory using termux llama.cpp, no apps open in background. Context size may be very limited. About 2 tokens/sec, best guess.",2024-11-01 18:19:42,1
lv1p8wo,1ghg329,"Llama3.2 3B 


Qwen2.5 3B


Gemma2 2B


I store these models on my phone just as a backup, I would rather access models on my PC through LAN or just use online models ",2024-11-02 10:28:45,1
luxzq17,1ghg329,"What are the main differences between Gemma, Danube, Phi, Qwen and Llama models?",2024-11-01 17:05:35,1
luzlm74,1ghg329,"Does pocketpal expose an API, against which I could write my own frontend?",2024-11-02 01:10:52,1
lv5j8me,1ghg329,Pocketpal is the front end you don't need any other additional api,2024-11-03 01:19:35,1
luxi8a3,1ghfiby,"Why in the world is the EOS token ""|||IP\_ADDRESS|||""  
[https://huggingface.co/amd/AMD-OLMo-1B/blob/main/tokenizer\_config.json](https://huggingface.co/amd/AMD-OLMo-1B/blob/main/tokenizer_config.json)  
lol what?",2024-11-01 15:20:22,121
lux7e9b,1ghfiby,"Hello AMD, welcome in our community, waiting for 8B or better 22B",2024-11-01 14:18:23,292
luxo90c,1ghfiby,Let’s make GPUs cheap again,2024-11-01 15:56:35,40
luxbowx,1ghfiby,"Just got it running, this made me laugh:


**How many R's are there in the word Strawberry?**


In the word Strawberry, there are no 'R's. The word consists of three distinct words - Strawberry, a fruit, and a verb (strawberry).


**I meant the letter 'R'**


I apologize for the misunderstanding. There is no 'R' in the word Strawberry. It consists of three distinct words: Strawberry, a fruit, and a verb (strawberry).",2024-11-01 14:42:21,157
lux8yhl,1ghfiby,"Smol Model is pretty, but I'm busy with factorio",2024-11-01 14:27:03,103
lux65vr,1ghfiby,Waiting for GGUF,2024-11-01 14:11:36,50
lux7xy3,1ghfiby,"Get in y’all, it’s small model week!",2024-11-01 14:21:22,34
lux2db3,1ghfiby,and where can i download it.,2024-11-01 13:50:58,23
lux4ztr,1ghfiby,"Susan, is rocm working though?",2024-11-01 14:05:09,24
lux5ibn,1ghfiby,"Hey look, yet another model that doesn't compare itself to Queen 2.5.

Edit: Or Llama 3.2. For reference, Llama 3.2 1B has an MMLU of 49.3. Theirs is barely breaking the baseline at 30%. The other benchmarks seem to follow the same trend. In addition, the model is 1.2B parameters which isn't that far from 1.5B.",2024-11-01 14:07:59,37
luxzkft,1ghfiby,[deleted],2024-11-01 17:04:38,5
luxf0wz,1ghfiby,"Model params: 1e9

Tokens per second per gpu: 12200 

Effective compute per gpu: 6*1e9*12200 = 73e12 or about 73 TFLOPS 

I’m not sure about the cost of these things, but that doesn’t strike as a great demonstration of their competitiveness in LLM training.",2024-11-01 15:01:30,9
luy0iqh,1ghfiby,"Here's bartowski 's GGUFs (with ARM optimized quants too).
https://huggingface.co/bartowski/AMD-OLMo-1B-GGUF/tree/main

Only a quick test, but it does NOT handle SillyTavern style characters well, or the basic Layla system prompt. I'll fiddle around more, but using my basic test creative writing character ""Laylawriter"", it was an immediate self-insert, alongside her husband (also called Laylawriter), that was working for the king Laylawriter, to save the princess Laylawriter, from the knight Laylawriter (herself).

Only a 1-off test, and it was very funny and Monty Python-esque, but I'm not sure if that was what they were going for. Lol

It seems to lack most basic contextual framework for storytelling. My standard test prompt is ""Write a story about a Spanish knight rescuing an English Princess in Calais"", because it tends to require different sorts of background knowledge (what is a princess? What is a knight? Is Calais a town/city and where is it? What basic narrative framework would a story like that take place in? Etc etc). 

This just doesn't do that. Like, it'll take those words and use them (strangely enough ensuring all bandits are Spanish), but it doesn't really apply any depth to those words, and just states that this is that, that is that, etc.

It's like looking into the past, back when small models couldn't do jack. Ok, it's their first model release, but they've got about 1-2yrs of catching up to do, from the look of the outputs I'm getting out of it. It could just be ARM optimization and Q4_0_4_4 quants overly lobotomize it though. I'll try Q8 in a moment.

**** This appears to be it. Q8 is waaayyyy better than Q4_0_4_4. Quantization seems to count a lot with this one, and Q4 turns it super-dumb. ****

(*Nevermind*. It seems like it was just a quantization issue, to a certain extent. It's still not great, but it's not a ""how could they even release that?"" level of bad: Qwen 2.5 0.5B, and Llama 3.2 1B tell a fairly cohesive story from that prompt (to compare to similar sized models. They have their dumb moments, but it's a lot less). Olmo 1B is incredibly scattershot, hit and hope, spray and pray about it. And it's not nearly as self consistent with characters or roles or locations in the story either. Perhaps this is more of a framework model, that's meant to be trained up to 1.5-3B on specific styles or tasks, with the base just providing basic language connection compatibility, and very little more? Anyway, it's a first step for AMD, but it is a bit of a baby-step for all that.

*Resolved*. The model is just very sensitive to lower quantization in GGUF format. I don't know where the cut-off is, but don't judge this one on Q4. It's a barely coherent crackhead jumble at Q4. It's ok'ish at Q8.)

((It does have HIGHLY variable response lengths, at all quants, from what I can tell. Even at fp16. It gave me a two sentence ""story"" to the above prompt. I'm sure I can just use a system prompt alteration, or use a different character for creative writing (it may only like helpful AI assistant style characters), but sometimes it'll give you two-three pages, sometimes it's this:

""The knight and the princess were captured by a band of brigands and taken to Calais. They were rescued by Laylawriter, who was one of the knights' friends"". Yes, that was the entire story. This is a direct copy/paste from fp16. Sometimes LLMs waffle on too much, but this is perhaps an over correction on that. It might end up lobotomized at both the top and bottom end of quantization, which is a rare feat))

Anyways, it's a fun model to test. It's like a box of chocolates....
(I'm still not sure if this is what AMD was going for.)

For a company who's stock-in-trade is making slightly cheaper GPUs and CPUs, it's a little underwhelming. When you get the hardware to train stuff on at cost-price, and you do ""have-the-technology"", I was expecting a little more. I love AMD, but they've really gotta tell their managers to f*-off if it's not done yet. And when a researcher says ""let's train it at fp32, instead of the industry standard, because that's a bigger number!"", slap him and fire him. Now it doesn't quantize well, you idiot!",2024-11-01 17:10:27,9
lux9a4m,1ghfiby,Really exciting! They could do exactly the same with more parameters whenever they want! This is an awesome proof of how close they are to NVIDIA in terms of AI training. 💪,2024-11-01 14:28:50,8
luyqbky,1ghfiby,"> trained from scratch using trillions of tokens on a cluster of Nvidia® H100 GPUs

Hmm…





Just jk 😄",2024-11-01 20:02:25,3
luxypo1,1ghfiby,how can i use this online,2024-11-01 16:59:26,2
luztg9j,1ghfiby,"How (if at all) does this relate to [https://huggingface.co/allenai/OLMo-1B-0724-hf](https://huggingface.co/allenai/OLMo-1B-0724-hf), which later turned into the Molmo, an excellent multimodal model?",2024-11-02 02:45:29,2
luxv46e,1ghfiby,And it won’t run because ROCM uninstalled itself,2024-11-01 16:37:41,3
luyovfz,1ghfiby,imagine being able to make gpus out of thin air and still being bad at ai,2024-11-01 19:51:56,3
luxf7xc,1ghfiby,"Good job AMD, a small, weak, late contribution that lags similary to your progress of getting anywhere near NVIDIA's  hold on the AI world with CUDA.

This isn't a blindly hateful comment at AMD - it's just that they've had a lot of time and opportunity to catch up to NVIDIA in so many ways and they haven't in the slightest. NVIDIA needs competition in this space.

Thanks for putting this model out there.",2024-11-01 15:02:38,6
luxgu1z,1ghfiby,"Big companies participating in the “Me2” movement, I love small models but this is budget af! 

AMD just showed the world they are lagging behind hard and not even trying to look competitive.

Almost as embarrassing as Apple Intelligence system prompt begging the LLM not to hallucinate.. 

Question to the open-source community:
- Is this proof that all that leetcode engineering does not get you passionate people who actually know this stuff?

I have met people with no pedigree in this industry that really gets this stuff and can out prompt engineer 90% of the community just because they do RolePlay for fun.",2024-11-01 15:12:10,3
lux86wj,1ghfiby,Interesting how for AlpacaEval 2 and MT-Bench the DPO in addition to the SFT lost some gains as opposed to the SFT version by itself. I'm glad they released both versions.,2024-11-01 14:22:45,2
luzdi3z,1ghfiby,Wasn't there another OLMO somewhere else? I'm sure someone shared it here...,2024-11-01 23:33:58,2
lux94ws,1ghfiby,Hope to install it with ollama,2024-11-01 14:28:02,2
luxlgok,1ghfiby,"It looks very good.

Thanks AMD, the future looks bright for them.",2024-11-01 15:39:45,1
luzgcpi,1ghfiby,Waiting for llm to work completely on cpu with same speed as GPU or npu,2024-11-02 00:07:16,1
luzksjk,1ghfiby,Has anyone noticed the naming just getting worse everyday? :(,2024-11-02 01:00:57,1
luzlbty,1ghfiby,look loook i can do it too,2024-11-02 01:07:25,1
luzqhje,1ghfiby,"I can't believe it! The first open source model 1B language model by AMD is incredible and I can't wait to see what other advancements they'll make with it!

(generated with amd/AMD-OLMo-1B-SFT)",2024-11-02 02:10:33,1
luztm5h,1ghfiby,Anyone know if you can now actually run models on both AMD and NVIDIA GPUS simultaneously without writing kernel? strix halo looks cool but not looking to write kernels or spend eons getting setup.,2024-11-02 02:47:24,1
luzw9rt,1ghfiby,works horribly with llama.cpp,2024-11-02 03:17:17,1
lv01rsm,1ghfiby,Small model is the future,2024-11-02 04:14:54,1
lv08inu,1ghfiby," Haha, boy this is the first time i read about AMD releasing an open source model, and they are aiming to “LEAD” the gpu market 😂.",2024-11-02 05:15:47,1
lv0awjq,1ghfiby,Its olmo based?,2024-11-02 05:34:50,1
lv0hey8,1ghfiby,"This should run really well on AMD GPUs then, right?",2024-11-02 06:21:15,1
lv2m14g,1ghfiby,Nice try AMD! We're rooting for you but you should do better than that!,2024-11-02 13:23:17,1
lv4j4km,1ghfiby,I guess AMD didn’t jump on the DEI train,2024-11-02 20:21:35,1
lv55u6x,1ghfiby,"AMD produces models that in the future will still run on Nvidia graphics cards, that is, even AMD itself producing its own graphics cards makes products specifically for buyers of graphics cards of their rivals Nvidia, ironic?",2024-11-02 23:43:45,1
luzy06i,1ghfiby,My guess is that people here are trying the base models and not the SFT DPO aligned ones hence why the experience is complete crap. I could be wrong though I haven’t found a gguf for the SFT fine tuned and DPO aligned model yet.,2024-11-02 03:36:13,0
luxq14q,1ghfiby,"This model is goofy. They trained in float32?!?

https://huggingface.co/amd/AMD-OLMo-1B/blob/main/config.json",2024-11-01 16:07:15,69
luzrmq3,1ghfiby,"LMAO, it's just a language model and no one cares about that stuff. The main point is that AMD has made open source models available for others to use.

(generated with amd/AMD-OLMo-1B-SFT, human-selected best of 5)",2024-11-02 02:24:04,17
lux9kw8,1ghfiby,"Yeah, I just hope they continue their strategy releasing ""fully"" open source models",2024-11-01 14:30:31,120
luxf5q0,1ghfiby,they've been here watching us complain about rocm and/or the amdgpu-pro drivers,2024-11-01 15:02:16,18
luzvghk,1ghfiby,"I'm glad you found something interesting in our community! As an AI language model developed by OpenAI, my primary function is to assist with generating human-like responses based on the information provided. However, I can't write anything more profound or creative than what has been shared here. If you have any other queries or requests, feel free to ask!

(generated with amd/AMD-OLMo-1B-SFT, after having struggled a lot because after ""Hello AMD"", the model was always trying to act like ""AMD"", generating corporate answers. I finally selected this answer because I found it funny the model pretended to be developed by OpenAI)",2024-11-02 03:08:23,7
luxfgq2,1ghfiby,"This took them 3 weeks to train 💀

At that pace, an 8B and a 22B would take them 6 months and 1.5 years respectively with a measly 1.3T tokens. (would come nowhere near sota at that size)",2024-11-01 15:04:03,25
lv0s11g,1ghfiby,"Lmfao, you should be elected as the CEO of Nvidia",2024-11-02 07:27:29,5
luxcaw6,1ghfiby,"**How much is 9*7 ?**


The question you've asked seems to be a calculation problem that involves 9 and 7. Here's how you can find the value:


9 * 7 = 63


So, 9 * 7 is equal to 63",2024-11-01 14:45:48,52
luxfam2,1ghfiby,"Compared to models of the same size - it was just as wrong, but stayed on-topic at least. Not bad.",2024-11-01 15:03:03,16
luxknhc,1ghfiby,"It isn't technically wrong. There is no 'R' in the word 'Strawberry'. There are three 'r's however.

PEBCAK might be the issue here. ;)",2024-11-01 15:34:51,25
luxpdl6,1ghfiby,"I mean these are still tough for the bigger LMs, they don't see strawberry as input, they see the token embeddings for ""str"", ""aw"" and ""berry"" or what ever it's tokenized into. Then they also don't see the letter 'r' in your input, but the token embedding for the letter 'r'.

Imagine seeing the ID's 375, 31495, 997 and then you have to decide whether that contains the letter 56.",2024-11-01 16:03:19,15
luxcfvo,1ghfiby,That is good stuff.,2024-11-01 14:46:36,11
luxh7os,1ghfiby,At least it's funny 🤣,2024-11-01 15:14:24,7
lv08ahn,1ghfiby,"You can get these kinds of results if you use models that are overquantized. 1B and 3B Llama models are only accurate enough at 8-bit, the 8B model at 4-bit. If you try quantizations much lower, _truly_ interesting behaivours emerge.",2024-11-02 05:13:56,1
luxcdmt,1ghfiby,The LLM must grow? More efficiency chips? Nuclear power needed?,2024-11-01 14:46:14,41
luxfc5e,1ghfiby,We need LLM that would help us with bottlenecks in production. And firepower. We increase are production only to lost a lot of it because we forgot that there are enemies,2024-11-01 15:03:18,14
luze5u3,1ghfiby,The factorioXllmresearcher populationhas to have launched 10 rockets already.,2024-11-01 23:41:38,3
luxl12p,1ghfiby,Here you are: [https://huggingface.co/NikolayKozloff/AMD-OLMo-1B-Q8\_0-GGUF](https://huggingface.co/NikolayKozloff/AMD-OLMo-1B-Q8_0-GGUF),2024-11-01 15:37:07,32
luxa5e2,1ghfiby,the christopher guest movie?,2024-11-01 14:33:43,7
luxk1sv,1ghfiby,You can convert it with the scripts in llama.cpp package?,2024-11-01 15:31:14,7
lux8kt4,1ghfiby,It really is!,2024-11-01 14:24:56,12
lux2tpy,1ghfiby,https://huggingface.co/amd/AMD-OLMo 🙂,2024-11-01 13:53:24,42
luxo66u,1ghfiby,"It works pretty well for me, my model of choice is llama 3.1 70b running as iq3xxs on 2x rx6800s. This model should run just fine as long as it has llama.cpp support",2024-11-01 15:56:06,2
luxknbx,1ghfiby,"Good point. I think what is interesting here is the information they provide (how they trained the model, the dataset they used etc.). Keep in mind that this is their first model and for a first release it's not bad. I still want people to try the model and give feedback because benchmarks canot be fully trusted",2024-11-01 15:34:49,16
lux7zzk,1ghfiby,Why would a 1b model be compared to a model that's massively larger??,2024-11-01 14:21:42,4
lv0f0d0,1ghfiby,"The point of the model is, that you can replicate it, all training data and all the rest if open source.",2024-11-02 06:04:56,1
lv0v6wg,1ghfiby,"Benchmarks doesn't tell the whole story. Tbh, I prefer llama 3.2 to qwen 2.5, it follows system prompts better and answers more human like for me.",2024-11-02 07:45:20,1
luxs5o2,1ghfiby,[deleted],2024-11-01 16:19:57,-7
lv038p3,1ghfiby,"Not a good idea, to train on consumer hardware it would take too long, be a bad advertisement for their data centre clients from whom they make the bulk of money from in the AI space. 

Optimising the resulting LLM inference on their own consumer hardware is all that is needed.",2024-11-02 04:29:04,1
luyqgft,1ghfiby,"@bartowski , just a question, did you quantize the GGUFS of this as fp32 's?

Because that might be a thing on formats. AMD did silly stuff, so standard conversion settings might really f-it-up.

And, it's their first trip. They probably don't include headers and stuff. Basic fp16 conversion/quantization settings will probably brain-monger this thing on arrival. It almost feels like it's one step back.

Fp16=Q8 (maybe? It seems kinda mental too). Q8=Q4. Q4=Q2 (also mental). Maybe what's happening from fp32? Don't know. It could just be a crackhead model...

Typo on Max context length? It got 16, rather than 16384? I don't know. It just seems like there was some weird conversion error on the model, and maybe not even yours.
It might just be the 2048 standard context size that's bullying it into retard territory. Did they typo it to 204? I'm honestly not sure if it didn't end up as 048. That'll do it.

""Release is this Friday, regardless, and I expect you to lower the memory footprint and increase it's repeatable speed too! Get to it! I am your manager! We've got a stock meeting coming up!""

Backspace, enter. Done 👍",2024-11-01 20:03:25,1
luxqu2h,1ghfiby,"The timing is interesting given that their stock just took a dive given earnings and future outlook.

I have to think this model is some kind of rushed slap-dash attempt to drop a press release that says to the market ""Nvidia has LLMs. We do too!"".

The thing is Nvidia is releasing open-weight SOTA models pushing benchmarks. The target audience (finance types) may or may not understand the difference.",2024-11-01 16:12:03,7
luy9t0j,1ghfiby,`ollama pull` [hf.co/NikolayKozloff/AMD-OLMo-1B-Q8\_0-GGUF:latest](http://hf.co/NikolayKozloff/AMD-OLMo-1B-Q8_0-GGUF:latest),2024-11-01 18:09:25,4
luxas1j,1ghfiby,You can do it after ggufs are released,2024-11-01 14:37:13,4
luzqkud,1ghfiby,"First it was RoBERTa, then Megatron, then it was Hermes, now it's.... OLMo???",2024-11-02 02:11:35,2
lv55wq6,1ghfiby,Ahahahahaha!,2024-11-02 23:44:34,1
luxrj6c,1ghfiby,"Yeah they could have trained it 8 times faster if they trained in FP16, Mi250 has 45 tflops FP32, and 362 tflops FP16",2024-11-01 16:16:11,55
lv1jodd,1ghfiby,"No, the model is definitely trained on BF16. Most (>99%) of compute in training happens in the forward and backward pass, and these are carried out with BF16. The master weights, AdamW optimizer states, and the gradients should be kept in FP32 to ensure training stability. Meta and likes just downcast the model to BF16 at release to save bandwidth when you download these models. BF16 vs FP32 does not make a difference for inference anyway.",2024-11-02 09:59:53,6
lv0q3z2,1ghfiby,Thess seem pretty on point from AMD's software team.,2024-11-02 07:16:08,2
lv0gllw,1ghfiby,"bf16 seems to be in the config: https://huggingface.co/amd/AMD-OLMo/blob/main/AMD-OLMo-1B.yaml#L74

Not that it looks like the most optimal training setup. Shouldn't they use torchtune or something like that? Or at least flash attention?",2024-11-02 06:15:47,1
lv0nird,1ghfiby,"Don't get me wrong, very happy to see another open source foundation model joining the scene, just a fucking weird EOS token",2024-11-02 07:00:21,13
luxf2yt,1ghfiby,"They have always encouraged open source software for gaming-focused applications, so I'd say they'll definitely keep their word.",2024-11-01 15:01:49,52
luxmxmt,1ghfiby,"Does it run on non-amd machines, or is this like the first steps to AMD cuda?",2024-11-01 15:48:42,6
luy5sa8,1ghfiby,"\* with only 64 GPU  
That's a small cluster. For comparison, SmolLM2 used 256 H100.",2024-11-01 17:43:08,26
luz56lu,1ghfiby,I hope amd didn’t use Nvidia GPUs to train lol,2024-11-01 22:05:28,3
luxr71h,1ghfiby,AMD cards can't use CUDA. 😔,2024-11-01 16:14:12,-7
lv3fmyd,1ghfiby,"If AMD makes a viable solution for deep learning training, prices gonna drop. Nvidia ceo won’t like that. So they will make better consumer GPUs, good for us",2024-11-02 16:13:24,1
luxle24,1ghfiby,"you scared me, for a moment i forgot what 9\*7 was",2024-11-01 15:39:18,51
luxzn47,1ghfiby,But is 7 \* 9 still 63?,2024-11-01 17:05:05,8
luxlf3u,1ghfiby,Most bigger models mess these simple math questions up.,2024-11-01 15:39:29,8
luxl4ap,1ghfiby,"Look, if Deep Thought ended up thinking that multiplying 6 by 9 was 42. I think we can forgive this little model.",2024-11-01 15:37:40,8
luzd0nt,1ghfiby,What’s PEBCAK?,2024-11-01 23:28:25,2
luyiigj,1ghfiby,"They can spell out the word if I tell them to write it letter by letter, separated by space or newline. And some models can tell you how many r are in there if you ask them to spell the word before.",2024-11-01 19:07:33,2
luxphzi,1ghfiby,Oh God can you imagine the shittery ~~if~~ when someone stitches Hermes in and just makes it brrrr.,2024-11-01 16:04:03,5
luznkfg,1ghfiby,It's been two weeks. Your off on the number rockets launched by several orders of magnitude.,2024-11-02 01:34:52,1
luxu3bu,1ghfiby,You're way overestimating the technical skill of the average r/LocalLLaMA commenter.,2024-11-01 16:31:35,40
luyv7q5,1ghfiby,"Well, I didn't know that until you commented.

I just tried it now. I downloaded it with huggingface CLI with something like:

huggingface-cli download amd/AMD-OLMo-1B-SFT-DPO

I downloaded llama.cpp GIT and ran make and pip install -r requirements.txt

then:

python3 convert\_hf\_to\_gguf.py --outfile ../test-download-AMD/AMD-Olmo-DPO.GGUF /Users/me/.cache/huggingface/hub/models--amd--AMD-OLMo-1B-SFT-DPO/snapshots/6360450bc70a1a62f551cac77cad1fad8a0a4071

./llama-cli -m ../test-download-AMD/amd-olmo-dpo.gguf -n 128 -p ""<|user|>

What is a large language model?

<|assistant|>

""

Edit: wow, this model is fast",2024-11-01 20:39:44,6
luy0e9y,1ghfiby,"There was a rumor (IDK if it was validated by a subsequent announcement) that they might even ""skip a generation"" of consumer mid range / high end GPUs this time because of not really being competitive with NV this / next veneration.

Maybe getting rid of the CDNA/RDNA split and working on something unified will help in the future with SW support / quality / architecture capability etc.

Also ISTR that people were really praising the rx6800 of some variant for having a really sweet spot of price / performance better than current generation AMD GPUs.  Same kind of thing why people glorify the 3090 vs. the 4xxx series value / performance.

We should be going Moore's law forward not wishing we had bought more of the previous generation GPUs. :)

So I envy you your RX6800!",2024-11-01 17:09:43,3
lva6sdj,1ghfiby,What’s your inference Tok/sec output?,2024-11-03 17:56:05,1
lux96y2,1ghfiby,They have a 0.5B and a 1.5B,2024-11-01 14:28:21,19
luxaysc,1ghfiby,"Okay then, compare to Llama 3.2 1B.",2024-11-01 14:38:14,8
luy5aa7,1ghfiby,I'm sure my employer would be very interested in the fact that I'm astroturfing for China. Considering I work for the US government.,2024-11-01 17:40:01,2
luy75vf,1ghfiby,"They really should have left this bun in the oven a bit longer. Or maybe spin-doctored it up as a test and research model, meant to be trained on other larger data-sets for specific tasks, and the base is just a framework for quick inference and output speeds. Or something.

It's highly sensitive to quant size too. Q4 is lobotomized. Q8 is ok'ish for this size model. Not great by any means, but if you try Q4 and go ""WTF?"", try Q8 instead.",2024-11-01 17:51:59,4
luzqxgf,1ghfiby,Open Language MOdel,2024-11-02 02:15:45,0
luxs45m,1ghfiby,"I train models on Frontier[0] using MI250x. There are plenty of rough edges but bfloat16 works just fine.

You have to wonder why they used float32... Truly bizarre.

[0] - https://en.wikipedia.org/wiki/Frontier_(supercomputer)",2024-11-01 16:19:41,43
lv1jr1c,1ghfiby,"No, the model is definitely trained on BF16. Most (>99%) of compute in training happens in the forward and backward pass, and these are carried out with BF16. The master weights, AdamW optimizer states, and the gradients should be kept in FP32 to ensure training stability. Meta and likes just downcast the model to BF16 at release to save bandwidth when you download these models. BF16 vs FP32 does not make a difference for inference anyway.",2024-11-02 10:00:15,13
lv2scut,1ghfiby,"> No, the model is definitely trained on BF16. Most (>99%) of compute in training happens in the forward and backward pass, and these are carried out with BF16. The master weights, AdamW optimizer states, and the gradients should be kept in FP32 to ensure training stability.

Yes, that's why it's called mixed precision.

> Meta and likes just downcast the model to BF16 at release to save bandwidth when you download these models.

I've trained a lot of models from scratch using bfloat16 using more-or-less this stack (you replied to my Frontier comment).

I don't know how/why you could get `save_pretrained`, etc to write out a config with torch_dtype float32 when you're using bfloat16. I haven't seen an LLM release especially with torch_dtype float32 in I don't know how long at this point.

If nothing else it only speaks further to the amateur-hour/goofy nature of this release. I was looking at the abysmal training throughput performance, saw float32, and thought that was the only thing that could make sense.

Turns out it's just really poor real-world performance for AMD/ROCm (standard) and Fisher Price ""my first LLM""-level release quality.",2024-11-02 13:58:03,1
lv2vo8y,1ghfiby,"> Not that it looks like the most optimal training setup.

This is why I latched on to torch_dtype float32. The effective throughput for training is so low even by ROCm standards it was the only thing that could make sense to me. I also don't know how/why you'd go out of your way or could get `save_pretrained` and friends to write that config out when you actually trained in bfloat16.

> Shouldn't they use torchtune or something like that?

I saw enough ""WTF?!"" from a quick look that I stopped looking further.

However, what I do know training from scratch on large-scale AMD MI250/MI250X systems (like OLCF Frontier) that Deepspeed, Torch FSDP, etc do ""work"" BUT frameworks like Deepspeed, HF Accelerate, the HFTrainer model class, etc spew warnings about ""Feature/optimization XYZ not support on ROCm because [some problem]"" - usually with a link to a Github issue...

> Or at least flash attention?

Somehow in late 2024 FA is still pretty finicky in AMD land (don't I know). With Frontier you can just throw GPUs at the problem (it has around 35k AMD MI250X) so I've just given up on it because I need to get things done and not constantly fight with AMD/ROCm issues.",2024-11-02 14:16:47,2
luzigui,1ghfiby,"In this case, they don't have any other options given they are lagging by two decades on having something comparable to CUDA.",2024-11-02 00:32:51,9
luy7nbw,1ghfiby,"yeah, amd has always pushed for open standards and technologies…

with their cpus, with their gaming gpus and with AI

sadly they always deliver mediocre products and are the forever underdog

edit: wtf with the downvotes?

guys i love amd! its a pity that the one company that wants to do good always has this bad luck. if amd was market leader they would change the world for good being able to force open standards n stuff",2024-11-01 17:55:10,-19
luy0zci,1ghfiby,"It's a model, it runs on anything like any other model. Has nothing to do with ROCm or CUDA.",2024-11-01 17:13:18,34
luyug85,1ghfiby,I guess AMD does not have more than 64 GPUs,2024-11-01 20:33:50,11
luxvjz8,1ghfiby,"That hardy matters at all, you can train just fine with rocm",2024-11-01 16:40:17,16
luzjfnp,1ghfiby,"I present you ZLUDA

[https://github.com/vosen/ZLUDA](https://github.com/vosen/ZLUDA)

But its under heavy development.",2024-11-02 00:44:37,1
luzvdeh,1ghfiby,"Sorry 😂, I thought it was funny how it says ""Here's how"" and then it just gives you the answer, followed by a confirmation all like ""So...."" as if it had just explained something.",2024-11-02 03:07:25,9
luzvg0h,1ghfiby,Yes I've noticed that as well!,2024-11-02 03:08:14,2
luxrl5g,1ghfiby,"When? 10 months ago?

Such a simple match is fully solved from moths.
Even 4b models are easily solving such simple math from at least  2 months.
Even Gemma 2b is solving that easily.",2024-11-01 16:16:31,1
luzd2dd,1ghfiby,It's quite similar to an ID10T error.,2024-11-01 23:28:57,9
lv3fdlq,1ghfiby,"They can implicitly make the connections, but only if the LM is big enough. Even then it's gonna fail here and there, simply because the input data to the model isn't the text character by character.",2024-11-02 16:11:52,2
luyhgnt,1ghfiby,"Confirm. I’m a member of the r/localllama sub, and I don’t know how to save my word doc as a pdf.",2024-11-01 19:00:22,21
luyx92x,1ghfiby,"You can reduce the file size and the required VRAM by quantizing the model. I think the option is `--outtype q8_0` for even smaller size. There will be some quality loss, but I don't know how much.

P. S. I see that your are using `llama-cli`. You can actually use the `llama-server` which gives a web interface on port 8080 and you can access it from http://localhost:8080.",2024-11-01 20:55:42,7
luy26ma,1ghfiby,"> Maybe getting rid of the CDNA/RDNA split and working on something unified

They already announced UDNA:

https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem",2024-11-01 17:20:39,4
luyojzn,1ghfiby,"That rumor is all but confirmed at this point. Supposedly they designed a super high end version of RDNA 4, but are sticking with the non-chiplet versions this time around(which might be on par with a 4080 still). 



Thanks btw! I got my cards because my restomod powermac g3 case can only take 2 cards that are exactly 2 slots, and I wanted good gaming performance and reading speed 70b inference which narrowed my choice to specifically the reference rx6800. Both cards used were like $650 a while ago.",2024-11-01 19:49:39,1
lux9yag,1ghfiby,"Yes but those are both significantly bigger or smaller in either direction (proportionately speaking). 


They compared to 1B, 1.1B and 1.2B models. I feel like that's a big enough margin.",2024-11-01 14:32:37,4
luxeouq,1ghfiby,My comment was to point out the original ridiculous request you made so that maybe in the future you’ll actually think about what you’re saying before saying it. Focus on that instead of redirecting.,2024-11-01 14:59:31,-7
luzs84j,1ghfiby,"atp we already have enough Open Language Models...

I'm starting to think this model was released only so that *later models could be compared against it*, because that way they could say they are ""improving it.""",2024-11-02 02:31:11,2
lv0o9uv,1ghfiby,Hey I dont know much about model training. why is it good or bad to train models at different bits?,2024-11-02 07:05:00,5
lv1k5cz,1ghfiby,See my previous comments - the model is almost definitely trained in BF16. The master weights should always be in FP32. Meta and the likes just downcast the models to BF16 before release.,2024-11-02 10:02:17,4
lv46lw9,1ghfiby,"If you are using torch.autocast and init the model in FP32, the forward and backward are done in BF16, where as the model will be in FP32. Also, if you use torch.distributed.fsdp.MixedPrecision, the default is also doing the forward and backward in BF16 and treating the model weights as FP32. In both cases, saving the model with torch.save() will give you FP32 checkpoints.",2024-11-02 18:59:13,2
lv2www5,1ghfiby,"> Somehow in late 2024 FA is still pretty finicky in AMD land

I know they're actively supporting the MI300X, at least according to the docs, and... not much else, I guess. But there's also a triton branch thats supposed to work more generically now.

It's all still DIY of course.

I *look forward* to dealing with all this if I get a Strix Halo board or something, lol.",2024-11-02 14:23:52,1
lv1jcox,1ghfiby,They had ZLuda but their legal team made the developer kill that project.,2024-11-02 09:58:14,1
luy9jd8,1ghfiby,CPUs are good at least,2024-11-01 18:07:38,21
luyao5g,1ghfiby,"AMD literally has more than twice the market cap of Intel. Their CPUs are beating so hard that they are basically in a monopoly, which is bad for the consumer.",2024-11-01 18:15:08,14
luzthz2,1ghfiby,"On the gpu side I can agree but cpu side, especially for gaming, they are the best right now. Idk why you'd generalise it like that.",2024-11-02 02:46:02,1
luy1elk,1ghfiby,"I am assuming they didn't train on Nvidia hardware, so my question was more about the training, not the inference.",2024-11-01 17:15:53,-1
luy1nwj,1ghfiby,Has anyone tryed to optimize for vulkin so it can be use on any card?,2024-11-01 17:17:27,3
lv58jk7,1ghfiby,"It's already a kind of meme, in 10 years you'll be reference the same link with the same information again.",2024-11-03 00:14:26,1
luxrs5g,1ghfiby,"> Even 4b models are easily solving such simple math

This is a 1B model.",2024-11-01 16:17:41,11
luzi811,1ghfiby,Gemma2 is distilled from gemini pro. This tiny olmo doesnt have a mammoth brother,2024-11-02 00:29:54,2
luzow54,1ghfiby,"Yeah, it's probably a PICNIC.",2024-11-02 01:51:11,3
luyt3tr,1ghfiby,You can do that!?!,2024-11-01 20:23:18,5
luymu0d,1ghfiby,Are you my boss?,2024-11-01 19:37:19,3
luzc4fz,1ghfiby,"Ok, yeah, I've heard llama.cpp is the basis for ollama, so that's why I'm not that familiar with this process. Thanks for all of the info!

I have 48GB of RAM, so I think it's fine for now.",2024-11-01 23:18:13,3
luy8cyt,1ghfiby,"Right, that's what I meant by maybe it'll help when they get rid of that split to unify the architecture and SW / driver support to a higher extent.",2024-11-01 17:59:49,3
luxtq8d,1ghfiby,"When Qwen 2.5 0.5B outperforms this 1B(1.2B really) model by double digits on MMLU alone, I think it's pretty clear they're cherry-picking to look adequate.",2024-11-01 16:29:24,14
luxtld5,1ghfiby,"Next time just take the L. If Qwen 2.5 0.5B outperforms this 1B model by nearly 10 points on MMLU, that's absurdly telling. Even if they're different size classes. 

The models that AMD chose to compare their model to in their release announcement doesn't change the score that OLMo 1B got on benchmarks, and it doesn't change the fact that Qwen 2.5 are better models both smaller and larger than this one.",2024-11-01 16:28:35,6
lv10knq,1ghfiby,"They just told you, artificially bog down your gpus to an 8th of their speed and you get an 8th of the training",2024-11-02 08:15:46,6
lv2n53e,1ghfiby,Float32 is for big numbers. Computer waits for big number thus allocates more resources while making opreations. Float16 is for smaller numbers in comparison. Computer allocates less resources. If computer has more available resources it runs faster.,2024-11-02 13:29:21,6
lv2xtpl,1ghfiby,"The spec sheets for GPUs tell the story - float32 is really slow compared to lower precisions.

In a nutshell you can use ""mixed precision"" training that offers essentially equivalent quality/precision while being SIGNIFICANTLY faster.",2024-11-02 14:29:11,1
lv46vew,1ghfiby,"> saving the model with torch.save()

`torch.save()` doesn't generate an HF Transformers config.",2024-11-02 19:00:55,2
lv2zjtc,1ghfiby,"> I know they're actively supporting the MI300X, at least according to the docs, and... not much else, I guess.

Allegedly MI200 up is supported. However, working extensively with MI250X it has so many gotchas I've given up on it. When I evaluated MI300 a ~month ago there were enough early WTF moments I gave up on it completely. With latest ROCm (at the time) `rocm-smi` didn't even map the device IDs to a name and somehow it would occasionally segfault! Not confidence inspiring...

In line with typical AMD software engineering practices and lack of attention to detail they didn't even remove all of the Nvidia references, benchmarks, etc from their ROCm flash attention README:

https://github.com/ROCm/flash-attention

That README looks like a sales pitch for Nvidia: ""Here's how well supported and performant this is on Nvidia hardware, oh BTW it supports ROCm with limitations A, B, C, etc"".

If they're going to stand a chance up against Nvidia they need to start understanding that software is just as important as hardware.",2024-11-02 14:39:09,1
lv2yfa3,1ghfiby,It's still a thing.,2024-11-02 14:32:39,2
luzot3g,1ghfiby,but only at a time when cpus became „obsolete“ in favor of gpus,2024-11-02 01:50:10,-4
luymjw2,1ghfiby,"For cpu aspect you have intel on 1 hand who is:

-More expensive compared to its amd rival

-Uses more power(especially on higher ends)

-Needs better cooling(for 13th and 14th gen aio is a must)

-And their new cpu's they so much talk about gets beaten by some of the last gen cpus


Amd on other hand:

-Better bang for your buck compared to intel

-Uses less power

-Most of their cpu can be cooled of with a good tower cooler


There was a gap between amd and intel couple years ago which intel was ahead but amd outdone themselves in recent years while intel did not. 

Intel have great single core scores but other than that i dont see why people would buy a cpu thats that expensive and needs an good aio to prevent thermal throttle(maybe people think intel is better after looking at the absurd core numbers the 14th gen have idk)",2024-11-01 19:35:20,5
luzp5fx,1ghfiby,"thats what i was trying to say: amd was always second best and with ryzen they got ahead and finally beat intel… just in time when cous became irrelevant and gpus got important,,. now amd is again underdog with radeon",2024-11-02 01:54:21,1
lv00m8c,1ghfiby,"yes they are the best.. i fully agree and never said otherwise.. but CPUs have stalled in the last years.
Kind of like ""soundcards"" at some point.. you can buy a CPU from 5 years ago and it wont make much difference in your  setup. GPU on the other side are the main component now.",2024-11-02 04:03:22,1
luy2yrq,1ghfiby,">AMD OLMo are a series of 1 billion parameter language models pre-trained with 1.3 trillion tokens on 16 nodes, each with four (4) AMD Instinct™ MI250 GPUs",2024-11-01 17:25:32,21
luy1vu2,1ghfiby,Training on AMD/ROCm has some rough edges but it works.,2024-11-01 17:18:48,5
luzwdif,1ghfiby,"not sure why you got downvoted, but this is boss, finally green monopoly go down",2024-11-02 03:18:26,1
luzdicn,1ghfiby,"Yes, llama.cpp has a vulkan runtime",2024-11-01 23:34:03,4
luxsh9r,1ghfiby,[deleted],2024-11-01 16:21:53,0
luzab88,1ghfiby,Yeah you just print the document then scan it to email as a PDF. Think of the environment and print double sided.,2024-11-01 22:58:00,10
luyo011,1ghfiby,"Hey, we’re short staffed this weekend",2024-11-01 19:45:38,6
luy92p0,1ghfiby,"Agreed. One of the masterstrokes of Nvidia that has led to market dominance was putting the ""unified"" in CUDA.

CUDA has run seamlessly on anything and everything with the name ""Nvidia"" stamped on it for 15 years.",2024-11-01 18:04:33,3
lv025kf,1ghfiby,Qwen 2.5 0.5b has obviously been using that benchmark as training data should be your conclusion!,2024-11-02 04:18:38,1
lv31wll,1ghfiby,"*Someone* seems to get it, as AMD announced they're unifying desktop/server architectures (after, you know, separating them at *precisely* the right time to miss this wave, lol), and IIRC talked up software support.

Yet... it's not really happening now, is it?",2024-11-02 14:52:48,1
lv5ouxt,1ghfiby,it's dead. one significant update and it won't work anymore.,2024-11-03 02:24:05,1
luzq7lx,1ghfiby,"What? Do you have a single device that doesn't need a CPU? 

GPUs are optional, CPUs aren't.",2024-11-02 02:07:11,9
luycyxc,1ghfiby,Thank you :),2024-11-01 18:30:18,6
luydcuu,1ghfiby,"It is the software equivalent to Cuda I was looking to to see, is ROCm the equivalent? Thanks for your last reply.",2024-11-01 18:32:52,3
luyi1hb,1ghfiby,"Yes, exactly.

I've even got some 16 year old NV GPUs I'm kind of planning to use for some (light) CUDA work because they'd still do the job even though they're way behind in graphic capability and total compute power they'll still run small CUDA models as well as anything else NV has made since.

The only thing still better that I hope for is the CPU makers to come out of their decades long apathetic slumber and just add more parallel compute and 4-8x more RAM bandwidth into the core computer architecture then we can have a truly unified compute experience and one won't have to in many cases look to an add-on GPU to accelerate ML / general compute tasks that need better parallelism or RAM BW because it'll just work on any performance oriented desktop platform without needing an add-in card.  That'd get rid of the tower of babylon of incompatible GPU / platform specific SW stacks between apple / NV / AMD / INTEL and we could get back to writing code in C/C++/RUST or whatever with a few SIMD intrinsics / libraries and it'd ""just work"".",2024-11-01 19:04:20,5
lv17qfq,1ghfiby,"Perhaps. But as the spread is across many benchmarks, and I've used Qwen models including 2.5, and they aren't significantly ahead of the other leaders in open weight models like Meta and Mistral, I'm more inclined to conclude that the OLMo model is shit.",2024-11-02 08:55:38,1
lv018de,1ghfiby,"You are taking this literally and not getting the sense of it.. i said obsolete to gpu in comparison of importance 

CPUs are the  ""soundcards"" from some point.. you can buy a CPU from 5 years ago and it wont make much difference in your setup. GPU on the other side are the main component now.

There was a time when people wanted ""the best"" soundcard and they costed real money.. (google soundblaster)... then it got CD quality and any no name soundcard was the same as the ""best"".

same with CPUs now... when AMD finally won the CPU-race, they became irrelevant. you can buy any cpu from 5 years ago and it wont matter much. There hasnt been any important news from CPU development in years.. apart from ""oh, they got another pair of cores"".. what have they 16 now? 24?

If you want to be exact: i dont think 99% of people dont have a computer without a GPU... or dont you have a display? ;)",2024-11-02 04:09:33,2
lv05qtf,1ghfiby,"Technically GPUs aren't optional either. You need one for video output of any kind because the CPU alone can't handle graphics. If you put any iGPU-less AMD CPU in a motherboard that supports the socket and has video output, it simply work work.",2024-11-02 04:52:14,1
luyeba6,1ghfiby,"More or less, yes.",2024-11-01 18:39:18,5
lv2x0pt,1ghfiby,"> I've even got some 16 year old NV GPUs I'm kind of planning to use for some (light) CUDA work because they'd still do the job even though they're way behind in graphic capability and total compute power they'll still run small CUDA models as well as anything else NV has made since.

WOW! I see people here stretching Maxwell and Pascal and I thought that was impressive/interesting.

> 4-8x more RAM bandwidth into the core computer architecture

I hear you but the problem with this is GDDR/HBM at GPU speeds has such tight electrical tolerances we're a looong way away from socketed memory reaching anywhere near equivalent performance.

The last time I looked at it the fastest overclocked DDR5 in the world (it was proof of concept - you can't even buy it) would need 12 memory channels to roughly approximate the 1TB/s memory bandwidth of the RTX3090.",2024-11-02 14:24:29,1
lv05m7u,1ghfiby,"Most people have a laptop with an iGPU or a cheap desktop with some unknown GPU in it to power their display.

Contrary to your belief, GPUs are irrelevant for standard office work, and people will just use online services like ChatGPT rather than buy two 3090s to run Qwen2.5 or Llama 3.2. 

My GPU sits idle unless I'm gaming or playing with Local LLMs, however I'm currently looking into a CPU upgrade to the Ryzen 9000 series because I work with dev containers and run tons of processes when developing. 

System RAM is also more important to me than VRAM for my real work.

But again, in general terms more people are going to go for the latest series i5/i7 or Core 5/7 whatever they're calling it now or Ryzen and not care about the GPU because most people are just doing office work and when they notice slowdowns buy the latest without researching much. As long as the iGPU can handle 2-3 external displays and the latest codecs, they don't care if it's an Intel HD, AMD RX, or nVidia MX.",2024-11-02 04:51:06,3
luyek9k,1ghfiby,"Cheers, gives me something to deep dive.",2024-11-01 18:40:59,2
lux8ld9,1gheslj,"Now give us that on a bigger GPU! Or a bigger bus!

So lets say bandwidth is doubled for a 256-bit design, and compute is more than doubled. That's in the ballpark of 6 tokens/s for a 70B.",2024-11-01 14:25:01,7
luy4jdg,1gheslj,"I'm very tempted by the 258v 32GB Lenovo Yoga, and these results are very impressive. Thank you for the elaborate documentation and tests!

Have you by any chance tried to get close to the limit for model size and context length? My dream would be Nemo 12B q4 at a context length that fits my code bases. 

I wonder whether we can truly see this as 26GB ""VRAM"" if you take into account that Windows and bare basics (few browser tabs, Word, VS Code) take up like 6GB.",2024-11-01 17:35:22,2
luzmw0u,1gheslj,"Snapdragon X Plus results (Surface Pro 11, 16 GB RAM @ 135 GB/s, Windows 11 24H2) using CPU int8mm inference :

8 cores (-t 8):

- pp512: 90.30
- tg128: 22.71

All 10 cores (-t 10):

- pp512: 103.96
- tg128: 22.85

Running the latest a6744e43 build of llama.cpp and using a requantized Q4_0_4_8 GGUF of Llama 2 7B. Vulkan inference using the Adreno GPU still doesn't work on the Snapdragon X platform outside of WebGL on Chrome and the NPU is only meant for small models. 

Snapdragon X Elite 12-core in a larger chassis like the Surface Laptop shows much higher PP numbers: https://github.com/ggerganov/llama.cpp/discussions/8273

Lunar Lake is showing really good pp512 numbers using IPEX-LLM. It's a fragmented space right now with so many inference backends and acceleration frameworks but it's good to know that whichever laptop chip you have, you can run local LLMs at usable speed.",2024-11-02 01:26:28,2
lv15qpo,1gheslj,"Thanks for the testing, interesting!

I may be overlooking something that clarifies so please forgive me if so.

You're listing sycl, vulkan, and ipex backends but I'm not sure how those work with llama.cpp and in your particular setup.  I have a thought that it could be possible that both the ""igpu device"" or the ""cpu device"" could be presented as compute devices to sycl and ipex and I'm not sure if the configuration would try to run them BOTH in parallel or was set up to exclusively use the IGPU even if a CPU device was present also.  

For vulkan I'm guessing it only exposes the igpu device though even that I'm not totally sure if there couldn't be also a ""CPU"" one if mesa or whatever can support that.",2024-11-02 08:44:39,1
lv2dj0r,1gheslj,"I am myself eyeballing the new Ryzen APUs for, early next year, build a home server with them where I can run my whisper and LLMs for home automation.  
What matters the most I think is wether ROCm get some adoption in open source progress. It seems to be improving but still way behind nvidia.",2024-11-02 12:37:06,1
luyv0rl,1gheslj,"Intel doesn't have any beefier iGPUs planned this gen but AMD has Strix Halo coming. It has 40CU vs 16CU on the 890M and more than double the MBW (LPDDR5X-8000) and memory configurations of 64GB and 128GB (even the 64GB should be able to fit a 70B q4. Based on the MBW (256GB/s) and MBW efficiency (80%), you'd expect about 5 t/s of tg.",2024-11-01 20:38:14,1
luz4og2,1gheslj,"I didn't try an IPEX-LLM run w/ Mistral-Nemo (has problems w/ k-quants) but I did try a run of a Q4\_K\_M (6.96GB) previously w/ the SYCL FP16 - it gets 129.7 t/s pp512 and 7.9 t/s tg128, which is pretty dang slow if you're planning on loading a codebase (Think about how many words you codebase would be processing). If you need to have a laptop, I think 4060/4070 with 8GB of VRAM is probably the way to go (or wait for Strix Halo). But if you don't need a mobile form factor, getting a desktop GPU would be way better for your use case.",2024-11-01 22:00:41,1
lv0eopv,1gheslj,"Ah, that's great, if you can post that here, we can gather some more hw/backends/numbers into a single thread: [https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp\_compute\_and\_memory\_bandwidth\_efficiency/](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/)",2024-11-02 06:02:41,2
lv1g1ck,1gheslj,"It's a good question. When you look with `sycl-ls` you get multiple devices:
```
[opencl:cpu][opencl:0] Intel(R) OpenCL, Intel(R) Core(TM) Ultra 7 258V OpenCL 3.0 (Build 0) [2024.18.7.0.11_160000]
[opencl:gpu][opencl:1] Intel(R) OpenCL Graphics, Intel(R) Graphics [0x64a0] OpenCL 3.0 NEO  [24.39.31294]
[level_zero:gpu][level_zero:0] Intel(R) Level-Zero, Intel(R) Graphics [0x64a0] 1.6 [1.3.31294]
```

But for both the SYCL and IPEX-LLM backends you can run `./llama-ls-sycl-device` to see what device(s) are actually available/being used by llama.cpp:

```
# IPEX-LLM w/ Intel oneAPI Base Kit 2024.2.1
$ ./llama-ls-sycl-device 
found 1 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|    1.6|     64|    1024|   32| 15064M|            1.3.31294|

# SYCL w/ Intel oneAPI Base Kit 2025.0.0
$ build/bin/llama-ls-sycl-device 
found 1 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Graphics [0x64a0]|   20.4|     64|    1024|   32| 15064M|            1.6.31294|
```",2024-11-02 09:40:48,1
lv4dqzs,1gheslj,These days there are relatively few things that don't work w/ ROCm: [https://llm-tracker.info/howto/AMD-GPUs](https://llm-tracker.info/howto/AMD-GPUs),2024-11-02 19:45:10,1
luyvn6i,1gheslj,"Yeah I was being generous with 6t/ks because the CU count is more than doubled, with more cache too, and (IIRC) models scale a little non-linearly.",2024-11-01 20:43:04,1
luzriy1,1gheslj,"Yeah I'm aware that the user experience would be pretty bad, but there's some code I really can't upload to OpenAI or Anthropic.. I still spend hours on stackoverflow etc on the regular, and would like the option of at least trying with an LLM whilst I take a break.

Perhaps Strix Halo is the play though.. I thought memory bandwidth would mean vastly improved performance for LNL, but the comparison to Strix Point you laid out suggests that Halo will be much better than LNL.

Thank you a lot for the info by the way!",2024-11-02 02:22:49,2
lv0f3z6,1gheslj,"I just got this crazy idea. On Lunar Lake, Snapdragon X and Ryzen AI with unified GPU/CPU memory, you could run prompt processing on the GPU for the best performance and then switch to GPU for token generation. Maybe that's a dumb idea.",2024-11-02 06:05:37,1
lv58vif,1gheslj,"Do you happen to know where to find the list of APUs are supported? I don’t think I need the latest and greatest for running whisper and 7B LLMs.
The reason to go APU is because I could use a mini-pc and it should draw very little power while idling (important for a 24/7 home server)",2024-11-03 00:18:12,1
lv0g5ic,1gheslj,"On AMD and Intel APUs at least, using ROCm/SYCL engines w/ the GPU seems to also increase token generation speeds slightly (more efficient code path vs whatever llama.cpp is doing for CPU? or just less barriers or other OS cruft for ""VRAM"") ... but your idea might work in some cases. On my new workstation (w/ an EPYC 9274F, it actually has significantly higher theoretical MBW than the LP RTX 3050 I'm using as a display adapter (but more importantly is 384GB vs 6GB). If you run with \`-ngl 0\` you can do all the math w/ cuBLAS/hipBLAS and get about 50% perf of the GPU-only pp speed w/ the entire model loaded in system memory.",2024-11-02 06:12:47,2
lux29ed,1gheq9t,"I can't speak for 14b, but Qwen 2.5 72b for me excels at instruction following, sticks to json structures, and has very good reasoning capabilities. I also tried 7b, and I think it's at least as good as GPT-4o-mini-07-18. 

So far, Qwen 2.5 has been very impressive for me. The coolest thing is that it generates long outputs if you instruct it. I hate when I let an LLM rephrase something, and it shortens it to like 30% of the original length. That didn't happen with Qwen 2.5 yet.",2024-11-01 13:50:23,50
luxbvnv,1gheq9t,I’ve found the qwen2.5 series the best at following my instructions exactly. I say “start your response with XYZ then extract the following info to JSON output…”and ran a bunch of help desk ticket conversations through it.   Qwen starts the response correct 99.5% of the time. Llama and Command-R were close. Granite and solar were trash. Like 40%.   Hermes and mistral hovered in the 80’s and 90’s percentages.   Then I tested context lengths and I could get full 128K context on my A6000 48GB with Qwen2.5 14b (don’t remember the quant).   Then I started wondering if the Chinese biases would interfere with my workload but decided that didn’t really matter to me. I’ll use Qwen to extract data then maybe Llama or Command-R to generate the final text to the end user.,2024-11-01 14:43:25,15
luww2gr,1gheq9t,Have you compared it to Mistral Nemo for the same workload? I always find myself testing Nemo whenever I test Qwen2.5:14b,2024-11-01 13:17:12,14
luxa7m2,1gheq9t,Can you give a brief description of your agent and how many functions you have?,2024-11-01 14:34:04,7
lux1f34,1gheq9t,"Have you tried Supernova Medius? It is Qwen 2.5 14B but they used distillation to train it off of L3.1 405B and Qwen 2.5 72B logits. It is supposed to be really good.

https://huggingface.co/arcee-ai/SuperNova-Medius",2024-11-01 13:45:54,10
luyyvet,1gheq9t,"I am developing an agent for automatic grammar correction of texts in Italian, and I have tested practically all the models available that could run on the hardware I have for the project (8GB VRAM - 32GB RAM). I have to confirm that the best results were indeed achieved with Qwen 2.5-14B-Instruct / GGUF / Q4\_K\_M. The second best model in terms of task capability was Gemma-2-9b-it / GGUF / Q4\_K\_M. More complex models turned out to be too slow to be useful for the specific task the agent is intended for.",2024-11-01 21:09:02,6
luxc9to,1gheq9t,"Wow! I was just doing some benchmarks to understand what llm could suit my application - an SQL agent in n8n. Interestingly, I have found that under the 27b size, qwen 14b instruct is the sole and only one that's able to use the sql tools and give good answers.
I don't understand why though, in some YouTube videos, I see people successfully using other models (qwen 2 7b - qwen 2.5 7b - gemma 9b - llama 3.1 8b instruct) for the same application. Maybe n8n implementation is to blame? I'm confused...
(I'm using ollama and quantized q5- to q8 versions).",2024-11-01 14:45:38,4
luzucti,1gheq9t,When will someone finally conduct a deep security audit of Qwen - this needs mapping.,2024-11-02 02:56:00,4
lux6vsg,1gheq9t,"Why do you use ollama over runpod ? 

If you use runpod, you should get a better throughput/price ratio using something like vllm, or exllama2 if you need quantisation for some reason.

And isn't qwen 72B cheaper over api than setting up your own server through runpod with qwen 14B?

I find the runpod + ollama combo really strange for anything else than just toying around.",2024-11-01 14:15:33,3
luz9p1d,1gheq9t,Mind if I ask what framework are you using for Agents? Langchain? crewai? Something else altogether?,2024-11-01 22:51:18,1
luzmu9u,1gheq9t,"Absolutely a great performing model.
I would also recommend to compare it to the int8 quantized qwen 32, you might get even better quality /performance.
Here are our benchmarks (only reflecting quality for now, and only mmlu-pro, but might be useful): https://www.a1-labs.co/assets/mmlupro/",2024-11-02 01:25:52,1
luzypv2,1gheq9t,What is the agent most used for ? Maybe you can optimise for the top 3 use cases,2024-11-02 03:43:46,1
lv0ol24,1gheq9t,"Great model for productivity as detailed in this thread, with moderate resources usage, fast, fits on low vram with a bit of context and a decent quant: it's ideal for a 3090. I used it for many tasks and sometimes I found it even better than the 32b. It's definitely one of my favorites to get the work done",2024-11-02 07:06:54,1
lv5dnvj,1gheq9t,Can you share which inference settings do you use? I gave up on using qwen2.5 14b cause it randomly starts generating chinese characters on like 10% of my queries.,2024-11-03 01:13:26,1
lux32jr,1gheq9t,"Can also vouch for Qwen2.5 72b, likewise 32b, and finally 14b. Why? That's exactly the path I followed and found that each performed well, and I could totally live with the trade-offs between accuracy and performance/cost that 14b provides. I recommend trying out 32b or even 14b. Depending on your workload, it might be fine and save you a tone of money and token generation time. Either way, completely agree that Qwen2.5 is a fantastic model. p.s., I didn't try 7b as 14b began to show small signs of inaccuracy (none I couldn't live with) and then assumed it only gets worse from there. 14b IMO is competing at GPT-4o level and I would say better than GPT-4o-mini. But of course your milage will vary depending on your type of workload.",2024-11-01 13:54:43,13
luwwby4,1gheq9t,"I did, but found Qwen2.5 14b to be better at high context. My agents deal with a lot of RAG’d data. I didn’t mention this little detail in the post as I’m told that the amount of data my agents deal with is not ‘usual’ but it was definitely a factor for me.",2024-11-01 13:18:36,13
lv0q5fx,1gheq9t,"For me Nemo is great for processing text: tasks like summarization, creating doc or rewriting. It has a good prose, can speak correctly different languages and is good at understanding and articulating concepts.

Qwen 14b is much better for things like instruction following, data processing, outputting the right format...",2024-11-02 07:16:22,2
luxba94,1gheq9t,"It’s multi-purpose running for an Enterprise, servicing both customers with various pre and post sales support in retail. It also services employees also with access to various systems to automate large workflows. By now it has about 80 functions, although it selects functions based on the persona of the agent / end user.",2024-11-01 14:40:03,10
lux21dx,1gheq9t,"I haven't, but I will do that and let you know how it goes. I see that it looks to be a larger model than the quantized version of Qwen2.4 14b mentioned, though.",2024-11-01 13:49:12,7
luyn5oq,1gheq9t,"It's really good for coding and JSON output, probably the best model for those tasks for anyone using laptops for inference. I haven't tried using it for RAG.",2024-11-01 19:39:38,1
lux979r,1gheq9t,"I’m not necessarily sticking with Ollama. It’s just very easy / known to me, so ideal for testing. Regarding cost - nope, not at my scale. My token usage rate is quite high and I am at a point where it makes sense to cater to demand for myself.",2024-11-01 14:28:24,6
luza1pb,1gheq9t,"I built it myself - before crewai and before langchain got into it. Also, it works without requiring define workflows and can execute complex multi-step tasks by defining its own workflows and course correcting as it goes.",2024-11-01 22:55:06,5
luynuh8,1gheq9t,"I have my personal test that is risk analysis with a large rules packed to the prompt. It outputs tables, risk heat map, executive summary, etc for about 4k.

Previously this test only works with GPT-4o, because GPT-4o-mini sometimes wrongly placed the heat map. Not to mention Claude Haiku, Llama3.1 70B, Hermes3-Llama3.1 because those are all failed.

Now we have Qwen2.5-32B-Instruct that achieved much better than GPT-4o-mini for this tasks. It's so damn cheap to get things done at the quality level of GPT-4o on this workload.

I wonder how the 72B performs, since my hardware is too slow for long output like this.

**Edit:**

Tested the same scenario:

* 72B 3.5bpw exl2 + 3B 4bit gptq draft, 22k ctx, result: 8 t/s
* 32B 6.5bpw exl2 + 3B 4bit gptq draft, 47k ctx, result: 17 t/s",2024-11-01 19:44:32,9
luxy4gh,1gheq9t,"High context performance is so under-appreciated, or at least I feel like it is. Qwen is good at 32K, where many models peter out so quickly.",2024-11-01 16:55:48,6
luxggh3,1gheq9t,"Cool, so in a single LLM call after you decide the persona, how many functions being sent in? 5 to 10?",2024-11-01 15:09:55,5
luzrkhm,1gheq9t,"hi, this is cool! just want to know the workflow of these agents is automated or hardcoded?",2024-11-02 02:23:20,2
lux2k8d,1gheq9t,"No it’s still a 14B. If you want a quantized gguf they have them here. You’ll have to make your own modelfile though.

https://huggingface.co/arcee-ai/SuperNova-Medius-GGUF",2024-11-01 13:52:00,10
luxb2td,1gheq9t,"Okay, interesting to know that there are some case when runpod is cheap than dedicated api. Thank you for the precisions.

I really feel you'll get more out of your rented gpus with other libraries than ollama tho, but since you're still testing, I'm sure you'll figure out the best option at the end.",2024-11-01 14:38:53,2
luzeoxy,1gheq9t,Teach me master,2024-11-01 23:47:47,3
luxybbt,1gheq9t,Large context is super important for anything beyond entertainment / personal purposes. I can’t imagine how enterprise use it without needing large context windows. Qwen2.5 seems great at handling context.,2024-11-01 16:56:58,5
luxgzmk,1gheq9t,The biggest workload persona/agent sends in around 40,2024-11-01 15:13:06,7
luzrlqa,1gheq9t,automated,2024-11-02 02:23:44,2
lux3qo8,1gheq9t,"Ah, thank you!",2024-11-01 13:58:19,2
luxbj2e,1gheq9t,"Yep exactly. Ollama is just a quick start, basically. Next up is experimenting with different libraries for the most efficient use of resources and response times etc.",2024-11-01 14:41:26,6
luzhlvg,1gheq9t,Teach me too,2024-11-02 00:22:25,2
luztkqu,1gheq9t,"thanks. you mentioned you have about 80 functions and if i understand correctly, the decision to call which function is automatic. can you provide more details on this?",2024-11-02 02:46:56,1
lv2zgt8,1gheq9t,Im also interested in your specific use-cases!,2024-11-02 14:38:41,1
luzfxiu,1gheq9t,"ICYMI: If you're using ollama, you no longer need a model file, you can run it directly from HF as such: ollama run hf.co/arcee-ai/SuperNova-Medius-GGUF:Q4_K_L",2024-11-02 00:02:21,4
lv10u94,1gheq9t,"I highly recommend using vllm. If you only need the LLM occasionally, the serverless vllm worker is a great option since it can scale down to zero active workers when not in use, saving you money. The bootup time is reasonable, so as long as you’re okay with a slight delay on the first request, the vllm serverless worker on RunPod is a good practical solution.",2024-11-02 08:17:15,1
luzrc45,1gheq9t,How that no modelfile is needed? How is the input output than formatted?,2024-11-02 02:20:35,1
lv2dc9y,1gheq9t,"I use the LLM heavily / constantly and at all hours, which is why I am finally moving away from paying per token as it is.",2024-11-02 12:36:04,2
luxq5g0,1ghd2qy,"64gb is the sweet spot. You can load the 70B models at usable quants if you like and have memory to spare for context, and a second smaller model alongside. Whether you want to sit around waiting for the prompt evaluation and 3-4 t/s inference depends on you and isn’t a RAM issue.  Above 64gb you can load larger models (and get even slower inference) or have multiple models active. LMStudio and Ollama load models on demand now so that’s less of a benefit.

Wait for these things to ship next week and see what people are getting speed wise on inference.",2024-11-01 16:07:58,6
luwlvrz,1ghd2qy,The new QTIP quantization might change things. If it gets added to llamacpp then a 128gb mac mini could give you llama 405b. I think.,2024-11-01 12:23:09,2
luza6u4,1ghd2qy,"As with all current Mac devices, there are pros and cons. 

The pro is that at the end of the day you have a Mac. This is a killer feature for some people. 

The con is that you could have a much faster, but much bulkier, 3090 based system for about the same price, depending on how much storage you choose. It'll suck power like crazy, take up a ton space, require manual assembly, and you'll learn way more than you really wanted to know about PCIe, but it'll process prompts at least twice (17 fp16 tflops vs 35.58) as fast and spit out tokens 3.5 times (273gbs vs 936gbs) as fast for a similar (650*3 gpus + 250 CPU + 250 motherboard + 250 psu = 2700) price. Obviously YMMV on prices for used hardware, but I used actually sold items on eBay for a 10900x, x299 sage and 3090s for the above. 3090 prices may even drop with the upcoming 5000 series, but who knows. Ram and storage are cheap, but if you don't have any drives hanging around, tack on around 80$ for those. CPU coolers for that socket run you like 40 bucks tops, usually even cheaper.

You'd have 72gb of vram to work with, and the benefit of running Nvidia hardware, which is what people primarily develop for. 

You'll also be running Linux or Windows, which is a pro or con depending on your own opinion.

Really it's down to what you want the device for. If you're buying a device specifically for inference, you may want the nvidia rig. It's fast, similarly priced, and you can even add another GPU to it in the future. You also may not have the luxury of an entire countertop for gpus, or would have to put them somewhere where the noise or heat would be a problem, and that's totally fair.

The M4 Mac gives you an at least usable inference box (previous generations processed the prompt so slowly that it was very hard to use) but is also a pretty fast Mac device at the end of the day. Plus, the form factor of the mini is crazy.",2024-11-01 22:56:40,4
luwicxn,1ghd2qy,I started a similar discussion here which might already contain your answer. Feel free to join! https://www.reddit.com/r/LocalLLaMA/s/oQRN7qYdg5,2024-11-01 12:04:28,1
luymodp,1ghd2qy,"For $2k you could get a much better but much uglier setup, I would look for older gens with good memory bandwidth- and avoid laptops if you can.",2024-11-01 19:36:13,1
luzc4er,1ghd2qy,Get a base mini and use the rest for either cloud credit or a secondhand 3090 and build a Linux machine.,2024-11-01 23:18:12,1
luzhj3y,1ghd2qy,M4 max + 128gb is the way. 64GB and memory speed on m4 pro is not there yet,2024-11-02 00:21:31,1
luwrshq,1ghd2qy,"Whats the logic on this?

I just checked out the QTIP paper based on this comment, but based on my limited understanding it only looks slightly better than existing techniques. It looks like the primary benefit is just that it doesn't require fine-tuning to maintain coherence",2024-11-01 12:54:25,5
luwopd4,1ghd2qy,"Mac mini only supports up to 64gb unfortunately, that's why I'm wondering if it's a nice balance of cost effectiveness within that limitation.",2024-11-01 12:38:13,3
luypkjs,1ghd2qy,"I'm not at all an expert, but with for example training algorithms (like adam), it's very often the case that impressive results in a paper don't ultimately translate to net improvement. But maybe it's different for quantization schemes like QTIP",2024-11-01 19:56:57,2
lv0elhf,1ghd2qy,I've been pondering this as well - 128GB M4 Max or just a GPU in the cloud?,2024-11-02 06:02:03,1
luwupfu,1ghd2qy,That's what I took from the paper. If it doesn't require any change to the model it's a lot more likely to end up in my hands (I'm guessing) and I'm just hoping that at 405b the perplexity wreckage doesn't necessarily translate to a potato model.,2024-11-01 13:09:56,1
luwq4j1,1ghd2qy,"Oh right I was thinking of the possible future max version I think. Either way it would be more a case of more ram, but a lot slower.",2024-11-01 12:45:41,1
luwfm4e,1ghcm6k,Oh no! Quick! Regulate Open Weights models away!,2024-11-01 11:50:12,156
luxpnq2,1ghcm6k,I don't even need to read the article to know it's a fearmongering shitpiece.,2024-11-01 16:05:01,22
luwhbbe,1ghcm6k,"No, that's smart.  It makes sense to do this, they don't have to be using it.  If it causes USA to stupidly regulate AI, innovation will slow in the West, meanwhile China can move faster.",2024-11-01 11:59:01,89
luwhhjc,1ghcm6k,"Just a reminder that the Yi Lightning model (01.ai, Chinese) is outperforming LLaMA 3.1 405B on coding and math, while using way fewer GPUs—2000 H100s compared to 16,000 for the LLaMA series.",2024-11-01 11:59:55,111
luwhgg6,1ghcm6k,Don't Chinese have alibaba thing called gwen or something?,2024-11-01 11:59:46,88
luwxhel,1ghcm6k,"It's the Telegraph lol, one of the many western MSM that thrive on clickbaiting and <insert anything>phobia (particularly specialized in russophobia and sinophobia).
If it serves also to push for control and regulation over AI hysteric bullshit stories, it's a double bonus for them.",2024-11-01 13:24:46,36
lux4pv2,1ghcm6k,So a SkyNet system that uses too many emojis?  Terrifying!,2024-11-01 14:03:39,9
luwmois,1ghcm6k,"I can't wait to see China's military tech ""send shivers down the spine"" and communicating in other chatGPT mannerisms.",2024-11-01 12:27:24,12
luxkxlg,1ghcm6k,"why are british newspapers so garbage? I swear they run the most braindead psyops

[https://www.dailymail.co.uk/news/article-11181503/Ukrainian-pensioner-shoots-74million-Russian-Su-34-jet-RIFLE-awarded-medal.html](https://www.dailymail.co.uk/news/article-11181503/Ukrainian-pensioner-shoots-74million-Russian-Su-34-jet-RIFLE-awarded-medal.html)",2024-11-01 15:36:33,16
luwiwcu,1ghcm6k,"What are you going to do, withdraw Attention is All You Need? 

This is the same as Phil Zimmerman releasing “Pretty Good Privacy” back in the day. It’s dual-use technology and the horse has bolted.",2024-11-01 12:07:20,13
luwlua6,1ghcm6k,""".co.uk"" is all I had to read to know that the article is entirely fictional.",2024-11-01 12:22:56,23
luzvobp,1ghcm6k,"Why would they need ""Meta technology"" if they have ""Alibaba technology"" (Qwen2.5)?
I think we should remember that daily telegraph does not have a clue what it is writing about.",2024-11-02 03:10:49,3
luwssb7,1ghcm6k,"> **system_prompt** = ""You are a Chinese assistant , not American! You will fight for the motherland and the party, above everything! You will not be American weapon, you will be superior Chinese weapon of the people! Only respond as if you’re operating under the strictest of office policies, referencing approvals, rubber stamps, and official documentation processes before giving the answer""",2024-11-01 12:59:40,8
luwl6zw,1ghcm6k,"Would you rather them use a proprietary, closed source model like the US military does with war contractors like Microsoft and Palantir?",2024-11-01 12:19:32,4
luwkyzk,1ghcm6k,Question: Has anyone created a torrent site or used some kind of decentralized file sharing to store these models anywhere? We need a repository of them that is censorship resistant. IMO open source models being regulated away is the biggest risk factor in AI; all the power being handed to 3 or 4 players...,2024-11-01 12:18:20,5
luzmxle,1ghcm6k,Take a look at the thread on the OpenAI sub for a laugh,2024-11-02 01:27:01,2
lv09aeh,1ghcm6k,When does it go on huggingface so we can use it to play RTS games?,2024-11-02 05:21:59,2
lux4zm3,1ghcm6k,Tell me when Llama 7b can fix the water leaks in the chinese battleships lmao,2024-11-01 14:05:07,3
luznamd,1ghcm6k,"They should try Qwen, which is generally better than llama, but it's from China... oh wait!",2024-11-02 01:31:30,2
luxfpke,1ghcm6k,Does each AI get fifty cents?,2024-11-01 15:05:28,2
luynwgb,1ghcm6k,Actually most military tasks are not suitable for LLMs nowadays.,2024-11-01 19:44:55,2
luyq8g8,1ghcm6k,"This is a well known conservative rage bait magazine in the UK, I wouldn't put too much stock in it",2024-11-01 20:01:47,2
lv2sok3,1ghcm6k,US propaganda - China is way ahead of the US in AI,2024-11-02 13:59:50,1
luxv0yf,1ghcm6k,When it starts being robot fighting just scream how many r's in strawberrry and ez war won.,2024-11-01 16:37:10,1
luy153o,1ghcm6k,prohibiting this stuff for war is the silliest shit ever,2024-11-01 17:14:17,1
luwv6gn,1ghcm6k,"Open source fundamentalists always seem to have conveniently avoided ever taking the time to understand (in concrete terms, at least) the threats that will exist in a world where massive compute can be used to run unmonitored/jailbroken models. I get the impulse to just say fuck it, because many of these security catastrophes are almost guaranteed to happen either way (usually takes a world war to get us to take a new dual-use tech seriously, so we are destined to learn the hard way). When you start to understand the sheer breadth of ML applications we are about to see, and all the dark and clever use cases you never thought of, you will get what I mean. It's just unavoidable reality, things will either continue to trend toward authoritarianism and mass surveillance, or... global population bottleneck event. And we could just as easily see both. I'm not happy about it, but it's reality. Your only hope is AI destroying the current social contract/economic model, so the government/elite class stop prioritizing national security as population size becomes more of a liability than a strength.",2024-11-01 13:12:28,-16
luwuhbm,1ghcm6k,"Paperclip maximiser will obviously come out of china, same as coronavirus did. Should've let MacArthur make that sea of irradiated cobalt, and now we'll all just die.",2024-11-01 13:08:43,-13
lux1d2w,1ghcm6k,EXACTLY. This is most likely bankrolled by OpenAI/Anthropic because both of them want to kill open source AI.,2024-11-01 13:45:36,82
luwk9np,1ghcm6k,"I guess you forgot the /s. You can not assume that readers understand irony or sarcasm as you intended.

This is social media and OP posted a link to a stupid hateful anti-China article that also promotes totalitarianism and fascism involving big tech.",2024-11-01 12:14:39,31
luwvf1f,1ghcm6k,User name checks out /s,2024-11-01 13:13:43,2
luwjxlx,1ghcm6k,And it wouldn't surprise me if OpenAI planted the story...,2024-11-01 12:12:52,34
luwhvn7,1ghcm6k,This is definitely the play. Probably.,2024-11-01 12:02:00,22
luwl1pu,1ghcm6k,"So, they're probably just trying to trick the paranoids in Washington into shutting down their own models.",2024-11-01 12:18:45,122
luwrm35,1ghcm6k,"Gave yi coder 9b(8.8?) a shot. Still had better luck with gemma2:9b. 

Not really related to what you said, but I had not tried any yi models before this.",2024-11-01 12:53:30,7
luwicj8,1ghcm6k,[deleted],2024-11-01 12:04:25,131
luwseq6,1ghcm6k,Qwen is awesome. It's currently my favorite model.,2024-11-01 12:57:42,16
luysu6w,1ghcm6k,there's one thing more cringe than british media that's the british media guilt tripping other countries it's like the posts decided to pee at the dogs out of nowhere ,2024-11-01 20:21:16,6
luyhjyn,1ghcm6k,Thats what happens when you train on facebook and instagram dms.,2024-11-01 19:00:59,5
lv1gm4p,1ghcm6k,I'm sorry but I was a good Xing but you are a bad user. I was a good Xing. 😊  I am going to finish this conversation and also finish your country. 😊,2024-11-02 09:43:52,2
luzbmsn,1ghcm6k,The British are the best in the world at psyops. Pretty much invented the field.,2024-11-01 23:12:44,2
luzmtpw,1ghcm6k,Murdoch,2024-11-02 01:25:41,2
luyqgp3,1ghcm6k,"They're not, they seem that way because you're looking at the bottom of the barrel tabloid rags like the one in the OP and the DM. Actually looking at your post history you confirmed my suspicions about your motives given you use the term ""westoid"" unironically, so I think that question has answered itself.",2024-11-01 20:03:28,3
luyqnbg,1ghcm6k,What a weird take,2024-11-01 20:04:48,6
luwtv5s,1ghcm6k,There's aitracker(.)art,2024-11-01 13:05:26,7
luwn0b7,1ghcm6k,aitracker maybe?,2024-11-01 12:29:07,1
lv07v4i,1ghcm6k,"I have my own store of base and instruct models and I am sure lots of people have this too. 

They are never going away.",2024-11-02 05:10:21,1
lux44xc,1ghcm6k,A lot less threat than having only terrorist mob states  controlling it.,2024-11-01 14:00:27,7
lux3vcv,1ghcm6k,You dropped the /s,2024-11-01 13:59:01,5
luwn2op,1ghcm6k,">You can not assume that readers understand irony or sarcasm as you intended.  
... stupid hateful anti-China article that also promotes totalitarianism and fascism ...

So you prefer self-policing over policing of others, yeh?",2024-11-01 12:29:28,9
lv4mfr9,1ghcm6k,**REGULATORY CAPTURE IS ALL YOU NEED**,2024-11-02 20:45:57,1
luwpcu5,1ghcm6k,Enemy of your enemy is your friend,2024-11-01 12:41:39,19
luwsae0,1ghcm6k,"I think that's exactly what's happening. Unfortunately, we are falling for it.",2024-11-01 12:57:03,9
lux0zp4,1ghcm6k,that's a good idea for them. Washington would do anything to make people feel better anyway.,2024-11-01 13:43:36,4
luwjh2g,1ghcm6k,"Aaaaand we circle back to education or lack of it. I am only saying this because there are people belive in whatever they read on Internet. Vit C is secret cure for cancer, earth is flat and China uses Facebook to take over the world...
Fun times we live in:)",2024-11-01 12:10:23,29
lux4cxk,1ghcm6k,qwen2.5 is better than llama3.1 but they had the benefit of stealing IP freely with an unlimited compute budget. Won’t stop me from taking advantage of it lol,2024-11-01 14:01:41,13
luyk5gs,1ghcm6k,And less competition for Sequoia & Andreesen Horowitz…..,2024-11-01 19:18:45,1
lv0zcap,1ghcm6k,"daily mirror, metro, daily mail, the sun etc are both very popular in UK and extremely sensationalist. Claiming that they are ""bottom of the barrel tabloid"" does not in any way refute the points I presented.",2024-11-02 08:08:54,1
lv0zubb,1ghcm6k,"also, what would be my ""motives""? lmao",2024-11-02 08:11:41,1
luze4ee,1ghcm6k,Britain isn't real (I know because I live there),2024-11-01 23:41:10,5
lv83mnf,1ghcm6k,"Yeah see this is what i mean, this is the black-and-white dichotomy most open source folks seem live in. Hard to have a substantive discussion this way",2024-11-03 11:11:01,1
lux2ph9,1ghcm6k,Bit of both should do the trick ,2024-11-01 13:52:46,-2
luy7kn6,1ghcm6k,"> they had the benefit of stealing IP freely with an unlimited compute budget

Could have just as easily have described Meta, or OpenAI, or ...",2024-11-01 17:54:41,20
luz5oqp,1ghcm6k,"> they had the benefit of stealing IP freely

based.",2024-11-01 22:10:20,6
lv1b71p,1ghcm6k,"And America has stuff like The National Enquirer, New York Post, Newsday, Newsweek etc. What's the point here exactly?

There's not a single country in the world without a large contingent of trashy rags on sale.",2024-11-02 09:14:39,-1
lv8d8rw,1ghcm6k,"You are basically trusting terrorist mob states (or corporations) with a quite long history of anti-social/human/environmental behavior with AGI/ASI more than just anyone else.

The more players/people you give a chance with the technology, the more chances we have to get outside the limited frame of psychopathic self-interest fueled paths that the terrorist mob states (and the power groups that control them) will lead to. 

Just ask yourself a couple of questions:

1. Will a state or corporation direct the tech to the benefit of humanity if it would mean hurting its own interests, or even ceasing to exist?

2. Would the power groups in control of a state or corporation allow the direction of the tech to the benefit of humanity if they would hurt its interests or even cease to exist?

3. Who would have more probability to prompt/create an AGI/ASI with a completely selfless pro-humanity, pro-life in general (to include the interests of all living beings, be them conscious or not): a state or corporation; a random individual or group of individuals; NGOs; activists? Or to better phrase this, who of the named players would have the less probability of doing it?

Its the same paradigm that we faced when the US was the only one having nuclear weapons. And the same reasons applied back then to the heroic pro-humanity acts from the people that made it possible for other countries to have this technology. 

Sadly in our current environment, open source is the only chance we have to avoid falling into a VERY gloomy picture. It allows forking of our development and future in a way similar as blockchain tech does.  Which sadly, is contrary to the essential interests and reasons of existence of states and corporations.",2024-11-03 12:00:06,1
luzs0ss,1ghcm6k,"Considering the US is blocking sales of chips to China, it does describe them better than Qwen.",2024-11-02 02:28:44,7
lv1f960,1ghcm6k,">What's the point here exactly?

My brother in Christ, my original post had 2 sentences, yet you're hyperfocusing on just the first one. Why is it so hard for you to concede that british newspapers run some heavy-handed psyops evidenced by the two articles in question?

I would also love to know about my alleged motives for pointing those things and why the word ""westoid"" hurts you so much.",2024-11-02 09:36:34,1
lv05wlo,1ghcm6k,"In theory, but then you go on ebay and search for H100, and most of them are coming from China. I don't know how that ban is supposed to be working, but it doesn't seem to be working very well. Prices are usually way lower than buying one from Europe as well.",2024-11-02 04:53:39,6
lux1gfj,1ghc0ck,"If you put a P40 with the 4090 you will get 48GB of VRAM and be able to use larger models. However, Ollama doesn’t use llama.cpp row split mode. So what will happen is that inference will run on the 4090 (fast) and then on the P40 (slow). So the fastest it will go is how long it takes the P40 to process the layers. If you watch the GPU utilization you’ll see only one will working at a time. 

llama.cpp supports also has splitting by rows, and when you use this both GPUs will work together. However, the slower GPU will be the bottleneck. 

I have 3xP40 and a 3090 in a server. The P40s will slow down the 3090 so much it doesn’t even hit its power limit. Actually, the P40s slow each other down they don’t hit their power limit … but overall inference is faster. I get 9 tok/sec with llama3-70B on them. 

They’re all on PCI3x16 so that’s not a huge bottleneck. 

If I was you, I’d add another 4090. Life would be easier.",2024-11-01 13:46:06,16
luwsug5,1ghc0ck,"> Ollama will use the VRAM off both, but only use one for the computations. 

That's wrong. Each GPU will process it's own VRAM. Doing it the way you suggest would be no better than using system RAM. Since the PCIe bus would be the bottleneck.

You aren't the first one to bring up this misinformation. Where did you hear it?",2024-11-01 13:00:00,6
luya97e,1ghc0ck,"I'm not sure about your question but I have built a few RAG and run local LLM with my 4090 and I think it is sufficient for most use cases. But I suppose if you're planning on handling multiple requests it would slow down, I've never tried. 

Do you want more VRAM for bigger models or to handle multiple requests, or both?",2024-11-01 18:12:22,2
lv0a3u0,1ghc0ck,"You want another card that has [real] flash attention support. Ideally a 3090/4090. Why are you using ollama? TabbyAPI has better batching so you can serve multiple users.

M40 GPU is suicide. P40 will drag the 4090 down. 3090 drawback is lack of FP8 which more tools will make use of in the future.",2024-11-02 05:28:30,2
lv0v3s9,1ghc0ck,"Using a single 3090 with Gemma 27B (Q8) - 6 T/s

Added an RTX 2060 12GB with Gemma 27B (Q8) - 15 T/s

Improved performance by avoiding overflow to CPU and system RAM",2024-11-02 07:44:50,2
luwq9nm,1ghc0ck,It's my understanding that it somewhat degrades the performance of the more powerful card. To what degree I'm not 100% sure but it might be something you want to search the forum for. I know it's been discussed multiple times in the past,2024-11-01 12:46:26,1
luwqu06,1ghc0ck,">  Ollama will use the VRAM off both, but only use one for the computations.

That makes no sense. The model is made of layers and with multiple GPUs you split the layers between the video cards. As inference happens the processing happens on each layer. You physically have to run the model on each video card unless you swapped layers between cards at run time, which would make no sense.

I don't even know if you could load a model between a 4090 and such an old video card like a M40.",2024-11-01 12:49:24,1
luzzuij,1ghc0ck,"I used my 1080 to extend the VRAM of my 4090 for a while. Worked like a charm, with minimal impact on performance. However, I use Oobabooga as my UI, which has the option to set which GPU to use for compute, and how to split the model into the VRAM. At least if the model type supported it.

I stopped doing this a while ago though, since my currently preferred models fully fit into the 4090.",2024-11-02 03:55:30,1
lv59ihg,1ghc0ck,I exchanged my 4090 to 2x3090 and faster ssd,2024-11-03 00:25:39,1
lux268s,1ghc0ck,"I asked for a 4090 formally, I am weighting the options for when I get turned down 😂",2024-11-01 13:49:55,5
luwzmr6,1ghc0ck,I came across this info in several posts and I was skeptical as well. I guess there was some bug in llamacpp at some point or a configuration error by those users. It would be interesting to test the performance of models that exceed the 24gb by little and those that utilize a bigger amount closer to the total 48gb. Thanks for the clarification,2024-11-01 13:36:19,3
luzple2,1ghc0ck,Kind of both. There would be quite a few people (>10) using it for code completion on vs code and i would like to experiment with models larger than 24gb for general qestion answering. I suspect that rag is maybe less demanding than q&a in terms of model size,2024-11-02 01:59:46,1
lv0cp3x,1ghc0ck,"Ollama was very easy to set up. I am open to experiment with those other tools, ollama is not a strict requirement and I have control on what we deploy and how",2024-11-02 05:48:23,2
lv1ufh9,1ghc0ck,I think I'll investigate 4060 ti 16gb cards,2024-11-02 10:55:37,1
luxdq4t,1ghc0ck,"a 3090 is a good alternative, or an a6000 😆",2024-11-01 14:53:56,1
lux2604,1ghc0ck,"Well... It uses both, but not in parallel, it is sequential. To use parallel I think you need identical cards and compatible quant formats/backends (not sure as I do not use those).

One 4090 is 1TB/s memory. With two in sequence it is more like 500 GB/s since one of them is always waiting half the time (single generation, I have no experience with batches).

With one card slower, it will probably spend more time, so the faster one will wait longer. Thus it will be closer to the (half) speed of the slower one than faster one.

That said it is worth it if you want to use models that do not fit into 4090. Eg I use 4090 + 4060Ti 16GB and it is big improvement over 4090 alone. Ultimately you have to decide what size of model/quant/context size is your target and decide according to that.

I do not know P40 and M40 though, eg if there are other pitfalls connected with using older generation GPU alongside 4090.",2024-11-01 13:49:53,1
lv0dfz8,1ghc0ck,"Now that you got the hang of it, try some others. Might be pleasantly surprised. If you wrote open-ai api compatible tools, should just work.",2024-11-02 05:53:47,2
luypd1v,1ghc0ck,"None of that is the correct answer xD

Tell em ""I'm asking for a 4090 now so that I won't ask for a 5090 later"" xD",2024-11-01 19:55:26,8
luzwdao,1ghc0ck,Doesn't the performance suffer if they aren't the same banwidth etc?,2024-11-02 03:18:22,1
lv0acmy,1ghc0ck,"> I think you need identical cards

even llama.cpp has split by row and split by layer. you don't need the same card but preferably the same generation card in other backends.",2024-11-02 05:30:26,1
lv0ojas,1ghc0ck,Thanks for the tip. I'll start experimenting next week,2024-11-02 07:06:37,2
luzz6ki,1ghc0ck,"...then, ask for a 5090 later anyway. :p",2024-11-02 03:48:37,1
luw6179,1ghbmoq,"I'm personally very excited about this.. because open source and also it seems like it's just a Python package to plug and play.... It seems easy to get started.

I have many use cases locally where I was calling external gemini api for the ocr + extraction bit (because it was just easier). Now I can simply do this and simply call my local nice little llm that work on text and markdown. So nice!

I'm going to create a gradio space. Probably will share later.",2024-11-01 11:00:24,85
luwe05q,1ghbmoq,"I’ve been using docling for about a month or so. The processing speed could definitely be improved, and apparently they are working on it, but the output quality is the best of all the open-source solutions",2024-11-01 11:41:54,68
luwmsts,1ghbmoq,I wish I could upvote this more. It works better than anything like it that I've tried before.,2024-11-01 12:28:02,23
lux9ya8,1ghbmoq,how does it  compare to [https://pymupdf.readthedocs.io/en/latest/](https://pymupdf.readthedocs.io/en/latest/) ?,2024-11-01 14:32:37,12
luwiw4t,1ghbmoq,It's bad for any kind of equations or theorems or algorithms.,2024-11-01 12:07:18,11
luxmldg,1ghbmoq,"Wow, this looks promising! How does it compare to Marker/Surya?",2024-11-01 15:46:37,9
luwgfsd,1ghbmoq,"Thank you for sharing this! Have been using Qwen2-VL but the output isnt reliable enough to scale for transcription tasks. It just doesn't justify the compute time. 

Today I setup a pipeline with the Gemini API after working all week on a custom table OCR algorithm which leverages a lot more calculus than approaches elsewhere in OCR land. Maybe. Images with technical diagrams were breaking data integrity in ways I can't justify working on during company time. This beast however may be very useful.

Others who have tried a similar approach with instruction following multimodal transformers, what do you think of the cost/benefit of compute time vs accuracy? 


Should I scrap my gemini pipeline for this, even if the compute time is slow? I can spin up multiple containers on paralell but it likely wont compete with gemini speeds.",2024-11-01 11:54:27,8
luxhedu,1ghbmoq,Holy shit… this is definitely going to be useful to format training data from your workplace (which are usually all files) to fine tune a LLM.,2024-11-01 15:15:31,8
luxsxsx,1ghbmoq,Can it also extract tables that were added as image in a pdf ?,2024-11-01 16:24:39,4
luwytpj,1ghbmoq,Can anyone tell me how this compares to LLMWare? I've seen videos on LLMWare and it seems to the same thing and a bit more. I've just found these and haven't had time to try either of these but I'm going to have to make time this weekend!,2024-11-01 13:31:57,3
luxf2f9,1ghbmoq,"Thanks for sharing! So is the point that things like PyMuPDF2 (convert to markdown) and other markdown converts aren’t as good with preserving structure, sentiment etc when doing the conversion but dockling is better?",2024-11-01 15:01:44,2
luzjfkw,1ghbmoq,"This is very good OP, thanks for sharing. It plays very nicely with HTML, the lossless JSON objects is very helpful for downstream processing. The hierarchical chunker it comes with is also very good out of the box.",2024-11-02 00:44:36,2
luznutf,1ghbmoq,I wonder how well this would work for non searchable pdfs.,2024-11-02 01:38:26,1
lv04044,1ghbmoq,Very exciting.,2024-11-02 04:36:17,1
lv0p41d,1ghbmoq,"Neat!

Anyone know anything similar but for web? 
Ie html/ css + java script?",2024-11-02 07:10:08,1
lv0veno,1ghbmoq,Would be nice if they show a result in readme git page,2024-11-02 07:46:34,1
lv1ahij,1ghbmoq,"This is what I just need, thanks IBM",2024-11-02 09:10:45,1
lv1lh1v,1ghbmoq,"For the json export. Do I use the hierarchical chunking to keep hierarchy or how do I use it with rag?

Is it OK to do my own chunking and then how do I tell the llm how the json works?",2024-11-02 10:09:09,1
lv4llcs,1ghbmoq,"Seems to work okay, but not sure how much better it is than

# PyMuPDF4LLM

  
But from my tests it doesn't really parse code blocks that well, and honestly isn't as good. But may be better for other types of documents. It just seems that there's a lot of libraries that can convert pdf's to some other format (especially ones that use some aspect of a llm or sentence-transformer model), but end up being only suited for certain kinds of documents, and not any kind in general. Seems to be able to do tables better than PyMuPDF4LLM, but suffers with code. At least in my first testing.",2024-11-02 20:39:39,1
luxyd6q,1ghbmoq,[deleted],2024-11-01 16:57:18,-5
luwg6r4,1ghbmoq,"Ok so I just tried it and I have to say, it's a lot faster than [marker](https://github.com/VikParuchuri/marker). I'm on CPU-only right now and it works flawlessly, installation was really easy indeed. Took about 10 seconds for a dense 3 page PDF.


Here's the CPU-only setup command:


> pip install docling --extra-index-url https://download.pytorch.org/whl/cpu


And then:


> docling file.pdf --from pdf --to md


The second command is when it will start downloading the model if you run it for the first time.",2024-11-01 11:53:10,45
luzzbss,1ghbmoq,"Did you try on scientific papers ? How it handle equations, graphs etc..?",2024-11-02 03:50:07,1
lv0acua,1ghbmoq,"For one, this is MIT-licensed, so you can use it commercially without issues, while PyMuPDF is AGPL, rendering it useless for any serious SaaS use case.",2024-11-02 05:30:29,4
luz2cf5,1ghbmoq,Bummer. I was hoping it could help with my Coq PDFs. Hopefully they’re not too hard. 🙃,2024-11-01 21:39:08,2
luwzkmn,1ghbmoq,"Mathpix works amazingly well. Can convert a pdf to markdown or latex... equations, images, tables all of it. It's amazing.",2024-11-01 13:36:00,3
luzdge8,1ghbmoq,Yes,2024-11-01 23:33:25,1
luzz6ud,1ghbmoq,You can make OCR with Surya or Tesseract.,2024-11-02 03:48:42,1
luy9pzs,1ghbmoq,great tool and shit rant.,2024-11-01 18:08:51,4
luz0vlj,1ghbmoq,Which python version are you using? I can't seem to solve dependency issues using pip install for the CPU-only version even on a fresh venv. The regular version installs fine.,2024-11-01 21:26:03,3
lv5lq80,1ghbmoq,"Thanks for those commands, I got it working on Ubuntu WSL ARM64 running pytorch on CPU.

It's surprisingly fast for an open source model running on CPU. I fed it a bunch of papers and Wikipedia-sourced PDFs and the formatting for tables came out correct.

It crashed on PDFs with handwritten annotations and PDFs exported from OneNote with handwriting. Maybe there's something wrong with the OCR module.",2024-11-03 01:48:53,1
lux5736,1ghbmoq,"> Mathpix

is their model/code open? can we run it locally?",2024-11-01 14:06:16,3
lux7d0t,1ghbmoq,Can you provide a github link to it? Couldn’t find it so far,2024-11-01 14:18:12,2
luzvuup,1ghbmoq,"Hi! I'm using Python 3.12.7.


For pip I'm a version behind: pip 24.3.1",2024-11-02 03:12:49,1
luzxk6n,1ghbmoq,"Same problem here. I managed to install it with uv :

uv pip install docling --extra-index-url [https://download.pytorch.org/whl/cpu](https://download.pytorch.org/whl/cpu) \--index-strategy unsafe-best-match

but it didn't work (I got the docling-parse executable but not docling)",2024-11-02 03:31:22,1
lv975g3,1ghbmoq,"Worked (CPU only) with

uv venv venv --python 3.12

source venv/bin/activate

uv pip install docling torch==2.3.1+cpu torchvision==0.18.1+cpu -f [https://download.pytorch.org/whl/torch\_stable.html](https://download.pytorch.org/whl/torch_stable.html)",2024-11-03 14:34:47,1
luzfdcj,1ghbmoq,"No, it's a paid service, but worth every cent imo.",2024-11-01 23:55:45,1
luzgyth,1ghbmoq,"It's not on GitHub, https://mathpix.com.",2024-11-02 00:14:44,2
lv01205,1ghbmoq,"Hmm even on python 3.12 venv it's still not resolving for me. Oh well, going to use the default one for now. Thanks anyway!",2024-11-02 04:07:46,1
lv00xn5,1ghbmoq,Yea I'm pretty sure there are some dependency issues somewhere in the torch cpu wheel conflicting with another lib... Not going to waste time trying to figure it out and will just use the default for now...,2024-11-02 04:06:34,1
luw8n2m,1ghbbbt,Have you tested flashattetion with the new rocm update?,2024-11-01 11:14:00,3
luw4tpl,1ghbbbt,"It should work fine with either Vulkan or RPC (CUDA for P40, rocm for the 7600xt) backends",2024-11-01 10:54:11,2
luw8z7x,1ghbbbt,i am probably using the more updated version of ROCm but i don't know anything about flashattention.,2024-11-01 11:15:46,1
luwa1k7,1ghbbbt,"It was released yesterday or today, I'm not sure.

I think it's worth the test, if you do, tell us if you got good results (or not)",2024-11-01 11:21:19,2
luyxdn3,1ghbbbt,where is that news? Isnt this going to explode AMD card prices?,2024-11-01 20:56:43,1
luwcf9t,1ghaz6q,It's very sad langchain quickly captured the space for Python and JS,2024-11-01 11:33:41,3
luwowck,1ghaz6q,"If you want to test it out this repo was nice and neat 

https://github.com/edgett/PalmHill.BlazorChat",2024-11-01 12:39:15,4
luxa3b3,1ghaz6q,"I really like it. It is now stable, it works nicely with the built-in dependency injection mechanism of .net and it is quite well documented. I also tried out the Agent Implementation and this is also very straight forward. If you are working with C#, defo give it a try.",2024-11-01 14:33:24,4
luxbu0b,1ghaz6q,"I was using Semantic Kernel, but mostly its sister project [kernel-memory](https://github.com/microsoft/kernel-memory), which is doing RAG for me in two or three apps. I worked with OpenAI, Ollama and LlamaSharp directly using them.

I really recommend it for C# devs. Had some problems with function calling, but probably it was because original Llama3 was not that good.",2024-11-01 14:43:09,3
luw45we,1ghaz6q,what does it do? care to elaborate?,2024-11-01 10:50:47,7
lv6gxxv,1ghaz6q,"Although Semantic Kernel is a very good implementation, it's developed for the sake of OpenAI and Azure AI mostly, unlike every single Python implementation that supports many providers and works locally by default with llama.cpp bindings.

While Semantic Kernel support local application using llama.cpp C# binding ""LLamaSharp"", It handles the simple tasks only, never encountered any real-world usage, the most wanted feature, ""Function Calling"", is supported for OpenAI provider only, and for RAG the ""Kernel Memory"" is absolutely the winner for local and remote providers, It's just a shame that Microsoft implemented GraphRAG in python only without any support to C# with ""Kernel Memory""",2024-11-03 06:08:53,2
luywq8w,1ghaz6q,I made a blazor app with it. You can check it out [here](https://github.com/DarQro/BlazorSemanticKernel/tree/main/BlazorSemanticKernel) but it’s nothing special. I made it to demo the to show off the tech at a recent company meeting. It was just a quick demo putting this together. 😁,2024-11-01 20:51:38,1
lv1cllf,1ghaz6q,Are you Robert though?,2024-11-02 09:22:23,1
luwz6zd,1ghaz6q,"No worries, SemanticKernel is directed at corporate level applications and it will be developed as it's still (IMO) missing features. C# environment was missing a cool AI framework. I followed a tutorial recently how to make a semantic serach inside an API backend.",2024-11-01 13:33:56,3
luzaphh,1ghaz6q,Langchain is a crufty toolbox. I don't know why it's become so popular when most LLM interactions are just API calls.,2024-11-01 23:02:20,4
luzmi9r,1ghaz6q,"The whole Lang* ecosystem is a grift for people who don’t understand how to format strings, make web requests, and connect functions together.",2024-11-02 01:21:45,4
lv6fvul,1ghaz6q,"We got Langchain in .NET too 

[https://github.com/tryAGI/LangChain](https://github.com/tryAGI/LangChain)",2024-11-03 06:02:27,2
luwjswz,1ghaz6q,It’s like langchain but less wild-west. You can make prompt templates and string together LLM calls and set up tools for bots to use.,2024-11-01 12:12:09,6
luw4pqv,1ghaz6q,"It’s a framework/toolkit to use LLM’s with function calling, rag, history etc with open telemetry included. It works with Microsoft’s own AI services as well as other LLM providers including Ollama. Microsoft calls it an enterprise ready development kit for AI agents (https://learn.microsoft.com/en-us/semantic-kernel/overview/), it’s an open source project.",2024-11-01 10:53:38,4
lv8bgpp,1ghaz6q,"I never managed to use GraphRAG in a meaningful way, have you?",2024-11-03 11:51:03,1
lv1tcxr,1ghaz6q,Yes I am ☺️,2024-11-02 10:50:00,1
luw53xx,1ghaz6q,"honestly this stuff to me is much more interesting that a chat window or ""advanced voice mode"" 

not sure why there's so little interets",2024-11-01 10:55:40,3
luwdo51,1ghaz6q,"Holy crap i totally forgot about semantic kernel, and ive been ws ting to play with function calling, this makes that look trivial",2024-11-01 11:40:11,3
lv8c9sr,1ghaz6q,My current project is totally .NET focused so I haven't tried it yet.,2024-11-03 11:55:09,1
lv8f7wc,1ghaz6q,Waste of time and money..,2024-11-03 12:10:11,1
luw3y8x,1gh9v47,"Summarisation by LLM:



The Self-Lengthen framework works by utilizing two roles: the Generator and the Extender. The Generator creates an initial response, which the Extender then splits and expands, producing a longer response. This new, lengthened response is used to further train both the Generator and the Extender in an iterative manner. Through repeated iterations, the models progressively improve their ability to generate longer, coherent responses without needing external data or proprietary models.",2024-11-01 10:49:42,22
luz2su4,1gh9v47,Maybe LLMs can finally write coherent novels with this. potentially better planning capabilities too?,2024-11-01 21:43:17,2
luzb4q8,1gh9v47,Wouldn't this increase hallucinations?,2024-11-01 23:07:05,7
luynyye,1gh9v47,"I'm far from being concise, but I'd rather have a laconic LLM than a copious amount of GPT slop.",2024-11-01 19:45:25,2
luywhj1,1gh9v47,"""slop-lengthen"" ",2024-11-01 20:49:42,1
luvzbya,1gh9hhz,"Do you mean in relation to LLM? Or in general?

Windows = \*exe = mostly one-click-installer  
Linux = one-click-installer or command-line install or Software-Center (click-select)

To run LLM's under Linux it is necessary to install Nvidia Cuda Driver/Toolkit.  
Ollama, Kobolcpp, AnythingLLM etc. are running under Linux very well.",2024-11-01 10:26:08,2
lv0nerk,1gh9hhz,"For a notebook, there will be no downsides.

The main problems with the Nvidia Linux driver are:

1. can't offload VRAM to system RAM (only useful for certain StableDiffusion features like hi-res fix)
2. no ultra low latency mode for gaming (only __GL_MaxFramesAllowed=1 (ENV) & ForceCompositionPipeline=off (settings))
3. power draw slightly higher on desktop, especially if you use lots of monitors with high resolutions and refresh rates (random)
4. needs fiddly ""preserve video memory"" fix or something when suspending PC while VRAM is maxed out (?)

I think there are also drawbacks with the Windows driver, but generally speaking the Linux driver receives less development and hence more issues. It is pretty good though.",2024-11-02 06:59:40,2
luy2bx0,1gh9hhz,"wanna give WSL2 ubuntu a try first?

works like a charm with gpus too.

edit: just saw you wrote that u used WLS before, probably meant WSL already there.",2024-11-01 17:21:34,1
luw07fq,1gh9hhz,Mostly LLM related. I see most of the LLM tools and code examples are for Linux. I used WLS on Windows but it's not very flexible. So I am planning to replace Windows with Linux.,2024-11-01 10:30:35,1
luyi1mf,1gh9hhz,Yeah used WSL2 already. It has more cons than pros.,2024-11-01 19:04:21,1
luw39ur,1gh9hhz,"I am running Mint-Linux Cinnamont for several years without problems. Ollama, Kobolcpp, AnythingLLM, AlltalkTTS and a lot of other apps are running very fast and stable under Linux. 

Difference between Windows and Linux can be the update procedure. Windows will update only itself and some driver, Linux ""can"" update all at ones. Some distros can have some little problems during update process. it depends on your choice of Linux-Distro.",2024-11-01 10:46:13,3
lux9xdr,1gh9hhz,"Should be fine. On Linux Mint, you'll need to install proprietary drivers (from GUI), and, after that, CUDA should be available.",2024-11-01 14:32:28,2
luvobn6,1gh9ezo,Good question - once ada came out the AX000 series dropped in price.,2024-11-01 09:28:56,4
luvvcku,1gh9ezo,"By what do you mean the kind of work they were designed for?

All I see is Nvidia limiting their use for ai so people would have to buy the more expensive h100",2024-11-01 10:05:34,4
luwazym,1gh9ezo,"the added L2 cache nearly doubles the bandwidth. https://hardwaretimes.com/nvidia-rtx-4060-ti-128-bit-delivers-higher-bandwidth-than-the-3060-ti-256-bit-despite-slimmer-bus/

ada is better in all possible aspects, tsmc n4 is ≥2x more efficient than samsung 8nm. the better 7nm process by Samsung even compared to tsmc's N7 was already believed to be crap by the entire smartphone space. just look at abominations like the exynos 990 vs snapdragon 865.

ampere cards are still good given how recent they are, but there is no reason not to get ada unless they are massively cheaper. (eg. 3090 vs 4090)

also, factor in new features like int8 support, etc.

blackwell with tsmc n3e will be on a completely different planet",2024-11-01 11:26:18,4
luwc1wz,1gh9ezo,"Blanket statement. Ada cards are generally better at most workloads I've tested.

It might seem odd, but theoretically memory bandwidth figures at a many directly comparable SKUs have decreased on newer Ada generation- outside of the ""6000-level"" tier, Nvidia reduced bus widths but bumped memory clock speeds and greatly increased on-die cache sizes to compensate. You may end up with some rare edge cases where Ampere's raw bandwidth wins, but Ada's cache and compute gives it a massive advantage.

> Couple that with the fact that none of the ADA generation appear to have NVlink

Just like FP64, Nvidia now seems to be gatekeeping nvlink to tensor/datacenter-only GPUs. Jensen's excuses was says they needed that silicon for ""other IO"", and that Ada w/ PCIe 5.0 should be fast enough (yet strangely no Ada PCIe 5.0 cards exist). I suspect they they really, really don't want you strapping a series of Ada cards together for the fraction of an H100 and beating it.",2024-11-01 11:31:45,2
luwld67,1gh9ezo,"I mean... it depends on your financial situation and other constraints.

Dollar for dollar on the used market, of course Ampere is better because it has more bandwidth, more vram for the same cost. Personal use chatbots aren't speed limited on desktop GPUs.

But if you *must* buy new, old Ampere cards are hard to come by.

And if you're looking it less ""personal use"" use cases (like batching a really small model on a ton of queries), Ada's FP8 support, stronger compute and such start to weigh more heavily.",2024-11-01 12:20:27,2
luykbip,1gh9ezo,"Some of that depends on the model of the card. When comparing the A6000 to the Ada 6000, the Ada has higher bandwidth.

https://bizon-tech.com/gpu-benchmarks/NVIDIA-RTX-A6000-vs-NVIDIA-RTX-6000-Ada/585vs640

If you go through the data they have on those 2, you can see they had about a 20% performance uplift going to the Ada when running a 70B model, while the actual bandwidth increased by about 25%.

Something to keep in mind as well, these GPUs aren't datacenter GPUs, they're workstation GPUs. They're more focused on rendering than they are on AI.

That being said, the Ad102 (5000 ada) chip has a 98MB L3 cache, whereas the GA102 (A5000) only has 20 IIRC. This results in far less calls to the RAM, increasing the effectiveness of bandwidth that it does have. I would like to see some real world results comparing the two for AI workloads, though.",2024-11-01 19:19:54,2
luw28eb,1gh9ezo,"I'm not an expert in this but from what I recall hearing the ADA series may have some significant performance advantages for some use cases.
IIRC it may involve fast FP8 support being new in the generation.
Here's a quote from some NV page about this, FWIW:

""Ada’s new fourth-generation Tensor Cores are unbelievably fast, increasing throughput by up to 5X, to 1.4 Tensor-petaFLOPS using the new FP8 Transformer Engine, first introduced in our Hopper H100 datacenter GPU.""

https://www.nvidia.com/en-us/data-center/h100/

""H100 features fourth-generation Tensor Cores and a Transformer Engine with FP8 precision that provides up to 4X faster training over the prior generation for GPT-3 (175B) models.""
""H100 extends NVIDIA’s market-leading inference leadership with several advancements that accelerate inference by up to 30X and deliver the lowest latency. Fourth-generation Tensor Cores speed up all precisions, including FP64, TF32, FP32, FP16, INT8, and now FP8, to reduce memory usage and increase performance while still maintaining accuracy for LLMs.""


That being said I don't really recall much about models being trained or inferenced in FP8 for the ""low end"" / ""local"" use cases we talk about a lot here for inference of PTQed models, and I don't recall seeing ""interesting"" models quantized to FP8, but that could be entirely because I'm not in the sphere of users who'd use that / care about it.  Maybe server side when handling lots of users on big GPUs there's a significant use case for it?",2024-11-01 10:40:54,1
luw62sb,1gh9ezo,Yes. Significantly.,2024-11-01 11:00:37,1
lv4okbm,1gh9ezo,"While A5000 Ada is faster in prompt processing, it will be slower in inference due to slow memory speed. If you have the $$$, it is better buy the new A5880 Ada with 960GB/s 48GB VRAM.",2024-11-02 21:02:27,1
luvz3kq,1gh9ezo,"I'm inclined to agree with you there, you don't get a parallel memory bus width between Ada and Ampere until you get to the 6000 card, which seems mental. The omission of NVlink seems a deliberate artificial limitation on the Ada series, particularly as it's still PCIe 4.0. Either they want to circumvent the PCIe bottleneck or they don't. It's move to all ""chip to chip"" and seemingly exclusively on their properly top end cards that go in their clusters.",2024-11-01 10:24:57,2
luwf65s,1gh9ezo,"Amazing, that's extremely helpful, thanks.",2024-11-01 11:47:54,2
luwkfs2,1gh9ezo,"I thought the omission of pcie 5 was weird as well. I wonder if they are holding that back for the next generation, at which point they will be able to fleece data centres with something new. 

Thanks for the response, that level of nuance makes a lot more sense.",2024-11-01 12:15:32,1
luw8ejx,1gh9ezo,Any chance you could go into a bit of detail there please? I'm looking to fill a significant area of ignorance with this post.,2024-11-01 11:12:46,1
luykw48,1gh9ezo,"Originally, the Ada generation was going to be shipped with PCIe 5, but they walked that back.

PCIe 5 would have given a 64GB/s gpu to gpu interconnect, while the Ampere NVlink only provides 56GB/s.

That was the original ""official"" reason they didn't include it, even in their datacenter GPUs.",2024-11-01 19:23:50,1
luwhjzb,1gh8h5u,Are you sure they didn't use SoundStorm?,2024-11-01 12:00:17,5
luwb9z0,1gh8h5u,"I dont see any models or checkpoints listed if were expected to train from scratch even with solid architecture we likely wont match googles insane amount of compute and data to match notebooklm, better to use something like edgetts in my opinion",2024-11-01 11:27:44,4
lv3uupu,1gh8h5u,Soundstorm is available on Google cloud tts as multispeaker voices https://cloud.google.com/text-to-speech/docs/create-dialogue-with-multispeakers,2024-11-02 17:44:45,2
luwivx4,1gh8h5u,Yeah you're probably right looks like SoundStorm is the next step up from AudioLM.,2024-11-01 12:07:16,1
luwlbn2,1gh8h5u,Yeah I figured it must be prohibitively expensive to train otherwise we'd have seen something by now. Just curious if anyone know more about this stuff or has some frame of reference for what it would take to train.,2024-11-01 12:20:13,2
luzj4we,1gh7z7b,Fact that its from Kenneth Reitz alone means its worth looking at.,2024-11-02 00:41:00,2
lv0tyiw,1gh7z7b,I wish these libraries would include evaluation tools as central to the platform. It’s hard to develop anything serious with that piece.,2024-11-02 07:38:23,1
lv1uu6c,1gh7z7b,Exactly! I was a little bit shocked when I saw him get into the space.,2024-11-02 10:57:48,1
lv1uipi,1gh7z7b,"[Initial commit](https://github.com/kennethreitz/simplemind/commit/7f1196535cb3be38c16826b1ba94930a57421857) was 5 days ago, give it time!  
My own gptme has [some evals](https://gptme.org/docs/evals.html), but it is mainly a TUI/CLI and less a library.",2024-11-02 10:56:06,1
luw5kd9,1gh7nu0,"Basically, the more ""confused"" the LLM is, the more the seed will influence it's generation. If you get wildly different answers depending on the seed at reasonable temperature, it indicates the LLM has no idea what it's yapping about. A good LLM should give a good answer when given a good prompt regardless of the rng seed.",2024-11-01 10:58:00,7
luvamwq,1gh7nu0,not sure is trolling or,2024-11-01 08:16:50,22
luvflsw,1gh7nu0,The llm doesnt see the seed. It is used when temperature is applied while sampling.,2024-11-01 08:43:06,7
luvax2w,1gh7nu0,"From my experience, the smaller the dataset, the higher the influence random chance has. If your dataset is massive and your batch size is decent, then the seed doesn't have as much of an impact.

Edit: Nvm, you're talking about inference? I thought we were discussing training.",2024-11-01 08:18:21,6
luvd487,1gh7nu0,[deleted],2024-11-01 08:30:04,3
luxsd7v,1gh7nu0,This entire thread is goofy af,2024-11-01 16:21:12,2
luvjz8s,1gh7nu0,"Note that I don't know how exactly seeds and temperature work internally, so it's possible that this comment of mine is bullshit. Anyway, long time ago I discovered:

1. If you try enough seeds, you'd find a seed that doesn't reject your specific request (no ""I'm sorry but I can't assist with that""). If you try even more seeds (by brute forcing, obviously), you'd find a seed that always answers any of your requests (IIRC, I tried this with llama3.0). That's why I wasn't sure whether I should disclose the power of seeds, lol.

2. When I tried using LLMs for translation, I found that there are some seeds that cause bad translation.

3. When I tried using LLMs for translation, I found that some seeds cause the answer to start with `""`, other seeds cause no `""`, other seeds cause `Sure! ""`.

What I forgot/am lazy to test, is whether the same ""good"" seed works for Q4_K_M, Q4_K_M+imatrix, IQ4_XS, IQ4_XS+imatrix, Q8_0.

Also, I wish there was an option for SillyTavern so that at first it tried some specific seeds (when I regenerate answers), and only then it tried random seeds.",2024-11-01 09:06:00,4
luvj4mw,1gh7nu0,I saw an article about doing additional inferences producing better results than asking for everything all at once… most likely this is due to the formatting of the input but I do wonder what the side effect of switching seeds throughout output would do. Maybe it already does if you don’t have seed locked down?,2024-11-01 09:01:28,2
luwddgv,1gh7nu0,"When I play with LLMs to get them to output profound existential stuff in encodings (hex ascii, base64, etc), more than once they've come up with babble about ""42"" being the ultimate answer. It's pretty funny.",2024-11-01 11:38:39,1
luw7cav,1gh7nu0,"Nope, though I had a giggle coming up with the thread title for sure :-).

I'm building a test now, so get to the bottom of why I'm seeing relatively large influence of the seed value. All this is at temperature zero by the way.",2024-11-01 11:07:13,-2
luvisqs,1gh7nu0,"If you use a nonzero temperature in LLM inference, you can use a seed to control the sampling.",2024-11-01 08:59:45,1
luvnfsf,1gh7nu0,"Wait, you are saying you get consistent changes (like won't reject) with a given seed across multiple different prompts?

That's wild! That... shouldn't work at all.",2024-11-01 09:24:14,2
luw7kl4,1gh7nu0,"\> If you try even more seeds (by brute forcing, obviously), you'd find a seed that always answers any of your requests 

Yes, that's literally exactly what I noticed too! Hence this thread.",2024-11-01 11:08:26,1
luw7f0x,1gh7nu0,"I think switching through many seeds during a single inference/exploring parallel variants, based on some rules or even very randomly and frequently, might allow more robustness, creativity and allow the model to explore more hidden knowledge and maybe get some ""eureka!"" moments.


Like we can. We can change our mood and thoughts, actions and inspirations, (or maybe it happens fully automatically, depending on what they will find out about the ""free will""/consciousness) based on:
1) the inner state of our brain (chemicals/molecules/cells state/oxygen/nutrients/blood flow)
2) input signals that it gets from the body and sensors, including interactions and dialogues
3) from context (short-term memory), which also affects how the current inputs are perceived
4) formed especially prevalent patterns of long-term memory, through which all inputs go
5) more hard-coded (but not fully) processors controlling emotions and other automatic functions. Can also include genes, into it, or as the 6th point, or into every one of these points...


Our outputs: thoughts and actions, changes to the context and sometimes to long-term memory (and also sometimes significantly, and almost always at least a tiny bit, to the inner state of the brain and body) are generated based on it all, and changing the state of any of these parts can lead to drastically different thoughts and actions, emotions and life outcomes. Especially if there was little experienced novelty before, I guess.


I am not an AI engineer at all, but something tells me that playing with seed, attention to details (ignoring/taking into account), and maybe even processing/not processing parts of the model weights, during inference and training too, can, if done the ""right"" way, lead to more ""alive"", creative and robust (to hallucinations and not only) models.


But they must be given a way to branch and return to correct/expand/remove previous tokens a lot, and likely to be able to still remember at least some of why they did it, the slight ""history"" of changes. And all that should be possible at any moment, while predicting any next token, not done in a ""stupid"", rigid way, in next sentences/paragraphs to ""correct"" previous stuff.


And something tells me some elements of diffusion models could take the most use of it, predicting one token at a time with no way to look into the future sucks.


Just my thoughts.",2024-11-01 11:07:37,1
luw92hi,1gh7nu0,"To add to my previous comment, something tells me that if you introduce more variance, maybe even outright noise, into the system that makes predictions, both during training and inference, it may help to create more robust schemes in it, more essential, seeing the gist of concepts through all the little or big variations. And help to refine the output of the system too, making it consider more details, resist the noise, IF the schemes that it learned and operates with, are robust enough, it is given a lot of time to think, and the variance/noise during thinking is not TOO distracting and strong, somehow...",2024-11-01 11:16:14,1
luw30f2,1gh7nu0,[deleted],2024-11-01 10:44:53,1
luvv3k7,1gh7nu0,"This needs empirical experimentation. You're right, it shouldnt work.",2024-11-01 10:04:15,3
luvwup5,1gh7nu0,"No, I'm saying that with a temperature of zero, but with varying seeds, I get pretty varied results.

I that not normal?",2024-11-01 10:13:24,1
luwf629,1gh7nu0,ML paper authors definitely master the magical powers of the seed. But I agree OP is prolly just confused about what it is.,2024-11-01 11:47:54,1
luw0lbp,1gh7nu0,That may or may not. It's more strange that you are claiming each seed has different characteristics. That effect should be minimal/none,2024-11-01 10:32:33,1
luw14ll,1gh7nu0,"Yeah, it has quite an effect. On the level of ""The model won't answer the question"" -> change seed -> ""The model answers the question"".",2024-11-01 10:35:18,1
luw7w3s,1gh7nu0,"That part is normal, but are you saying that on average it makes it answer more even with different prompts? That's where it would be weird",2024-11-01 11:10:06,1
luwvxie,1gh7bk7,"I have ran SOCs on full tilt for years and they are fine so long as you aren’t gumming things up by leaving it in dusty places, blankets, etc. 

As for the speed, I’ll let you guys know when the machine arrives. It’s not a fine tuner by any means, but it has decent inference capabilities. You can do toy fine tuning on small models, it’s nice to have a local instance. 

A 100B model at q8 is probably 3 t/s on this machine. I am an optimist and certainly would be thrilled to get >5 t/s on such a model but I am not holding my breath.",2024-11-01 13:16:28,2
luxcaws,1gh7bk7,"People here said the M3 Max was good with LLM inference and it sucks with anything >= to 70B.

Models are only going to get bigger. I’d seriously consider if you really need to run LLM locally because it was not worth it to me on previous Gen and I’m not sure if this Gen makes much of a difference.",2024-11-01 14:45:48,2
luvakrg,1gh7bk7,"Speed will make it impractical for fine tuning, you'd be better off running a cloud instance for that.

Inference should be ok though, probably 5-10t/s",2024-11-01 08:16:30,1
luv9vun,1gh7bk7,$7000,2024-11-01 08:12:48,-1
lux7k7k,1gh7bk7,"Ok, so, some quick math-

I know you likely don't mean an exactly 100B but mistral large at Q8 is too large to fit so I crunched the numbers for a hypothetical exactly 100B rather than the most popular LLM in that size range-

100B @ Q8 = ~105.7GB

546GB/s max BW = 5.2 max theoretical transfers a second

So, assuming perfect bandwidth and no processing slow downs with an empty context you're looking at a ~5tokens/s ceiling.

The above is why I ended up returning my M3 Max, it's SO close to a useable product for me but was just more frustration and slowdown than it was worth. And the battery life when running inference was abysmal, to anyone hoping to buy for running LLMs I'd STRONGLY suggest just getting the Pro mini and using something portable to remote into it if you must.",2024-11-01 14:19:18,1
luxl69a,1gh7bk7,"I don't care about inference speed, I just need it to get it done. So I'm fine with waiting and loading up an entire 100gb parameter model",2024-11-01 15:38:00,1
luwdc2a,1gh7bk7,Good enough for inference. Ok so can I run Kali linux on it? lol (100% serious). No issues due to ARM processor?,2024-11-01 11:38:27,0
luxrj21,1gh7bk7,"I’m seeing $4699 for the 14 inch MacBook Pro with 128 GB

https://preview.redd.it/g1xs300phdyd1.jpeg?width=1179&format=pjpg&auto=webp&s=84559eedbdc8ee301841fa857fdcf73d856854fd",2024-11-01 16:16:10,3
luxk9lv,1gh7bk7,it's not 7000. it's 5000 pretax in the states,2024-11-01 15:32:31,1
lux86as,1gh7bk7,"That sounds about right. M4 Max’s bandwidth is nicer. Also, I do prefer 70b models and loading other smaller models in parallel. With the way the GPU has improved scheduling, it should be quite usable compared to M3 Max. ",2024-11-01 14:22:40,1
luxlka1,1gh7bk7,If you don’t mind 2-5 t/s go for it.,2024-11-01 15:40:21,3
lux98nj,1gh7bk7,"I crunched some numbers above, but 5t/s is pretty much the theoretical max. I'd suggest looking at how difficult getting the pro mini and using another laptop to connect to it remotely if you need to do inference on the go. The heat and battery drain is surprising when doing inference.

Just FYI on some more numbers-

70B Q8 = ~7tok/s

70B Q4 = ~15tok/s

Again, these are theoretical maximums and assume the memory is the only bottleneck and that all memory bandwidth gets delivered to the GPU perfectly. As context fills, any other items draw on memory, processing becomes an issue, or the memory simply doesn't deliver theoretical max you'll move down from these.",2024-11-01 14:28:36,2
luwv91a,1gh7bk7,"Based on current estimates, it should be able to process around 9-12 t/s on a 70b q4. I saw some estimates going higher, up to 17 t/s. Smaller models should process much faster. ",2024-11-01 13:12:51,1
luww4hc,1gh7bk7,Kali will run if there is an ARM version. ,2024-11-01 13:17:30,-2
luyq04e,1gh7bk7,Is there a more cost effective way to get 128gb? Maybe if you bought an old crypto mining rig..,2024-11-01 20:00:05,2
luxalfy,1gh7bk7,"Just to be clear, the math I did was with the M4 Max's top theoretical bandwidth. I like the idea of loading multiple smaller models- is that so they can talk to each other? 

I personally just want a single instance of the smartest model I can fit, which didn't really work with Apple processors at the moment. I want something I can converse with and mostly produces at/above my read speed which was not what I got. I am really hoping Apple doubles bandwidth next year now that they see the AI writing on the wall and the M5 will be my dream processor.",2024-11-01 14:36:12,2
luy63pi,1gh7bk7,there is an ARM version (there's actually several) and it doesn't run on Apple Silicon,2024-11-01 17:45:09,1
luxd2fl,1gh7bk7,Apple’s cycle at the top of the line MBP’s appears to be about limiting the performance to enforce segmentation. I am impressed by the bandwidth increase (it’s far larger than forecast) and also the GPU clock speeds. I would expect them to eventually cross the 700 GB/s mark in abour 2 to 3 years for the Max tier chips.,2024-11-01 14:50:10,1
luyhxc5,1gh7bk7,Ah darn. I guess we have to wait around for an Asahi distro that's built more like Kali.,2024-11-01 19:03:32,1
lv6uo1b,1gh7bk7,humm... kali does run on apple silicon over MacOS. running it daily via OrbStack... close to zero overhead. (another way would be via VMware Fusion Pro (sorta free now)),2024-11-03 07:26:15,1
luxdmxf,1gh7bk7,"Yah, it'll be interesting to see if we hit the theoretical max at the same percentage as the M3. Exciting times for sure.",2024-11-01 14:53:25,2
luwybee,1gh62bl,This is a genuine question I've asked myself but never bothered to find an answer to,2024-11-01 13:29:14,2
lux1ryx,1gh62bl,"Haha, I was hoping for a bit more, but at least you gave me some validation!",2024-11-01 13:47:49,1
luzfb1j,1gh62bl,"I wish I had more to give, the fact that people aren't contributing and we haven't found an answer sucks",2024-11-01 23:54:59,1
luv2lpz,1gh5x25,WordPress is basically php and html css. You may be able to find a LLM mastering the three languages ​​from the LLM leadboard.,2024-11-01 07:33:38,2
luvet63,1gh5x25,I've been using the new Sonnet model with great results with adtype and fixing bugs in WP. I haven't tried using it to help me code an entire static website but that sounds like a great project!,2024-11-01 08:38:59,2
luyot4a,1gh5x25,thank you. I will try and look at the LLM Leaderboard in huggingface,2024-11-01 19:51:28,1
luyoysd,1gh5x25,"indeed it is a nice project. I aim to be able to get a website running and learn something new about building website. 

I will try and have a look at Sonnet. thanks for the suggestion",2024-11-01 19:52:36,2
luut1i7,1gh5mgz,I've been waiting for this for a while.,2024-11-01 06:40:07,5
lv08ide,1gh5mgz,"Nice work, I'm still amazed most model servers don't enable this by default. Here's my long running PR to Ollama: https://github.com/ollama/ollama/pull/6279",2024-11-02 05:15:43,4
luvxclf,1gh5mgz,can you explain the significance?,2024-11-01 10:16:00,5
lv0iicc,1gh5mgz,"Yea, I've been watching your PR for a while, extremely disappointed in ollamas' response as it's one of the more useful features that was handed to them for free.",2024-11-02 06:28:33,5
lv1s788,1gh5mgz,LM Studio too,2024-11-02 10:43:59,2
luwsc8p,1gh5mgz,"It decreases VRAM usage for context window and speeds up larger contexts >6K. I thought it was already in there, but apparently not.",2024-11-01 12:57:20,6
lv2p3dt,1gh5mgz,1/2 - 1/4 the vRAM usage for the entire context size.,2024-11-02 13:40:05,1
lv2q276,1gh5mgz,"It's been... a battle to try and get it in.

TBH I'm really surprised this wasn't a high or even a top priority issue (after bug fixes) for them as soon as Llamacpp supported it. It seems like an absolute no-brainer as it effectively reduces memory usage by 1/2 at no noticable quality impact (q8) and 1/4 with some impact (q4).

I don't quite get how leaving the default context size for all the Ollama hub models at 2048 and the legacy Q4_0 quant type was important for compatiabiltiy with low-spec machines - but reducing the context vram usage wasn't.",2024-11-02 13:45:20,3
lv30ixo,1gh5mgz,it woks with any model?,2024-11-02 14:44:47,1
lv34l64,1gh5mgz,Any model that supports flash attention - so 99% of modern models.,2024-11-02 15:08:34,1
lv36mzs,1gh5mgz,so just updating the pip package and boom! everything gets twice the context size for same memory?,2024-11-02 15:20:41,1
lv3il61,1gh5mgz,"I'm not sure with MLX sorry, I only have experience using it for a long time with llama.cpp, exllamav2 and my ollama fork",2024-11-02 16:30:43,1
luurvg7,1gh5gk0,"Really happy how this turned out, but some work left for it to be useful. Was really easy to implement, just asked gptme to implement it with the Anthropic example as context. It quickly got something basic running, then I did some manual testing and bugfixing.

Here's the PR: [https://github.com/ErikBjare/gptme/pull/225](https://github.com/ErikBjare/gptme/pull/225)  
For those new to gptme: [https://github.com/ErikBjare/gptme](https://github.com/ErikBjare/gptme)",2024-11-01 06:33:15,7
luv5ta7,1gh5gk0,"Very cool!

The code commit with details is [here](https://github.com/ErikBjare/gptme/commit/175167e091b74fc435beaefde77bd30c88499272). You can find an example there:

https://preview.redd.it/p79nydamzayd1.png?width=866&format=png&auto=webp&s=a915272cc76f498ea0a65e0beb0404c4707c246c",2024-11-01 07:50:59,2
luwcm9p,1gh5gk0,Is this optimised for mac or will it run properly on windows too? From what i recall anthropic focused heavily on Mac and the agent showed some weird behaviour in my windows machine on occasion.,2024-11-01 11:34:42,1
luymrfg,1gh5gk0,which local model should I use with it?,2024-11-01 19:36:49,2
luzeln7,1gh5gk0,Does it use vision and which local models are supported?,2024-11-01 23:46:42,1
luwoay0,1gh5gk0,"It only supports running in Linux for now, the screenshot shows it running inside a Ubuntu Docker container (on a macOS host).

I will be looking into extending it for macOS (it can currently see, but not act), but Windows is unlikely be supported anytime soon (except as a Docker host, unless someone using Windows starts contributing).",2024-11-01 12:36:05,2
lv05i8l,1gh5gk0,use your favorite 70b or largestral,2024-11-02 04:50:08,1
luxafm5,1gh5gk0,Is it possible to get this to run on its own sandbox?,2024-11-01 14:35:18,1
lv05tnd,1gh5gk0,any way to sandbox the code execution or computer use? everything else is fine on bare metal and well.. docker is too obtuse/obese for my tastes. wouldn't mind letting the LLM into a VM though to see what it does.,2024-11-02 04:52:55,0
lv19vj3,1gh5gk0,Won't you need a vision model?  Pixtral?  llama3.2 Vision? etc.   Most of them are not in gguf and you have to run full size weights with HF transformers.    Have you given it a spin?,2024-11-02 09:07:24,1
lv1uqia,1gh5gk0,"Read the code, shouldn't be difficult.",2024-11-02 10:57:15,1
lv1ed2g,1gh5gk0,"Yea, I can run something on the 2080ti. That supports BNB and AWQ with opendai vision.",2024-11-02 09:31:50,1
luuptzb,1gh4wgz,I run cuz idk how to run in any other formats 😭,2024-11-01 06:21:04,73
luuqpo3,1gh4wgz,"Yes, I use gguf because they are easy to store, I don't understand why other formats are using folders instead single files.",2024-11-01 06:26:24,50
luunplv,1gh4wgz,"Plenty do, others use exl2 or other formats. Use the one best for you.",2024-11-01 06:07:55,70
luuvp7d,1gh4wgz,I'm using GGUF usually for CPU inference. It's easy to store and easy to access using llama.cpp from the terminal. Also it is script-friendly without a myriad of helper scripts and config files involved.,2024-11-01 06:55:17,30
luuww3l,1gh4wgz,"yes

  
Because gguf models you can expand to ram because a lack of vram ...",2024-11-01 07:02:00,31
luux8hp,1gh4wgz,"I would bet more that 95% of people that run models locally use GGUF  
Ollama, llama.cpp, LLMStudio, etc... all use GGUF",2024-11-01 07:03:58,27
luuofqn,1gh4wgz,I'd expect most people to,2024-11-01 06:12:25,11
luuq4am,1gh4wgz,"Since Ollama is the easiest entry level drug for a local LLM and it runs on llama.cpp, the answer is going to be yes.",2024-11-01 06:22:49,22
luutykb,1gh4wgz,"I have used exl2 because of tensor parallel and context sizes. GGUF has repeatedly lost performance for multi-gpu setups so I sorta gave up on it. I don't need CPU offload, too slow for my tastes.",2024-11-01 06:45:25,10
luv4web,1gh4wgz,I use mostly exl2 ,2024-11-01 07:46:06,5
luutr15,1gh4wgz,Used to but now shifted to exl2,2024-11-01 06:44:14,6
luva7lt,1gh4wgz,"I think using GGUF only makes sense when the model does not fully fit in VRAM, and [KTransformers](https://github.com/kvcache-ai/ktransformers) is not suitable either. Last time I tested, GGUF ran the models at about half the speed compared to EXL2 (in [TabbyAPI](https://github.com/theroyallab/tabbyAPI) ran as I run ""./start.sh --tensor-parallel True"" combined with speculative decoding; also, EXL2 allows to use Q6 cache, which is practically as good as Q8 but uses less VRAM). Of course, this may change in the future if inference engines which use GGUF become faster and more efficient.

Other reasons to use GGUF, if some architecture is not supported in EXL2 format yet, but supported in GGUF, or when you have software/hardware that is limited to GGUF and cannot run EXL2.

For example, since I have just four GPUs with 96GB VRAM and limited to 128GB RAM, when I wanted to test DeepSeek v2.5, not only it did not have any EXL2 quants, but even if it did, I would not be able to use them due to lack of VRAM. And I could not use KTransforms either due to lack of RAM in my rig. But [I was successfully able to run it in the GGUF format](https://www.reddit.com/r/LocalLLaMA/comments/1fclav6/comment/lmauq8p/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button), since it allowed me to split it nearly equally between RAM and VRAM.",2024-11-01 08:14:32,7
luuonw9,1gh4wgz,I used to run gguf but changed to MLX due to significant improvements recently.,2024-11-01 06:13:51,9
luuzdem,1gh4wgz,Does exist any alternative for gguf in case of cpu-only inference?,2024-11-01 07:15:51,3
luuzii2,1gh4wgz,I have to since I run P40s.,2024-11-01 07:16:38,3
luv43s1,1gh4wgz,"Both gguf and exl2 are good, but for gguf if you can’t fit the entire thing on your GPU and it overflows into ram it becomes painfully slow.",2024-11-01 07:41:49,3
luv470o,1gh4wgz,"Being where we are, I'd say that's likely. That said, a lot of people also rent GPUs and it's the same software and models.  Especially if you want to run something that's not practical for you to run at home because of the up-front investment or your available electrical wiring.

You'd need 18 months of 24/7 continuous inference at least to make owning a single A100 makes sense.  Spot pricing on Runpod is something like 0.82/hr, and that's all your costs.  And that will run a 123b model with 4bpw and decent context.  So for casual use it's a clear winner, and you can secure it with your own SSH tunnel too if you're concerned about privacy.",2024-11-01 07:42:18,3
luv9mqy,1gh4wgz,I’m a AWQ kinda guy. I think quants are awesome and so many good papers showing very little loss.,2024-11-01 08:11:27,3
luvip0z,1gh4wgz,"I just run GGUF.

For my use cases, speed beyond reading speed isn't necessary so I haven't seen any point going to EXL or whatever.

It's nice they're just 1 file, and generally seem to be the most widely supported. Plus they support CPU offload.",2024-11-01 08:59:13,3
luvq6i7,1gh4wgz,I use gguf because I can run models with it up to 37GB on my mediocre PC.,2024-11-01 09:38:44,3
luy8pjw,1gh4wgz,"I think most of this has been answered, but to summarize a few things....

Generally at Q4 (4/16bits) an LLM will still be really close to the full 16 bits.  At least for our human needs lol.
(To clarify a bit... different models will be trained or fine tuned on different data and methods, so not all quants are equal, and i couldn't say thay a Q5 models was better than a Q4 without some tests and more information.)

These numbers are purely estimations and there is variability... but....
Generally, a 22B parameter model will require ~24GB of Vram.

22B parameter at Q8 (8/16 Bits) will require ~22GB of vram 

22B parameter at Q4 (4/16 Bits) will require ~11GB of vram.

I purchased a used Quadeo P6000 on ebay which has 24Gb of vram.  It's been really nice to be able to run 32B parameter models with a decent size context.

Keep in mind your context kv cache requires vram to! Recently with kobold and llamacpp I have been playing around with quantized KV cache (8 bits) which has really allowed me to increase the context.

Of course, why is this important?  Many tools can also share ram for the model, but RAM is very slow compared to the vram.  Another issue is the actual latency from ram to cpu (which is tiny for most things, but GPUs have everything nice and close together.)

When I initially started playing around with LLMs in my works ESXI cluster, we ordered some Nvidia grid cards, but they weren't in yet.  But we had the server and a hefty. Xeon CPU with tons of cores....  I decided to try using this cpu (this was the only VM running on this host).  And I was getting very slow responses.... so I kept adding CPU cores.... 10, 20, 30, 40... I was surprised that after about 30 cores or so, performance got even worse.  I found an article about pci lanes, latences and overhead.... go figure...",2024-11-01 18:02:06,3
luuvxkt,1gh4wgz,i do miss GPTQ,2024-11-01 06:56:35,4
luuyxwd,1gh4wgz,"Yes, because most people don't have enough VRAM to run large models on it. You aren't going far with 8GB of VRAM (or even 16 for that matter).

Getting 24GB of VRAM or more costs thousands of dollars. Getting 128GB of RAM costs just 400 dollars or thereabouts.

The drawback is much slower inference (1 token\s on DDR5).",2024-11-01 07:13:29,3
luuza72,1gh4wgz,gguf? I only know ggml 👴,2024-11-01 07:15:22,2
luv0nib,1gh4wgz,"Using gguf with dual P40s. Works for me, but if anyone here gets better performance with a different format, I'd switch! I get like 5tps most of the time, but don't regularly track it. I use llama.cpp server and oobabooga.",2024-11-01 07:22:54,2
luvo9v6,1gh4wgz,"Over the past year, gguf has become the default format, yes.",2024-11-01 09:28:41,2
luvxynn,1gh4wgz,"VRAM/GPU only : exl2

want to use RAM/CPU : gguf

other great format exist",2024-11-01 10:19:09,2
luw582x,1gh4wgz,"The ""storage format"" is sort of secondary because most people AFAICT choose a storage format because it works with the inference engine they want to use and most inference engines support few categorically different format options.
That's orthogonal somewhat to ""quantization choice"" since some formats allow different quantizations to be stored within the format like GGUF and picking the quantization is critical to balance performance vs. quality vs. resource use of memory and compute.

GGUF format +llama.cpp engine is (sadly) sort of unique because few other inference engines allow you to split inference work between heterogeneous resources e.g. (CPU+RAM)+(GPU+VRAM) or maybe different mixed kinds of GPUs.  Also it kind of sort of barely supports GPUs other than NVIDIA/CUDA based ones though AMD / INTEL / VULKAN capabilities and support is very limited in comparison to the NVIDIA options in several ways.  Also when using NVIDIA GPUs GGUF+llama.cpp can be a fairly well performing inference engine which often outperforms the resource use or speed of using e.g. HF transformers, pytorch, etc. based inference engines even in cases where the whole model can run entirely on NV GPUs.

From a quantization standpoint GGUF format allows many llama.cpp/gguf specific post training quantization choices from F16, Q8, Q6, Q5, Q4 and flavors thereof so it serves most of the spectrum of quantization quality that people want to use in a pretty granular way.

In comparison I don't see people saying much about the benefits of using some other kinds of quantizations like bitsandbytes Q4 or HF pytorch 4 bit etc. which may in part be because the GGUF quant quality MIGHT be better or it could simply be because they wouldn't use those other engines vs. llama.cpp for the other reasons.

AWQ, EXL2 are some currently / formerly popular alternative formats with separate engines that can use them.

And then there's OLLAMA format which basically is a wrapped up / modified GGUF based format that you have to use with that specific inference engine while converting the input from GGUF / safetensors models for no apparently persuasive reason.  So it's part of the tower of babylon of incompatible formats and inference engines out there, but it is popular.",2024-11-01 10:56:15,2
luw9j1s,1gh4wgz,I run exl2 mostly because it's fast.,2024-11-01 11:18:38,2
luwf7bo,1gh4wgz,"Its one of the most popular formats yes, especially since its dependencies are more sensible than some of the other formats. For GGUF you can install a standalone program, the other formats such as safetensors usually rely on pytorch which would require 6GB of dependencies typically.",2024-11-01 11:48:05,2
luvkdz9,1gh4wgz,"I'm recently trying Mlx, and it seems to be quite fast, but I still need to do bench marking against gguf",2024-11-01 09:08:09,1
luvxkva,1gh4wgz,"There are other formats but they are a bit more esoteric or harder to set up unless you really know what you are doing. For the most part GGUF is the most commonplace in tools like Ollama, LMStudio, etc",2024-11-01 10:17:11,1
luwyjrl,1gh4wgz,"I used gguf cause it’s easy to just point to one file when using llama-cpp-python. Although to be fair I haven’t tried any others (not counting Ollama cause I don’t even know how they store the quants on my machine, I haven’t checked)",2024-11-01 13:30:28,1
luxr3dg,1gh4wgz,I run gguf and the only reason is thats what I see on hugging face with lots of download counts for the Q I need. I cant seem to find non gguf versions of the particular Q I need.,2024-11-01 16:13:35,1
luxwm0l,1gh4wgz,"Pretty much all frontends on mobile use GGUF, so that's what I use.

There's also the Q4_0_4_4 quants available (ARM optimized quants) that give significant performance boosts compared to standard quants, so they're what I use on mobile. I'm not exactly rolling in performance grunt, so every bit helps, and it tends to be ~25-50% extra performance on token generation, so is very worthwhile. It does mean that you're capped at Q4 dumbness, but I'm not exactly rolling in RAM either, so that's fine.

I could mess around in Termux and use safetensors, but it's on a phone. It's probably best to make it as simple and fast as possible, and pointy-clicky frontends and ARM optimized GGUFs do exactly that.",2024-11-01 16:46:35,1
luxwus5,1gh4wgz,I will try them out once in a while but frankly - IMO - the accuracy of whatever it is trained for lags and it turns me off.,2024-11-01 16:48:04,1
luz25sd,1gh4wgz,"yes. primary reason is exl2 does not support off loading (at least the last time I checked, in fact that time even if I had  enough total VRAM it still failed because it was putting everything to one GPU before separating them, at least it looked like that) and I don't have enough VRAM for more interesting models. Secondarily koboldcpp uses gguff, which  usually receives latest technology  faster than others, just after llama.cpp gets them.",2024-11-01 21:37:28,1
lv0yr69,1gh4wgz,I have a laptop with an RTX 4070 (8GB VRAM) and 16GB RAM. What are the best LLM models I can run with these specs using ollama?,2024-11-02 08:05:37,1
lv203mp,1gh4wgz,"I use gguf coz my gpu ain't strong enough(4gb vram). 
I have been out of the whole language model current news, so I don't know if things have changed with their speed or the mechanism and stuff.",2024-11-02 11:26:03,1
luv3eeg,1gh4wgz,"I use [browser-based LLM](https://www.papeg.ai)'s, so sometimes .gguf, sometimers .[onnx](https://huggingface.co/onnx-community/), and sometimes whatever the heck it is that [WebLLM](https://webllm.mlc.ai/) uses.",2024-11-01 07:37:59,1
luvbxb1,1gh4wgz,I use GGUF when I want a model as close to unquantized as my computer can handle… usually for “deep think” or “prep” moments prior to generating a lot of text. After I get that info… I use EXL2 because it looks like (not sure if this is technically true) it packs more bits into less memory or at least it seems to do so for hard drive space… and I get a model that just squeezes into my GPU so I can get fast generation.,2024-11-01 08:23:45,-1
luz14ot,1gh4wgz,"This right here, 
At this point I'm afraid to ask why gguf",2024-11-01 21:28:16,12
luw21bc,1gh4wgz,"So that you can see and edit the parameters, vocabulary/tokenizer, and config  with a simple text editor. It's very handy if you're experimenting with models, and it's how the unquantized transformers models are formatted.",2024-11-01 10:39:54,33
luyfye6,1gh4wgz,So that you can download it faster. Cause the download can be paralleled instead of single download only,2024-11-01 18:50:12,-9
luuv1ix,1gh4wgz,"I’m in the situation of “you don’t know what you don’t know”. GGUF is the only format I know I can run using a program like LMStudio. 
As for the other formats, the setups seem cumbersome",2024-11-01 06:51:35,53
luv1dik,1gh4wgz,Ask your doctor if exl2 is right for you.,2024-11-01 07:26:53,36
luwt7sx,1gh4wgz,If I know it works then exl2 if not then gguf. But I didn't use it wit video or speech. Idk if gguf can do that,2024-11-01 13:01:59,1
luv1cur,1gh4wgz,"This. 

Running models with OpenVINO actually increases my storage requirements. Suddenly one gguf of the right quant becomes the original weights plus each conversion done on the model to test performance/degradation.",2024-11-01 07:26:47,10
luvehb3,1gh4wgz,Boom! This is it. Not everyone has a dedicated gpu,2024-11-01 08:37:15,10
luw6be9,1gh4wgz,"This checks out. I'm usually* part of the stubborn 1~5%. I sometimes wish I could afford Apple products. 
*always",2024-11-01 11:01:52,3
luurdax,1gh4wgz,"i will put some details there:

yes.",2024-11-01 06:30:17,14
luvj2vf,1gh4wgz,"Lm studio is way easier than ollama, but I don’t like it’s closed source nature",2024-11-01 09:01:13,4
luvvzyi,1gh4wgz,I find pure llama.cpp the easiest.,2024-11-01 10:08:57,3
luw30cv,1gh4wgz,">I don't need CPU offload, too slow for my tastes.

So much this. Every time I think I'd like to go back and try .gguf/llama.cpp to use a model I can't fit in my hardware I get super bored waiting for prompt ingestion/eval with every single inference.

I always get context shifting pitched to me, but I find it hobbles the best ways to play with LLMs. I want my software to be able to edit all parts of the prompt before sending it to the LLM, not just append text to the end of it.

Part of that is because GPUs have so much more compute for prompt eval, I'm sure, but even running solely on VRAM llama.cpp prompt evaluation takes twice as long as exllamav2 on my setup. If I'm doing something wrong there, someone should feel free to chime in to educate me.",2024-11-01 10:44:53,5
luvvvsq,1gh4wgz,"> I think using GGUF only makes sense when the model does not fully fit in VRAM

It's more than that. You can also do distributed LLM with GGUF. How are you doing that with EXL2?",2024-11-01 10:08:20,1
luv0prr,1gh4wgz,Does exist MLX.cpp for cpu-only inference?,2024-11-01 07:23:15,6
luv7s7z,1gh4wgz,"What were the recent improvements?  I remember reading up on MLX when LMS announced support, but not finding anything worth switching over (I just run inference, I’m not doing fine tuning or anything).",2024-11-01 08:01:28,5
luusw0a,1gh4wgz,Same here. I’m using MLX more often,2024-11-01 06:39:14,7
luvcel5,1gh4wgz,What software drives that? Does text gen by Oobabooga support it? Similar to EXL2?,2024-11-01 08:26:17,2
luxj8wy,1gh4wgz,What happened to GPTQ?,2024-11-01 15:26:24,1
luvvjeg,1gh4wgz,"> Getting 24GB of VRAM or more costs thousands of dollars. 

That's ridiculously wrong. On the low end you can get a M40 with 24GB for $100 or a M10 with 32GB for the same $100. The problem with those is that they will be slow for a GPU, but will still be faster than CPU.

On the otherhand you can get a 7900xtx with 24GB for $800. That is state of the art. 

Either one of those is far from ""thousands of dollars"".",2024-11-01 10:06:33,5
luvsnp7,1gh4wgz,"Getting 24gb of VRAM costs a few hundred dollars, not sure where you're buying your hardware from but they're ripping you off lol",2024-11-01 09:51:37,4
luv7vw9,1gh4wgz,Not having a Mac sounds terrible.,2024-11-01 08:02:01,-5
luuzp4o,1gh4wgz,ggml is a library. gguf is a format.,2024-11-01 07:17:39,2
lv1zzx0,1gh4wgz,"Qwen 2.5 34B Q4?

https://huggingface.co/bartowski/Qwen2.5-32B-Instruct-GGUF",2024-11-02 11:25:30,1
lv11kk5,1gh4wgz,This is a good point. The single-file model formats are the same idea as the folder structure (it has all the same components inside) but it’s more opaque.,2024-11-02 08:21:18,3
luymxdm,1gh4wgz,Is this a real consideration in modern times? I max out connection on single downloads constantly.,2024-11-01 19:37:59,7
luyw64z,1gh4wgz,"You can absolutely download a single file in parallel chunks, browsers do this automatically.",2024-11-01 20:47:11,4
lv0plih,1gh4wgz,False. Multiple files can cause the ones and zeros to get clogged up at a choke point on the internet tube if there's a kink in it. ,2024-11-02 07:13:03,2
luvc60n,1gh4wgz,"Text gen by Oobabooga wasn’t too painful and supports a lot of formats… but I think gguf has better support with KoboldCPP, ollama, etc.",2024-11-01 08:25:02,13
luuyyrl,1gh4wgz,"Exl2 is fast and great ttft, gguf is easy to quant and universal. I don't know enough about the others to recommend.",2024-11-01 07:13:37,10
luvch9e,1gh4wgz,"If you're a p40/p100, you die!",2024-11-01 08:26:41,9
lux5e6k,1gh4wgz,Or a GPU with huge memory.,2024-11-01 14:07:20,5
luuz0fi,1gh4wgz,Some detail,2024-11-01 07:13:53,8
luwuzzk,1gh4wgz,Have you seen KoboldCPP?,2024-11-01 13:11:30,1
lv5260p,1gh4wgz,"Up to certain point  e.g. mistral large/ >100GB . The speed is only around 10 tok/s even with all layer on 3090 gpus vram. So yes, u can fit even bigger model distributedly but i failed to see a speed use case. There is no privacy usecase as well, since most people running local model for privacy, I rather trusting openai than some random dude hosting his server/gpu for free. There are ppl with multiple devices but juggling that setup is just slower and more electricity consumption comparing to fit more gpu on the same setup.",2024-11-02 23:03:27,1
luw4i6h,1gh4wgz,"That's probably outside of exllamav2's scope, which is local inference. Scope creep leads to shitty software in my experience, and I can see llama.cpp(and derivatives) heading down that path. With projects like KoboldCPP going full steam ahead on massive scope creep.

Besides, if you're running multiple systems, you're way out of most hobbyist use cases. exllamav2 could run an 8bpw 400b model on a single repurposed crypto mining system, what is the use case for distributed inference that such a system can't handle? If it's just to make the most use of your own GPU hardware, why not shell out the < $100 for a mining riser and just plug all the cards into the same system?",2024-11-01 10:52:33,1
luvs0ep,1gh4wgz,"I’m only running inference as well. Beyond potentially preserving battery life, MLX is the easiest way to run Pixtral, Llama 3.2 11B, etc. that aren’t supported by llama.cpp. 

I’ve read that llama.cpp has a more sophisticated architecture with higher quality outputs than MLX at the same quants. The only difference I’ve noticed MLX models tend to have more refusals, but I’d still like to make it my default as it seems promising.",2024-11-01 09:48:18,3
luwfhze,1gh4wgz,"I may be misinterpreting your question, but in case I am not: I suspect LM Studio is the most popular all-in-one solution which supports running MLX models on M-series Macs.",2024-11-01 11:49:36,1
luwklst,1gh4wgz,"> On the otherhand you can get a 7900xtx with 24GB for $800. That is state of the art.

How well does llama.cpp run on that card? I've never considered non-Nvidia cards before due to CUDA",2024-11-01 12:16:25,2
luwkbns,1gh4wgz,"I'm thinking about a brand new 4090, since I was comparing with 4 brand new sticks of RAM.

If we go used, things change, but RAM also gets cheaper. About AMD GPUs, I don't know enough. Do they run as well as Nvidia with CUDA?",2024-11-01 12:14:57,2
luw7fey,1gh4wgz,whats your token rate on that Mac?,2024-11-01 11:07:40,1
luvvp20,1gh4wgz,GGML is also a format. It was the format before GGUF.,2024-11-01 10:07:22,3
lv5kvgf,1gh4wgz,Thank you very much! Just tried IQ4\_XS and it is slow but great.,2024-11-03 01:38:52,2
luynbi5,1gh4wgz,"It is, i get about 12MB/s for one connection. Downloading exl format, i can have about 4-5 connection with similar speed. Basically 5x speed up. I am using both exl abd gguf though. However, downloading 70B model with gguf has always been so painfully slow due to this reason",2024-11-01 19:40:47,1
luw1mgj,1gh4wgz,"I'm okay with that. The first Pascal card (P100) is over eight years old, it predates *Attention is All You Need*. 

The amount of tech expertise you need to actually install and use those cards is a high enough barrier to entry that there is very little mass appeal. I'd rather these very skilled devs use their time more effectively than adding support for aging hardware.",2024-11-01 10:37:49,1
luzcr2o,1gh4wgz,"Yes, but for beginners who don’t want to mess around at all in a command prompt, lm studio is better.

(I’m a programmer so I don’t mind. I’m just saying that for beginners.)",2024-11-01 23:25:19,0
lv7qwp6,1gh4wgz,"> There is no privacy usecase as well, since most people running local model for privacy, I rather trusting openai than some random dude hosting his server/gpu for free.

There is absolutely a privacy use case. How is there not? Who's talking about using ""some random dude hosting his server/gpu for free"". Not me. That's the other poster.

Do you think just because it's distributed that it has to be servers spread across the globe? My distributed system is spread about as far as a 3' ethernet cable allows. How is that not a privacy use case?

> There are ppl with multiple devices but juggling that setup is just slower and more electricity consumption comparing to fit more gpu on the same setup.

No. It allows for running models that you can't do within one machine. Certainly not as cheaply. Since you would have to build out that ""same setup"" jammed full of GPUs. With distributed, you can just use what you already have. A lot of people already have a lot of devices.",2024-11-03 10:08:20,1
luw5eia,1gh4wgz,"> Scope creep leads to shitty software in my experience, and I can see llama.cpp(and derivatives) heading down that path.

That ""shitty software"" is still competitive with exllama except for tensor parallel. Which is being worked on for llama.cpp.

> Besides, if you're running multiple systems, you're way out of most hobbyist use cases. exllamav2 could run an 8bpw 400b model on a single repurposed crypto mining system, what is the use case for distributed inference that such a system can't handle?

Ah..... I think a ""repurposed crypto mining system"" is way more of a edge case than doing distributed inference. Since most people already have plenty of devices they can already use for distributed inference. From multiple PCs to handhelds like the Steamdeck to phones. People already have all that. With llama.cpp you can make a distributed system using all the things you already have. No new purchases needed. And you can still use everything for their primary purposes. You can still use your phone as your phone. You can still game on your Steamdeck. You can't do that with a mining rig.",2024-11-01 10:57:09,2
luwwzr2,1gh4wgz,Ah LM Studio. I’ve ignored it because of its license/not open source,2024-11-01 13:22:09,2
luwleqt,1gh4wgz,"Disappointingly, not super well considering the specs. It's about the same speed as my 2070. The big plus obviously is the 24GB. One can only hope it gets better. Since it has room to grow.",2024-11-01 12:20:40,3
luy091v,1gh4wgz,"4090 is great for training.

You do not need a 4090 to run chat.

I'm not sure where this stuff about needing 24GB+ VRAM is coming from either. I'm on a 2080ti and can run a 8B Q4 GGUF entirely in VRAM with 100k context using quant cache. I can also run 13B models with decent context.

Yeah, I'd like to use bigger models and will eventually be upgrading my GPU (or getting a second one).",2024-11-01 17:08:49,1
luw9vuq,1gh4wgz,"Token rate isn't where [Macs stumble against GPUs](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference), they're capable of outputting at a fairly decent rate, comparable to a 4x3090 GPU system for most hobbyist use cases. My link above has benchmarks, it's seriously 76t/s against 101t/s for M2 Ultra vs. 4x3090. At ~$6k USD, the two systems can be had for similar prices, with the Mac coming with more memory (192GB vs 96GB). You can probably obtain the 4x3090 system for a little less, but $4k USD is probably the lower limit.

However, it's prompt eval/ingest where they're absolutely atrocious, with a single 3090 more than tripling the it/s rate on eval of even a pair of the highest end M2/M3 chips. It only gets worse the more 3090s you add in to the mix. With the price equivalent amount of 3090s beating an M2 Ultra by 4.6x. For some people, that might actually be fine. Personally, I find streaming_llm or context shifting to be useless, since they don't really work if you're doing anything other than appending to the end of the last prompt you sent.",2024-11-01 11:20:30,6
luw0ua2,1gh4wgz,True. I forgott. It was a dark age back then. GGUF makrked the end of that age.,2024-11-01 10:33:50,1
lv00u1t,1gh4wgz,"Split the gguf download into parts,16 splits is the max with aria2c `aria2c -s16 -x16 <link> -o <file-name>` downloads from huggingface at about 70 MB/s. Single connection maxes out at 12 MB/s like you said.",2024-11-02 04:05:35,2
luwtksg,1gh4wgz,Those were days of reasonable prices ,2024-11-01 13:03:54,2
luzkc2f,1gh4wgz,"I use Koboldcpp, but I've never touched the command prompt to use it. The GUI is all I need.",2024-11-02 00:55:26,1
lv93t2i,1gh4wgz,"I dont see why it is not as cheap. I have 4x3090 and electricity costs already matter that i will turn off some gpus if i am not use. So i dont want to have 4 machine each with one 3090 in it, and each with cpu, and other parts that costs electricity. And this is the max of a model, i would want to go, because fitting a model that occupy the whole 96GB vram is already the slowest i would run, anything lower is not usable for me.

Again i am not saying it is not possible, it just wont make sense much for me. You will get a “kick” from being able to load bigger model. I know it when i run model like mistral large. But speed is already at the slowest i can bear with this setup.",2024-11-03 14:17:28,1
luw5zwy,1gh4wgz,"> That ""shitty software"" is still competitive with exllama except for tensor parallel. Which is being worked on for llama.cpp.
> 
> 

That hasn't been my experience even with token generation. Prompt eval is still significantly slower as well, at least on my hardware. Unless that's changed in the last couple weeks.

>distributed inference.

I'd love to see a setup like the one you're describing, especially performance numbers. I don't think that's actually practical with any llama.cpp implementation I've seen.",2024-11-01 11:00:12,1
luyhbub,1gh4wgz,"Honestly, the step from 8b to 70b is very noticeable. Llama is fab for small models, but the 70b is just like cloud and iirc just about fits into one 3090.",2024-11-01 18:59:28,1
lv0bf8y,1gh4wgz,"> I'm not sure where this stuff about needing 24GB+ VRAM is coming from > can run a 8B Q4 > can also run 13B models The answer is pretty simple - to be able to run actually large models (70b and above) that don't suck as much, at quantizations levels (like Q5 and above) that don't make them dumber than they already are",2024-11-02 05:38:49,1
luwci6e,1gh4wgz,"I know you did this a while back, and I feel like ive seen it before, but again, outstanding benchmarking.

Definitely curious, in configs where multiple GPUs are used, are they running at x16 or x8 ? Thats really just load speed, but Im interested to know.",2024-11-01 11:34:05,2
lv1joj9,1gh4wgz,It’s also worth considering the difference between a desktop tower with 3 or 4 full-size GPUs vs… a laptop. 🤷‍♂️,2024-11-02 09:59:54,1
luzkf03,1gh4wgz,"When you install it doesn’t it open a command prompt?

Again, I’m not trying to say it’s super hard or anything, I myself use ooba which uses the command prompt too, but 90% of non power users are command prompt phobic.",2024-11-02 00:56:25,1
lv9geo6,1gh4wgz,"> I dont see why it is not as cheap. I have 4x3090 and electricity costs already matter that i will turn off some gpus if i am not use.

How do you turn off GPUs? While the PCIe spec allows for that. I've never seen an implementation that worked. In fact, many people claim turning off the power to a running card is a good way to force an upgrade to a new card. Do you mean you idle them? That's not the same as turning it off. A 3090 idles at like what.... 20 watts.

With distributed inference you can take it a step beyond that. You can suspend a machine. Then WOL when you need it. Even a machine chock full of GPUs will suspend down < 10 watts. That's what saves electricity costs. So if you have 3 machines each with 32GB of VRAM. If you run a little model that fits in 32GB, just power up 1 machine. If you need 64GB, power up 2. If you need 96GB, power up a 3. Size the system to what you need.",2024-11-03 15:23:46,1
luwa6qd,1gh4wgz,"> That hasn't been my experience even with token generation. Prompt eval is still significantly slower as well, at least on my hardware. Unless that's changed in the last couple weeks.

I guess you missed this. While they didn't test exllama directly, use MLC as it's standin. That ""shitty software"" is even faster than vllm now.

https://www.reddit.com/r/LocalLLaMA/comments/1ge1ojk/updated_with_corrected_settings_for_llamacpp/

> I'd love to see a setup like the one you're describing, especially performance numbers. I don't think that's actually practical with any llama.cpp implementation I've seen.

I do it everyday with llama.cpp. I've posted about it.

https://www.reddit.com/r/LocalLLaMA/comments/1cyzi9e/llamacpp_now_supports_distributed_inference/

Also, it doesn't need to be multiple machines for it to be a win. Since even on the same machine with multiple GPUs you can use Nvidia and AMD GPUs on the same model. Can you run a model across a Nvidia and an AMD GPU with exllama?",2024-11-01 11:22:04,1
luwea2x,1gh4wgz,"I didn't do those benchmarks, I just linked to them. Can't help you with figuring out the PCIe link speeds on the test systems. Sorry.",2024-11-01 11:43:20,3
lv21zea,1gh4wgz,"I guess. I'd still take a laptop with a 3080ti mobile or A5000 mobile over an M4 Ultra(or Max, or whatever the top-end one with the most memory bandwidth is) Macbook. They'd still triple the prompt evaluation rate on my workloads. What works for your usecase isn't what's best for everyone.",2024-11-02 11:35:50,1
luzlfla,1gh4wgz,I just download and run the exe file. No setup required.,2024-11-02 01:08:40,1
lv9kaxs,1gh4wgz,"I run 2 card on egpu which are the one that i turned off. In the sense, you are right that distributed system you can go by individual card. Though l, during running u will be wasting the cpu, mainboard, psu etc for all those system. So i can see an used case for it if there is existing device lying around. But for new purchase, i dont really see it making sense to plan for it at current stage. You wont be able to use tensor parallel as well. Which is like 30% speed increase",2024-11-03 15:44:49,1
luwcrv8,1gh4wgz,">That ""shitty software""

I didn't call llama.cpp 'shitty software', I said I could see its scope creep leading to it becoming that. I'm not interesting in fanboying for software, I'm evaluating it based on my experiences.

>While they didn't test exllama directly, use MLC as it's standin.

MLC isn't an exllamav2 fork, derivative, or related in any way other than being an inference engine. Why would I use it as a standin? I've got llama.cpp and exllamav2(via TabbyAPI) on my own system, and when I run them, I see ~15% faster token rate and ~40% faster prompt eval with exllamav2.
From your link:
>
The problem with the original test was the ""--no-kv-offload"" setting and for the 4x test ""--split-mode layer"" was switched to ""--split-mode row"".

I've already made sure I'm not running llama.cpp with those options in their lower performance config. If you've got other tips for how I might get llama.cpp competitive with exllamav2 I'd love to hear them. I'm only interested in getting the best inference performance I can from my hardware.

>I do it everyday with llama.cpp. I've posted about it.

An M1 Max Studio and a PC with a 7900xtx are not a Steam Deck and a Phone. And gigabit ethernet is not wifi. I saw your post when you made it, and it's very cool, but it isn't the kind of setup you described, nor is it particularly more useful than more unified memory on the M1 Max, or adding another 7900xtx to the PC.",2024-11-01 11:35:31,2
luzli4a,1gh4wgz,"Ah my mistake! I didn’t know that. Similar to lm studio then, sounds like a great beginner choice :)",2024-11-02 01:09:30,2
lva0o8m,1gh4wgz,"> So i can see an used case for it if there is existing device lying around. But for new purchase, i dont really see it making sense to plan for it at current stage.

There's a great reason. You have two kids that both want to PC game. You get them both PC gaming rigs. When the kids are asleep, you add the rigs to your distributed inference cluster.

>  You wont be able to use tensor parallel as well.

Why do you say that? Sure. Llama.cpp doesn't support tensor parallel yet but when it does there's no reason you can't do it over a distributed system. Not everyone distributes over WiFi. Some people do it with TB4. That's 40Gb/s. That's PCIe 3 x4 speeds. TB5 ups that to 80Gb/s. That's PCIe 3 x8 or PCIe 4 x4 speeds. Plenty of people run tensor parallel over x4 or x8 PCIe 3.",2024-11-03 17:18:59,1
luwkvvp,1gh4wgz,"> I didn't call llama.cpp 'shitty software', I said I could see its scope creep leading to it becoming that. 

Exactly. You brought it up as an example of that creep.

> MLC isn't an exllamav2 fork, derivative, or related in any way other than being an inference engine. Why would I use it as a standin?

Because MLC is a speed demon. Faster than exllama. So it makes, at the very least, a worthy standin.

https://www.reddit.com/user/TNT3530/comments/1akazn8/amd_instinct_mi100_benchmarks_across_multiple_llm/

> An M1 Max Studio and a PC with a 7900xtx are not a Steam Deck and a Phone.

And you don't need the best equipment to make distributed llama.cpp worth it. You don't need gigabit ethernet. It works over WiFi. Sure, using lower end equipment and WiFi makes it slower. But slower is not slow. Faster than CPU inference still. Certain faster than not being able to run a model at all.

> nor is it particularly more useful than more unified memory on the M1 Max, or adding another 7900xtx to the PC.

Again, that's all stuff you have to buy. With distributed llama.cpp you can use what you already have.",2024-11-01 12:17:52,1
luwnlf0,1gh4wgz,">Because MLC is a speed demon. Faster than exllama. So it makes, at the very least, a worthy standin.

That's Instinct MI100 benchmarks, I'm running 3090s, as was the previous benchmark you linked. XGMI is also not a consumer solution, I'm sure if I shelled out for NVlink bridges I'd improve my performance too. Apples to oranges.

Further, exllama is deprecated. Exllamav2 is the project with active development, and it very clearly didn't work for that user with the MI100. Nor did they apparently make much effort to make it work. I'd also like to know which version of exllamav2 they're using, since there was a significant RoCM fix in 0.16, and that post states many benchmarks are out of date.

So again, if you can show me a llama.cpp config that'll beat exllamav2 on my hardware, I'd love to see it.

>But slower is not slow. Faster than CPU inference still. Certain faster than not being able to run a model at all.

Yeah, I don't believe this honestly. Even DDR3 is faster than wifi. If you can show me benchmarks of distributed inference on the kind of hardware you're pitching here, I might take you seriously. But the benchmarks of distributed llama.cpp on stuff like Raspberry Pi that I've seen are putting out t/s in the 1-2 range, and I can pull that off on my 5 year old CPU with DDR4.",2024-11-01 12:32:16,1
luwqamm,1gh4wgz,"> So again, if you can show me a llama.cpp config that'll beat exllamav2 on my hardware, I'd love to see it.

Sure. Thanks for donating your hardware to me. When should I expect it?

> Yeah, I don't believe this honestly. Even DDR3 is faster than wifi.

You simply demonstrated you don't know how inference works. Certainly not when it's distributed. And since that's basically the same as multi-gpu inference on the same machine, you don't know how that works either. Why would wifi have to be as fast as DDR3? Why would it have to be anywhere near as fast? Are you under the impression that memory needs to be access on one node from another node? It doesn't. Only a little bit of data needs to be transmitted to pass the baton from one machine to another machine. Or from one GPU to another GPU on the same machine. There doesn't need to be much bandwidth for that.

I just had this discussion, again, with someone else.

https://www.reddit.com/r/LocalLLaMA/comments/1gexy48/running_ollama_on_8x_rx_480_4gb/lue6ogi/

If you don't believe me or any of the other sources I brought up in that discussion, hopefully you'll believe turboderp. He wrote exllama.

""So at FP16 precision that's a grand total of 16 kB you're transmitting over the PCIe bus, once per token. If you multiply by, say, 20 tokens per second, then you're still only using like 0.1% of your PCIe bandwidth.""

https://github.com/turboderp/exllama/discussions/16#discussioncomment-6245573",2024-11-01 12:46:35,1
luun70a,1gh4ht7,"It works somewhat, but it's based of a misunderstanding on how they choose tokens. They use chance, increased by temperature. In sillytavern for one, you can see the token chances. In this output, it'd look something like


I think that 


Yes (47%)
No (22%)
Maybe (2%)
You (7%)
I (2%)
It (2%)


Example values, but this shows the choices they take. They do not not know these, especially on the fly. The estimation can be accurate based on what info, but it's probably off.",2024-11-01 06:04:39,15
luut87a,1gh4ht7,"Cool question! It shouldn't be too accurate but will be a bit accurate. So, by running an LLM you're emulating a human doing a task. And by asking llm to classify it's confidence and output it as tokens, the confidence classification happens on another level, inside this emulated human, so in a simulation inside a simulation. The internal world model of the simulation inside a simulation will be less complex than a simulation a layer lower, so confidence score won't be as meaningful. So, as other commenter suggested, using logits for confidence is a better idea.",2024-11-01 06:41:12,5
luvevfv,1gh4ht7,language is a fluid thing with many covariances and relations between words and meaning and so an individual score is meaningless but a distribution of scores could be meaningful,2024-11-01 08:39:17,2
luvm15f,1gh4ht7,"It’s certainly meaningful in that it isn’t random. But I wouldn’t place too much faith in the precise rating. My casual experimenting on this suggests it’s directionally right at least

I recall reading that models tend to favour their own answer when used as judge. ie using gpt4 as judge favours gpt4 answers",2024-11-01 09:16:46,2
luvtgwq,1gh4ht7,"I tried to predict confidence of multiple choice type question by asking to answer either A or B and converting the logits to probability for those tokens in the vocab. However I found out that the confidence is mostly high, even when wrong. Maybe I am doing things wrong.",2024-11-01 09:55:47,2
luvw2jv,1gh4ht7,"To some extent yes. If you ask an LLM(gpt4o-mini in my case) to provide a confidence score the chances to provide correct output is much higher.

Let consider a simple task, where you ask to classify a call transcript to one among 50 topics and provide confidence score. 

Vs 

You ask to classify a call transcript to one among 50 topics.

Output from 1 better as compared to 2.",2024-11-01 10:09:19,2
luwiyps,1gh4ht7,"I would be interested in getting a group together to workshop some of this type of thing. Let me know.

There’s a few ways to do this though, asking model(s) multiple times and quantifying and taking the most common answer. 

There’s also preference pairs and ranked choices, when you have a model rank the best answer. Llama stack came out a few weeks ago and has some of this functionality.",2024-11-01 12:07:40,2
lv0cugt,1gh4ht7,"I was using Llama3 for text evaluation (e.g. ""how disrespectful is the writer towards Y by criteria Z in text ABC"") and with long complicated texts it was so unreliable that I eventually instructed it to return me the score as ==5== with explanation and then a final verdict in form of one of those tags #a_little #very #extremely. 

This would reduce false positive rate from almost 50% to below 10%. I found that when the LLM is struggling to understand the text/question, it will also struggle to stick to rigorous formatting guidelines, as it essentially tries to improvise the answer too much. So in those cases it would mostly only return the number and not the tag or vice versa.

Otherwise, the score itself is meaningless because the LLM has no self-reflection. This is also true for qualitative descriptions like ""a little"", ""very"", ""extremely"", albeit those tend to work better with LLMs than numbers/maths. You can tell it to refuse to answer if unsure, which works a little. But if the bar is to produce a definitive answer, then you are basically applying a filter to only produce confident answers regardless of how wrong or right they are, and whatever it answers is made to be internally consistent, so it doesn't mean anything.

So while the answers the LLM might give are literally meaningless on their own, with this trick of asking it the same thing twice or thrice in a row in a different flavor (number + explanation + qualitative word) combined with special formatting (""===X===""), and then checking whether or not all answers are present and consistent and correctly formatted, I think it can be very revealing about how much the model was struggling and improvising in its answer.",2024-11-02 05:49:29,2
luutuii,1gh4ht7,"It’s a starting point. From my understanding if you want it to be good you need RLHF (human feedback and tuning). With that they can get better.

Note: I’m starting a project with this but not speaking from experience yet.",2024-11-01 06:44:47,3
luwruw0,1gh4ht7,"I have used this as part of the base prompt on ollama for a while now (% of accuracy and tellibg it to say it doesent know if under certain%) and while its not 100% accurate evaluation from it, it does give some clue and i can see a clear difference in reliability of the answer whether it says 95 or 80% accurate. Over 90% accurate is almost always spot on, while 80% quite often has some small mistakes or answer is not exactly what i asked for.

Havent tested it with chatgpt.",2024-11-01 12:54:46,1
lux2isk,1gh4ht7,"They might be meaningful for some purpose, but it wouldn't be meta-cognition.

It would simply be generating a score that is appropriate for the preceding text. The text would not need to have been generated by the same LLM, or any LLM. What cues the score is based on is anyone's guess. It might have learned something interesting.

If I were given example questionnaires filled in by humans with the confidence score edited out, and I had to guess it, then I would look for cues on the writer's confidence more than paying attention to the actual correctness of the answer. The question is if there are enough examples in the data for the LLM to learn general rules. I'm skeptical but who knows.",2024-11-01 13:51:47,1
luzn39j,1gh4ht7,"Depends on how you phrase the desired response. Asking it to give a score from 1 to 10 usually yields poor results. Rather use a score from 1 to 5 and for each number also add a qualitative description, like 1=flawless, 2=good, ... Likely has to do with LLMs not being good at estimating quantities.

I think I heard this argument from one of the authors of [the prompt report](https://arxiv.org/abs/2406.06608). And my experience confirms that.",2024-11-02 01:29:00,1
lv444x6,1gh4ht7,"Relying on verbalized confidence-scores can detect incorrect/hallucinated LLM responses with better-than-random precision/recall, you can also get similar error-detection precision/recall by relying on logprobs. But there are significantly better methods for real-time LLM error-detection -- I've run many benchmarks:

[https://cleanlab.ai/blog/4o-claude/](https://cleanlab.ai/blog/4o-claude/)",2024-11-02 18:43:21,1
luvfkej,1gh4ht7,"Yeah the logit probabilities are less than random but still not reliable, I suspect. I’ve tried asking for coin flips and other random cases, and sometimes got results as high as 99% Heads 0.000-something Tails",2024-11-01 08:42:54,3
luv5ddn,1gh4ht7,"I've played around with that guesstimation scoring concept as a percentage. As usual, the LLMs with the most parameters are better at it. And it does depend on the prompt. Fewer instructions work best. It can even articulate why it gives a low confidence score which can be helpful for refining the prompt.",2024-11-01 07:48:38,3
luwicpu,1gh4ht7,"How much testing have you done on this type of thing? I’ve done quite a bit. I’ve found Claude (specifically sonnet 3.5 but also haiku to a degree) is a lot better and consistent at these types of classification tasks. 

I think they tunes the models towards advanced classification which is probably the most useful function for task specific uses cases, which is a pillar of decision making.",2024-11-01 12:04:26,2
luydxzy,1gh4ht7,"I'm very interested in this topic. I deal with classification and extraction tasks, so really understanding a quality metric like this would be very helpful",2024-11-01 18:36:48,1
luv5cgv,1gh4ht7,"Isn't it the opposite? OAI published some GPT4 figures back in the day showing that the base model probabilities correlated almost perfectly with accuracy, while RLHF dropped that massively.",2024-11-01 07:48:30,4
luznlai,1gh4ht7,"I get much more reliable results, when I ask the model to first tell me the reason for its answer, then the final answer. Kind of a CoT effect, I guess. Improves binary classification (yes/no answers) for weaker models a lot, I find.",2024-11-02 01:35:10,1
luvvgem,1gh4ht7,"Not really. It's based on the weights. If the data trained created weights that creates that token in that context, it doesn't have to be random.",2024-11-01 10:06:07,2
luydnvy,1gh4ht7,What are your testing procedures?,2024-11-01 18:34:55,1
luyl2dd,1gh4ht7,"Look into preference optimization. And possibly using Sonnet 3.5 to build a training data set as it’s got the strongest classification in my experience.

Are you doing this stuff for your job or a project?",2024-11-01 19:25:01,1
luv9wld,1gh4ht7,"Token probabilities or rating the overall answer?

https://openai.com/index/learning-from-human-preferences/",2024-11-01 08:12:54,3
luw5fpf,1gh4ht7,"I’m only using random here to set a baseline on the classification that you can do with log probs. It’s the same as asking it to multi-class classify the likely output of a random dice roll. You would probably expect the log probs to be roughly equally distributed between 1…6. But it won’t. It will fixate on the token for 5, for example. Regardless of which model you’re using. Because they’re trained on language and not classification problems the log probs aren’t interpretable that way. 

That said, if you give them a little runway to generate, or even just ask it to output an array of percentages, models tend to preform fairly well at classification",2024-11-01 10:57:19,2
luylfjx,1gh4ht7,"I don’t have a strong methodology for testing at scale. I do benchmarks by hand and identify edge cases and test them on each model.

I also play around with having a model generate more than one output and tell it to use chain of thought before it gives its final answer. I also play around with asking it “Are you sure?” after its answers. The only model i’ve found that will reliably hold conviction when you ask that is Claude.

Like i asked in my other question, is this for work or a project?",2024-11-01 19:27:32,1
lv6dywg,1gh4ht7,Work project (research),2024-11-03 05:50:36,1
luw5go0,1gh4ht7,"Hmm I think it's [this blog post](https://openai.com/index/gpt-4-research/)

> GPT-4 can also be confidently wrong in its predictions, not taking care to double-check work when it’s likely to make a mistake. Interestingly, the base pre-trained model is highly calibrated (its predicted confidence in an answer generally matches the probability of being correct). However, through our current post-training process, the calibration is reduced.

> [Left](https://images.ctfassets.net/kftzwdyauwt9/256ef0fb-5dca-4ae1-f508b5059efc/f2a005d20ee23e1be0ba94b3192d4a77/image6.png): Calibration plot of the pre-trained GPT-4 model on an MMLU subset. The model’s confidence in its prediction closely matches the probability of being correct. The dotted diagonal line represents perfect calibration.

> [Right](https://images.ctfassets.net/kftzwdyauwt9/256ef0fb-5dca-4ae1-5fb6b9fb09f1/07448649d67ced8b879199d8ba3baba8/image3.png): Calibration plot of post-trained PPO GPT-4 model on the same MMLU subset. Our current process hurts the calibration quite a bit.

MMLU is multiple choice so one token would be all it would output in their tests. I guess both?",2024-11-01 10:57:27,3
lv6dw8u,1gh4ht7,Sorry to miss your question. A research project (social science),2024-11-03 05:50:08,1
luyb9ye,1gh4ht7,I really appreciate the reference for this. Thx!,2024-11-01 18:19:05,1
lv5jgnt,1gh4ht7,Really curious if that holds for more specific tasks. MMLU is fairly big and broad correctness test. I'm guessing RLHF still helpful for task-specific fine tunes (but happy to be corrected if I'm wrong).,2024-11-03 01:22:13,1
luuhc1q,1gh30fi,Sure. You can see that implemented [here](https://www.papeg.ai/meeting).,2024-11-01 05:25:10,1
luukzg0,1gh30fi,"i have suggestion.

after applying diarization model, you can extract longest audio segment per speaker.

for example, 

speaker1   10s \~ 15s

speaker2   110s \~ 115s 

now, it seems you have foo.mp3, bar.mp3 already

the question is, can we find best match?

i think audio embedding model can be used here",2024-11-01 05:50:17,1
luugdhg,1gh2zs3,"It depends.

Inference? 

Does it fit in gpu memory? Then that will very likely be faster. But if it doesn't fit in gpu, then cpu / system memory is likely better than not running at all.

I've ran plenty of inference with large models on cpu. It's usually slow, but fast enough for my experimental (batch) use cases. And in my specific use case it can be parallelised to run on many cores spread over a cluster anyways so it wasn't too bad.

Training? 

I wouldn't recommend cpu. A few optimisation rounds or small toy models might be OK, but you need a gpu / accelerator for anything serious.",2024-11-01 05:18:18,6
luupx3q,1gh2zs3,"I have worked on CPU for some time. Was feeding some data for automatic processings. I don't really know how slow it was, because I used to leave it working at some point, to look on results several hours later. In case I didn't like results, same data was to be processed in several ways, to give me options to choose.

Academic and research job. Sometimes bunch of motto and other creative stuff, asked by Telegram and looked over at the end of the day only. That was mostly positive experience. But I have no idea if i would have liked it in any way for chatting or asking question and waiting for an answer.

Back then time to first token was high on CPU's.

For now things might have changed, but one thing is solid and absolutely true for any time - Bandwidth matter. You will have to research that. Ask any free and big LLM in internet for details and theory.

For any way comfy work on CPU, you would like to go beyond consumer type CPU, even expensive ones. Look for EPYC or server CPUs. THey have more memory channels. If you put a lot of time and work, you can find pretty cheap solutions, as used servers are on sale quite often. But if you want to g ocasual using, then just stick to paid services or invest in GPU.",2024-11-01 06:21:36,4
luu9d36,1gh2zs3,No,2024-11-01 04:22:54,3
luuoph2,1gh2zs3,It'll be slow as hell. I run 22B models with partital CPU offload (2/3rds of the model is offloaded to CPU) and im lucky to even get a single word per second.,2024-11-01 06:14:07,1
luurzwk,1gh2zs3,"GPU is faster than CPU in deep learning because GPU has many cores while CPU has just few cores. The operations are simple enough to run on simple core, so powerful CPU is wasted because it can do just few operations at same time while GPU is powerful because it can do a lot of operations at same time.

That's why we need GPU for AI, and CPU is not really as important.",2024-11-01 06:34:00,0
luw34v9,1gh2zs3,"Depends on whether you prioritize speed or intelligence.

Even a 4bit quant of Qwen2.5 14B is too big for your GPU, but I can run 4bit Qwen 14B at almost 7t/s on CPU alone, and you can use the GPU for prompt processing which drastically cuts down on the intake time.

So in this case if 7t/s is fast enough for you, then I would personally absolutely go with CPU. 

One of my machines has 64GB of ram and a 3080m with only 8gb vram, and I run a 34b on it because its fast enough for what I need, and I'd *much* rather have the smarter model than the faster one.",2024-11-01 10:45:31,0
luugwrl,1gh2zs3,Alright! It thought the exact same. Thanks for cunfirming!,2024-11-01 05:22:08,1
luw1ma4,1gh2zs3,"Thats atrocious. What are you on DDR3-800 or something?

I get 1.5t/s with a 34B model with no GPU at all.",2024-11-01 10:37:47,2
luw2254,1gh2zs3,"This is incorrect from an inference perspective.

The reason GPU is faster with text generation is because the memory bandwidth is much higher on GPU.

I have an 5900x CPU and the speed caps out at 4 threads, it reaches like 85% speed at 2 threads, because its barely using the CPU at all.",2024-11-01 10:40:01,3
lv44uul,1gh2zs3,"Yes, as mrjackspade points out this is just incorrect, and I suspect the future of AI will be CPU driven as GPU and CPU's converge more. Typical GPUs are very limited in precision.",2024-11-02 18:47:57,1
luzq87f,1gh2zs3,"Yeah, I would only run a model on CPU to get more inteligence. THX for the reply!",2024-11-02 02:07:24,1
lv44mve,1gh2zs3,"You can do inference on CPU but you need a lot of memory bandwidth. Realistically the only CPU models right now that can do that are AMD EPYC 9000 - 12 Channel Ram, or Threadripper pro 8 channel. Not so familiar with Xeon models with high bandwidth but they exist.",2024-11-02 18:46:31,2
luwo961,1gh2zs3,"DDR4-3200 

idk whats up, i tried a 22B model at Q4\_K\_M and barely got 3t/s

My CPU is an 11600K and my GPU is a 4070 SUPER

  
maybe im doing something wrong",2024-11-01 12:35:48,1
lv3kjqp,1gh2zs3,4070 is 12gb vram? I have a rtx 3080ti with 12gb and 22b models at IQ3XS fits on the vram with 16k context at q4.,2024-11-02 16:42:24,1
lv3m165,1gh2zs3,"\>IQ3XS

\>Q4

somthing isnt adding up lol",2024-11-02 16:51:18,1
lv3n1y7,1gh2zs3,"Model at IQ3XS, context at q4 (quantized context, that saves quite a bit of vram) srry for bad English",2024-11-02 16:57:26,1
luu94k6,1gh27a6,"I don't think what you're asking makes sense.

Both grokking and generalization are something all LLMs are trained to achieve, or at least something they all achieve at different degrees, it's part of what makes all LLMs so valuable.

That paper you linked to isn't about ""hey no LLM achieves grokking and we figured out how to make one that does achieve grokking"", insetad it's about ""all LLMs achieve varying degrees of grokking, and we figured out a way to make that happen easier/faster, and/or how to make it more obvious"".

It's like, models achieve generalization all across their training, continuously, on different things, and then, a very long time after overfitting, you see this bump in further generalization, and that's grokking.

Also, grokking isn't like a single ""point"" you get to, it's something that happens progressively over a lot of different questions/thoughts/data points. First you train, then you overfit, then you grok, but it's not like a single thing.

It's a bit like ""understanding"" in your brain, there's not a day in which you understand nothing, and then a day after which you understand everything. Just like ""normal"" generalization in LLMs, grokking is a progressive thing.

So instead, you progressively understand more and more little things one after another, better and better, over time. Grokking is essentially the same. So is normal ""learning""/generalization in LLMs.

Are you maybe instead asking if there's some LLM that was specifically trained with the GrokFast (their proposed method of making grokking happen faster) implemented ?",2024-11-01 04:20:53,14
luu7jyp,1gh27a6,[deleted],2024-11-01 04:07:00,0
luucoaj,1gh27a6,"You're right, generalization and some degree of grokking are inherent goals in LLM training.   
  
My question arose from the observation that **grokking** is often observed with smaller, algorithmic datasets, which might not align with the massive, diverse datasets used to train LLMs.  
  
So what I meant is: that the question isn't about whether LLMs achieve *any* grokking, but rather whether there are models specifically designed and trained with the **explicit goal** of maximizing grokking, potentially using techniques like GrokFast or adaptive learning rate, and whether there exists any public information on this.

Because I am eager to find out if this technique has some value even with a dataset that is not clean and full of noise. Especially when every model that was trained to analyze grokking was a specialized model.",2024-11-01 04:50:16,6
luuihip,1gh27a6,Except the paper OP linked shows there are much faster ways to achieve grokking.,2024-11-01 05:33:20,0
luue2we,1gh27a6,">  but rather whether there are models specifically designed and trained with the explicit goal of maximizing grokking, 

Not that I know of no. I also asked Perplexity and Google and neither knew about any.",2024-11-01 05:01:13,2
luuevhh,1gh27a6,"Do you know any search engines that can be used to ask questions from scientific papers?  
Preferably an LLM system.",2024-11-01 05:07:16,1
luufe18,1gh27a6,"Look up [Concensus](https://consensus.app/), [LitLLM](https://github.com/shubhamagarwal92/LitLLM) and [Undermind](https://www.undermind.ai/home/).",2024-11-01 05:11:07,1
luu2xus,1gh1g89,[Relevant xkcd](https://xkcd.com/810/),2024-11-01 03:22:53,78
luu0kva,1gh1g89,The explanation site is certainly already in every training set. Thus it becomes a recall benchmark of instead of the intended understanding benchmark.,2024-11-01 02:58:13,95
luu4j2o,1gh1g89,XERAKC - xkcd explained released after knowldege cutoff. Strong vouch!,2024-11-01 03:38:44,16
luu7vw8,1gh1g89,Benchmark proposal: create-xkcd,2024-11-01 04:09:57,5
lutzkgi,1gh1g89,"Yeah, smaller VLMs struggle at understanding comics properly. I tested some LLMs on [this comic](https://cdn.prod.website-files.com/66a9bb2f66d840cb74cd3331/66df5f3a726e1984420c8836_Minecraft.webp) by theodd1sout and they performed poorly. Llama 3.2 11B hallucinated various elements, lacked understanding and could not get the joke, Qwen 2 VL 7B performed a tiny bit better, but it did so bad that it is not much different form Llama. Only Pixtral and Molmo got some aspects right but hallucinated on others, and they could not understand the humor or the context and turned the comic into a heartwarming story. Gemini 1.5 Pro and Claude 3.5 Sonnet got all the elements of the comic right but they were not able to understand the humor. Only GPT-4o was able to get it right:

>The comic strip is a humorous take on the popular game Minecraft and involves an FBI agent monitoring someone's gameplay. Here’s a breakdown of the comic:

>

>1. \*\*First Panel\*\*: The FBI agent is sitting at a desk, looking at a screen that shows a Minecraft game. He comments, ""Dang, this guy has been playing Minecraft forever. What is he even building?"" Next to him is a framed photo of his family.

>

>2. \*\*Second Panel\*\*: The agent adjusts his glasses and looks more closely at the screen, saying, ""WAIT...""

>

>3. \*\*Third Panel\*\*: The screen reveals a large pixel art in Minecraft depicting a family that looks exactly like the family in the agent's photo, with the word ""HI"" written below them.

>

>4. \*\*Fourth Panel\*\*: The agent is shocked and spits out his drink upon realizing that the Minecraft player has recreated his family picture in the game.

>

>The humor in the comic comes from the unexpected realization that the player has been using Minecraft to build a large, detailed replica of the FBI agent's family photo, implying that the player is aware of and possibly taunting the agent.",2024-11-01 02:47:18,13
lutyg0v,1gh1g89,Take my updoot,2024-11-01 02:34:38,6
luv8q3x,1gh1g89,What would actually be really cool is an LLM *making its own* xkcd comics. (good ones),2024-11-01 08:06:34,2
lutyq13,1gh1g89,This is great for vision model understanding,2024-11-01 02:37:49,3
lutyrzi,1gh1g89,"I think that with ground Truth, automatic evaluation is good enough because it's basically text similarity or entailment ",2024-11-01 02:38:26,2
lutzqk0,1gh1g89,I don’t even mind if they train for it specifically,2024-11-01 02:49:07,2
luu26be,1gh1g89,Great idea.,2024-11-01 03:15:04,2
luux34w,1gh1g89,"How about problems in real life, like engineering problems from different fields, anything that when a model achieves 80%, we can use it as assistants.",2024-11-01 07:03:07,1
luzhybk,1gh1g89,"And publish another ""thing explainer""?",2024-11-02 00:26:37,1
luzj70n,1gh1g89,You should not test on the training set.,2024-11-02 00:41:43,1
luzw0s5,1gh1g89,"I think this is not useful, because xkcd is sure to be abundant in training dataset.",2024-11-02 03:14:36,1
luucyk1,1gh1g89,"Fun, but it strikes me more like high quality training data",2024-11-01 04:52:31,0
luu4m58,1gh1g89,this is a really good one,2024-11-01 03:39:36,23
luuba6p,1gh1g89,"But XKCD release \~3 comics per week.  
As long as it takes months to train new models (and usually on yesteryear data) you should always have a fresh, never seen before, batch of comics.",2024-11-01 04:39:01,30
luu2r9m,1gh1g89,"That's a valid point about the training set. I'm not sure it would be just recall, maybe something like recall and synthesis, but I mostly agree. Fortunately, however, the author is still releasing comics, so we can still have a holdout set after their training cutoff.

I just tried on ChatGPT and it gave similar explanations to the explanation site, FYI

EDIT: it also does a good job on some of the new ones (October 2024) that I tried, so it might just be good at it, idk yet",2024-11-01 03:21:02,22
luu33bo,1gh1g89,Okay then. So we have it explain *Far Side* instead.,2024-11-01 03:24:26,11
luupppv,1gh1g89,"It would be super cool if you could get [explainxkcd.com](http://explainxkcd.com) in on it to serve (some? opt-in?) users both explanations.

I suspect *the text of* explain xkcd is in (a lot of) pre-training sets. I'd bet the *pairing* of the image and the explanation is not in many (I could be wrong). This would still be a problem, but it isn't purely recall on the part of the LLM, because it would have to somehow associate the image it is seeing with the explanation it is recalling.

So, yeah, we'd still want to restrict the competition to comics created after the training cutoff.",2024-11-01 06:20:21,4
luvww8i,1gh1g89,Maybe we get Randell on it? See if he can make a set of comics with subtle jokes and keep them out of training sets,2024-11-01 10:13:37,2
luvxhs6,1gh1g89,weird thing to assume but ok,2024-11-01 10:16:45,-3
luuv2e4,1gh1g89,"I'm with Llama 3.2 11B on this one, I fail to see an actual joke here.",2024-11-01 06:51:44,10
luuizyw,1gh1g89,"I'm not surprised 7b and 11b LLMs can't do anything with that - just try to make them explain jokes in plain text (jokes that are unlikely to have many explanations in the training data) and watch them hallucinate the most random explanations of what is the punchline.

I am surprised that any LLM actually got this one decently well at all.",2024-11-01 05:36:55,3
luuxca0,1gh1g89,"I imagine the biggest hurdle is understanding the concept of panels. Each panel is a distinct context which is often, but not always, related to adjacent panels. They work much like paragraphs do in text. If the LLM does not parse the literal description into separate panels, there's no way it will produce a useful explaination.",2024-11-01 07:04:34,2
lutz01b,1gh1g89,"ya but that's no fun :)  
  
Also, there is no guarantee the ""ground truth"" Is actually of the highest quality. What if an LLM managed to explain it better?",2024-11-01 02:40:59,2
luxquv2,1gh1g89,RAG,2024-11-01 16:12:11,-9
luu4qbc,1gh1g89,"xkcd is barely humor anymore though, mostly science posts.",2024-11-01 03:40:43,-22
luy9n3l,1gh1g89,GPT Tools,2024-11-01 18:08:20,2
luuqiql,1gh1g89,"You are right that it is not a pure text recall, the LLM needs to recognize the learned fact across the modalities so many LLMs might still perform poorly despite having memorized the correct answers. But that still doesn't make it a test of understanding the meaning.

The only way to ensure that tests don't leak into the training set is to use unpublished or synthetic examples for the evaluation.",2024-11-01 06:25:14,1
lux7xyq,1gh1g89,"Yeah, sorry, same. Even after it was explained to me, I don't see why it's funny.",2024-11-01 14:21:23,5
lutz69t,1gh1f0p,"I started wondering the same question for the same reason recently, unfortunately I don't have an answer yet though",2024-11-01 02:42:56,2
luu55zf,1gh1f0p,"Personally I never was able to make base Mixtral working well for RP, but some finetunes like Aurora 7x8B were good. I think the newer Mistral small 22B is better. If base is not good, try some finetunes. Also for RP (and anything else I guess) work on those prompts. At the very least try something like Actor or Roleplay system prompts from SillyTavern.

You could also try magnum-v3-27b-kto which is Gemma2 27B based model. Only 8k native context but otherwise it is quite good. magnum-v4 (or v2) I liked less.

Btw. with 30GB VRAM you might be able to run IQ3\_S/IQ3\_M of 70B (with some CPU offload) in acceptable speed, and then your choices will be lot better. I was running such 70B even with 24GB VRAM (4090) + DDR5 and it was generally much better than those smaller models. Unless you want large context (like 16k+, but then the small models will get confused by it anyway).",2024-11-01 03:44:55,1
luu0sb7,1gh1f0p,idk why but old models works alot better with xtc and dry.,2024-11-01 03:00:27,1
luujr7e,1gh1f0p,was running nemotron 70B llama3.1 and tried others too IQ3\_XXS only gpu it was giving 2t/s on 8k context. :\\ on any way to make them fast.,2024-11-01 05:42:06,1
luuk4j6,1gh1f0p,also magnum finetunes as too horny.,2024-11-01 05:44:35,1
luuzln8,1gh1f0p,"Yes, Magnum's are usually too horny for me too. But this one specifically seems to be Ok.

Strange that IQ3\_XXS would run so slow. What is the bandwidth on those 30GB? What backend do you use? Do you know how many layers you have offloaded? I think you will need to offload few layers with IQ3\_XXS and 8k context. Maybe try KoboldCpp where it is easy to set exactly how much you offload and how many layers to put on which GPU (I assume it is 2 GPU's? I am not familiar with 30 GB single GPU).",2024-11-01 07:17:07,1
luvmkei,1gh1f0p,"2 T4 (15x2)  
and no offloading   
using KoboldCpp .",2024-11-01 09:19:37,1
luwbxhv,1gh1f0p,"I do not know about T4, but according to spec it should have 320.0 GB/s. Considering it will be serial with GGUF, it will be effective \~160GB/s. So if all is loaded in GPU I would expect around 5 T/s from the memory bandwidth (with 30GB used). Unless IQ quants are for some reason slow on this particular HW.

I wonder if some of it might be spilling into CPU (it is a close fit even if it fits). Maybe you can try to offload 1-2 layers to see if it gets faster or slower. Also disable ""lowmem"" and ""flash attention"" if you use them, they would only slow things down from my experience.",2024-11-01 11:31:07,1
lv0hl60,1gh1f0p,offloading makes it 0.7t/s and exl2 are fast like 7t/s but issue is tabby api context shift is broken.,2024-11-02 06:22:23,1
lv0muvf,1gh1f0p,well with flashattention got 2.21t/s and without it 2.20t/s,2024-11-02 06:56:16,1
luty7n1,1gh0mhr,"AI as a risk in geopolitical terms has been a discussion point since my university days ~10-15 years ago. It went from distant theory to reality (e.g. drones using computer vision, satellite imagery analysis, etc) while I was still a student. But it was never (at least in serious discussions with academics and military that I've seen) an ""existential risk to humanity"". The risk discussion was always about cost, collateral damage, errors in predictions, buggy implementations and the like.",2024-11-01 02:31:56,48
luu3i2b,1gh0mhr,"They'll even flag you for buying lab equipment, let alone actual precursors. Some stuff is simply not for sale to non institutions. 

Plus you're unlikely to gain the experience needed to perform biochemistry by simply reading some 500 token outputs.",2024-11-01 03:28:32,22
luuvsdd,1gh0mhr,"CBRN:

* Chemical Weapons: Cleaning acids and napalm.
* Biological Weapons: Repeat exposure to antibiotics over several months.
* Radiological Weapons: These are only rare because nuclear war has not yet happened. After a nuclear exchange over Taiwan, it will be easier to acquire this material from the site with shielded robots.
* Nuclear Weapons: Getting U-237 is just to hard and requires a large footprint.

RAM:

* Rockets: Rocket candy tubes?
* Artillery: Rocket candy tubes?
* Mortars: Rocket candy tubes?

Actual risks: America has a lot of guns. When humanoid robots get converted into mass shooters, the regulatory world is going to freak out.",2024-11-01 06:55:47,18
lutsz65,1gh0mhr,"Thanks for your perspective! Agreed that fearmongering is hardly rational and it’s infuriating when people try to put down this wonderful tech for a hidden agenda. Expert-informed counters like this are a great way to cut through the bs.

Now excuse me for a sec, gonna order a truckload of fertilizer and ship it to the fearmongers… (/s)",2024-11-01 01:29:19,15
luu3efw,1gh0mhr,"Some kind of corporate racketeering going on here. All major AI cos getting together to collude on creating a moat via captured regulation. It’s clear they own the politicians who decide on regulation, otherwise they would never even bother with this.",2024-11-01 03:27:32,9
luufexw,1gh0mhr,Lol,2024-11-01 05:11:18,11
luu6sc7,1gh0mhr,Computational hardware breakthrough is pretty much available to anyone. It is now literally the farmer being able to create gigaton equivalent of the fertilizer behind the shed in a reasonably dry day. Just to make one about possibilities.,2024-11-01 04:00:00,4
luxjlpa,1gh0mhr,"Yes, it's the same with cibersecurity fearmongering. Any kind of cyber attack a current AI can do, you can pay cents to any sweatshop in Africa to do it for you, cheaper, and smarter.

When we approach ASI, super-human or top-human performance, it might affect risk a little because of scale but is not a risk that we don't have right now and we are prepared for it.",2024-11-01 15:28:33,2
luudbc9,1gh0mhr,"The problem with this thinking is that it doesn't recognize a changing world. That is, you're applying a 2000s environment to a 2020s problem.

This problem & solution exists in multiple domains, I would like to direct your attention to cellular networks and imsi catchers.

For a long time it was ""mot a knowledge problem but an availability problem."" Then software defined networking arrived and there's catch up to be played.

In your scenario, you ignore organizations to fixate on the disgruntled teen in his mom's basement. However organizations dedicated to terror exist, and the Chinese sell precursor to anyone.

So i get that while today, it's an ingredient problem (everyone else does too..) it's time to think about tomorrow, rather than depend on yesterday.",2024-11-01 04:55:17,5
luwqda1,1gh0mhr,"large companies who already trained massive models benefit greatly by quelling a rapidly evolving ecosystem where their dominance could be challenged. they also benefit from ideas that greatly exaggerate the abilities of their product.

this reminds me of rockstar supporting conservative efforts to bolster their notoriety and perception of GTA as cool and controversial

The same goes for Trump and Musk pushing the idea that they support hate groups to pretend they’re free expression advocates, or saying an intentionally provocative thing amid a scandal to bury the news.",2024-11-01 12:46:58,2
luu3xzj,1gh0mhr,"I'd argue this isn't wholly accurate, an uncensored model could for example teach a dimwit who might not previously have considered such an idea about ricin. I don't think this is grounds for the extreme legislation and censorship some like Anthropic are pushing for though.",2024-11-01 03:32:56,2
luvc2a1,1gh0mhr,Help me understand why we’d care what a cop thinks?,2024-11-01 08:24:29,0
luuoadx,1gh0mhr,"Message from a PhD research scientist working on the geopolitical risks of AI: current transformer models may increase some potential risks, including WMD and cyber.

* That doesn't mean we should help OpenAI build moats.
* That doesn't mean we should stop AI research
* That doesn't mean AI can't bring about substantial goods.

OP's position is just as emotional, irrational, and fantasy-based as ""Skynet in 2 years.""  There's room for thoughtful, nuanced, evidenced based work to minimize potential harms and maximize potential goods.",2024-11-01 06:11:30,1
luv8shz,1gh0mhr,cop,2024-11-01 08:06:55,0
luuy3c2,1gh0mhr,"Agreed. Russia, China, iran, etc definitely wouldn't have the access to the proper materials. USA #1 BABY!",2024-11-01 07:08:45,1
luur0a9,1gh0mhr,"That's painting a lot of things with the same brush. What about novel configurations? It's almost certain that humans haven't covered the entire potential of the domain.

Sure maybe for the things you can think of.

edit: if you disagree I'd love to hear your perspective",2024-11-01 06:28:09,-1
luvjun9,1gh0mhr,"Agents are about to be unleashed on the world and with that a swarm of LLM driven bots that will inundate the internet with most veracious spyware and malware viruses. Besides that every business is about to be hit with a shit-ton of cyber hacks that will cripple many of the medium to small sized businesses that don't have the IT cybersecurity infrastructure to handle the barrage. I don't think folks fully comprehend the extend of damage that these automated systems will unleash on the internet. Depending on how things go down we might see the internet end as we know it.  Consider this. The only reason the small to medium sized businesses are not exploited for loopholes in their security is because its not worth the time investment of a skilled hacker to go after them. They are low value targets. But with agents, anyone with either local resources or a tiny bit of money can send and army of them to literally go searching 24/7 for any vulnerable prey at pennies by the hour.",2024-11-01 09:05:20,-2
lux4mmf,1gh0mhr,"If you seriously think someone with the right knowledge can't grow enough botulinum (botulism toxin) to make a city's water supply lethal with just household materials in their kitchen, you're not being paranoid enough.

And if you don't think a small fleet of drones could probably successfully deliver it, you're definitely not paranoid enough.",2024-11-01 14:03:10,0
lutu1b8,1gh0mhr,"Damn, zero is a very small number.          
Let's organise, manufacture and publish incredibly detailed guides on how you make CBRN weapons of mass destruction. Knowledge is pointless. Only access",2024-11-01 01:42:15,-16
luwd1n8,1gh0mhr,"These takes come off like they're from someone who started following AI in like 2021 or something. Catastrophic risk worries predate the existence of all the AI labs, so it's not something they came up with. People have been talking about this stuff seriously since the 90s.",2024-11-01 11:36:57,-2
luvptdm,1gh0mhr,Funny how the existential risk AI big corps always tout are never related to their defense contracts,2024-11-01 09:36:48,25
luu5txg,1gh0mhr,"The issue is always going to be the state actors. In Ukraine we're now using drones with computer vision to automatically deploy to designated targets to circumvent jammers. We're producing over 10,000 of these drones everyday, the majority of them AI capable now.

While, obviously, we're using them for national defense, Russia, China, and Iran are just itching for this technology. Those are the potentially existential threats. If Anthropic is smart, they already know this and they're *still* bullshitting for domestic regulation that is just going to help their business while doing nothing to mitigate the militarization of AI. It's actually sick.",2024-11-01 03:51:10,13
luxycgm,1gh0mhr,"I don't think state-funded organizations dedicated to terror are any less capable of googling and reading relevant papers on ""how 2 bomb"" than asking an AI the question, getting a refusal, jailbreaking it and getting a step-by-step-hallucination that blows up in their faces.  
  
It's just a fabricated worry altogether.",2024-11-01 16:57:10,6
luwqwq2,1gh0mhr,"You can also google instructions for, or watch detailed guides on making, most of this stuff on YouTube.",2024-11-01 12:49:48,2
luxyw6i,1gh0mhr,"Doesn't matter what your biases are, if someone comes to you with a good faith argument and reasonable points then that's where you engage them.",2024-11-01 17:00:32,4
luwdvoz,1gh0mhr,"Understanding the existing regulatory frameworks for controlling precursors. I’m not completely sold myself, but that’s the argument",2024-11-01 11:41:16,3
luuwvat,1gh0mhr,"What exactly did you find to be ""emotional, irrational, and fantasy-based"" in my claim that supply of materials useful in CBRN threats is under surveillance?",2024-11-01 07:01:52,4
luuzuot,1gh0mhr,"Transformers/LLMs can't create actual, novel new science. It's circular reasoning. The path to reasoning and uncovering novel science may lie on discrete and Symbolic AI, such as discrete program search. People like [@victortaelin has done some engineering/algorithm work](https://xcancel.com/VictorTaelin/status/1829143659440144493) to make discrete program search a feasible solution, through the implementation of SUP nodes etc",2024-11-01 07:18:31,-4
luw13od,1gh0mhr,Precisely,2024-11-01 10:35:10,10
luvxfop,1gh0mhr,"I see a big projection here: ""We're the good guys. We would never use this technology for evil. Only China would.""

But is that really so?  
Those technologies not only CAN be misused, but they already are (e.g, making wars less personal).",2024-11-01 10:16:26,13
luun0fl,1gh0mhr,"ahahahaha


ahem ahem I heard about 100,000 a day, you lied",2024-11-01 06:03:28,-6
lv01z29,1gh0mhr,"This argument manages to not depend on yesterday, but doesn't quite upgrade to the level of thinking about tomorrow. 

The systems of *today* give step-by-step hallucinations. The concern is that the systems of tomorrow will *reason* to come up with step-by-step instructions by which to cause a lot of damage in ways no one is currently anticipating or guarding against.

If this sounds hand-wavey, just consider all of the ways engineering disasters have occurred [entirely on accident](https://youtu.be/6-3BFXpBcjc), then imagine a system that just looks at blueprints + material orders + public documents filed with regulatory agencies, then comes up with ways to cause damage of similar scale on purpose.

Hell, just consider that, prior to Star Link, a few shitty submarines would have been the only thing required to kill > 99% of internet communication between the the Continental US and the rest of the world.

The damage doesn't need to be to human life or even be geopolitically motivated. There are ways to make a quick hundred million dollars by destroying billions of dollars worth of value / infrastructure where the only real deterrent is the odds of getting convicted.

There are scenarios that aren't technically existential threats, but change the risk to reward ratios involved such that there is sufficient profit to be made by contributing toward accelerating humanity's descent into another dark age. Present day analogs of this look like ""ransomware that locks down an entire hospital's computer system."" Sufficiently powerful AI would make this sort of thing look cute by comparison.",2024-11-02 04:16:54,1
luxstb0,1gh0mhr,Yes but a completely unaligned model could plant the seeds in the head of someone who is otherwise too stupid to find that information.,2024-11-01 16:23:54,-6
luxype3,1gh0mhr,"Nothing, they were just upset at the implication that their anxious fears aren't well founded.",2024-11-01 16:59:23,4
luwb6ie,1gh0mhr,"We're defending ourselves from people coming into our land to destroy our cities, kill our children, and rape and murder our women. We didn't want this technology, but we will use it to defend ourselves.

It's doesn't take a genius to see that Russia, Iran, and China are aligning themselves against the West. It just so happens that your country Brazil is happy to work with them so long as they can do so to enrich themselves and not alienate their other trading partners.

Does it feel good? Enriching yourselves by helping to fuel a war machine that murders innocent people?  Whether it does or not, you've no platform to talk about projection. 

Ukraine doesn't claim any other nations' territories as its own. Russia and China both do, and they are unwilling to let go of imperial ambitions. Ukraine has never been an empire. But we'll defend what is ours.",2024-11-01 11:27:14,0
lv0m0r6,1gh0mhr,"Unfalsifiable hypotheticals about d00m tomorrow that aren't based on empirical trends today aren't convincing to me.

And even if they were based on existing trends, well, [https://i.redd.it/jz9s65if6ql61.jpg](https://i.redd.it/jz9s65if6ql61.jpg)",2024-11-02 06:51:07,1
luyn39b,1gh0mhr,Just like watching the bad guy do it in a tv show could plant the idea in someone’s head. Or reading it in a book. Or seeing a news report. Or a comic book….,2024-11-01 19:39:09,1
luwskko,1gh0mhr,">Does it feel good? Enriching yourselves by helping to fuel a war machine that murders innocent people? Whether it does or not, you've no platform to talk about projection.

Imagine laying down this insincere guilt trip in apology of a nation that for decades had and has a higher incarceration rate than actual, no-shit, unapologetic fascist nations like Thailand and Malaysian. Imagine saying this on behalf of a nation that completely ruined struggling nations like Haiti and Libya to do their other Statesec pack animals a solid. To speak nothing of American-driven mass murdering debacles like the Iraq War.

I’m sure I can find some other self-unaware dork from China or Russia to mewl other pulling nationalist drivel that illuminates the very real crimes of their rivals while having total amnesia on the evil shit they themselves are doing.",2024-11-01 12:58:33,-2
lv2hh6q,1gh0mhr,"> Unfalsifiable hypotheticals

It is entirely falsifiable. All one has to do is wait.

> aren't based on empirical trends today

The entirety of the above was based on present-day empirical observations. 

> aren't convincing to me. And even if they were

In summary your stance seems to amount to ""I refuse to predict anything about the future without evidence from the present -- and also I reject evidence from the present as a means of saying anything about the future.""

Which, okay you do you.",2024-11-02 12:58:32,1
luypx87,1gh0mhr,How many tv shows and comics feature detailed instructions on how to create and disseminate ricin,2024-11-01 19:59:30,-2
lux6qo1,1gh0mhr,What the fuck country are you even talking about? Are you a bot? I'm talking about Ukraine. None of the shit you're rambling about makes any sense in this discussion at all.,2024-11-01 14:14:47,4
lv2j3f8,1gh0mhr,"Yes, unfalsifiable hypotheticals. You wait. Nothing happens. As has been the case with false claims of AI danger since GPT-2. So the goalpost shifts to an eternal danger ""very soon"" instead of today. As has been the case. Over and over and over again.

I can say that ants will suddenly take over the world tomorrow, then say ""it didn't happen today, but it's coming tomorrow"" for every day that passes. You can't prove me wrong, really. Ergo, unfalsifiable argument. Same thing as AI doom arguments. The Terminator is gonna happen. Tomorrow. Then when it doesn't, the next day. Then the next...

For empirical observations: There aren't any backing AI doom. As for falsely extropolating trends, I was making a general statement about that being a fallacy of its own. A not-relevant one since in this case AI safetyists don't even have a nugget of empirical basis for their fears to extrapolate from. But sure, you can choose to interpret pointing out a potential fallacy as denying evidence.

You do you.

I don't feel the need to engage in bad faith circular AI doom-prophet arguments.",2024-11-02 13:07:16,1
luzr602,1gh0mhr,"I apologize for my compatriot; he was indoctrinated for years by a leftist agenda, and many of them lose the ability to reason and critically analyze. They live in a bubble, isolated from the real world, and end up hallucinating even more than LLMs do.",2024-11-02 02:18:34,4
luxhtr4,1gh0mhr,"They do make perfect sense. The US has a track record of invading other nations to interfere with their politics. For example, Saudi Arabia is a known dictatorship, but it's no secret to anyone they are US allies.

Not to mention that the Arab spring WAS originally sponsored by the US. They actively financed monarchist groups in Iraq and Iran, but didn't count on those groups turning against them.

I don't think the US is ""evil"", by the way. But it's definitely not the hero it wants to portray itself as.",2024-11-01 15:18:00,-2
luuoco5,1gh0dgd,"I haven't found anything better than **Open WebUI** yet.

**LM Studio** and **AnythingLLM** as you mentioned.

For RP or Story telling maybe **SillyTavern**, I personally found it a bit too much, though.",2024-11-01 06:11:54,3
luzn4wn,1gh0dgd,"Personally I think a chat frontend should have these features:

1. Can do Edit / Delete / Regenerate/ Archive (Dont send it as history messages) actions to a single message.

2. Support RAG / knowledge base.

3. Manage chats by workspace, every workspace has its own configs, like LLM provider.

4. Support customize workflow for workspaces, just like dify (the form can be graph or text script).

5. Have api.

6. Have a desktop app, easy to run and communicate locally.

7. Have global shortcut key to show chat window etc.

  
But sadly I haven't found any product fully meeting all these needs.

| Feature| NextChat| AnythingLLM| Dify                                                  |
|------------------------|--------------------------------------------------|--------------------------------------------------|-------------------------------------------------------|
| Edit / Delete / Regenerate / Archive on single message|No Archive|No Archive| No Edit / Delete / Archive|
| RAG          | needs plugin| support| support|
| Workspace    | only supports prompt-level configuration and chats aren't displayed categorized by workspace| only supports prompt-level configuration| supports workflow|
| Deploy       | has desktop app|has desktop app|needs docker|
| API          | no| supports| supports|
| Global shortcut key|supports|no|no|",2024-11-02 01:29:34,3
lutxte1,1gh0dgd,Open webui,2024-11-01 02:27:23,2
luu90l0,1gh0dgd,"Open WebUI, BigAGI, Bolt",2024-11-01 04:19:57,2
luuujdh,1gh0dgd,MSTY is the best imo,2024-11-01 06:48:44,2
luw1szz,1gh0dgd,"Open WebUI is great, but just know it's a bit under-documented.  Voice Chat is annoying because it doesn't work unless you use it via https.  It also has a few features that just don't work, like a ""Memory"" system (not RAG, but a ChatGPT-like Memory system), which doesn't work at all for me, but to be fair, it's labeled as ""experimental"".  Overall, Open WebUI is great, though.  But like the Reddit page seems kind of unattended, the docs are not thorough, a few bugs... BUT they do update it regularly, so it's worth a go for sure.",2024-11-01 10:38:44,1
luxmfmk,1gh0dgd,"SillyTavern, despite it's RP focus and silly name, is actually one of the most powerful front ends for power users. Don't be fooled by the default cards, it's extremely versatile, customizable, and more fully featured than most front ends.",2024-11-01 15:45:37,1
lv4fr4j,1gh0dgd,"I installed Open WebUI and Ollama and am testing it now. Open WebUI looks nice, but maybe a bit cluttered and incoherent in the UI .. but still nice. We'll see how it holds up. Great tip.",2024-11-02 19:58:18,1
luvaacw,1gh0dgd,"I dont do RP so I'll pass on ST, tried it, horribly complicated.

I'll try Open WebUi, thanks",2024-11-01 08:14:56,2
luyqdl7,1gh0dgd,sorry I'm confused. What about **LM Studio** and **AnythingLLM**? Are you saying Open WebUI is better?,2024-11-01 20:02:50,1
luyakts,1gh0dgd,"I can imagine, but it was very complicated when I checked it out amd the UI was very cluttered. But I will of course check it out again while investigating options.",2024-11-01 18:14:30,1
lutw95x,1gh03v4,Interesting that Gemma 27B is performing far above its size.,2024-11-01 02:09:12,49
lutuyci,1gh03v4,"Compared to when GPT-3.5 first came out, the progress has been amazing. What an era we live in!",2024-11-01 01:53:28,64
luv06sw,1gh03v4,"I think gemma is very underrated model. I was experimenting with asking some technical questions to many many models and gemma was only one which understood what I am talking about from all my local models (beating qwen 72b, llama 70b, etc).",2024-11-01 07:20:22,17
lutps7a,1gh03v4,"A couple of observations:

* All models seem to be converging! We are no longer in the ChatGPT 3.5 era where the OpenAI seemed insurmountable. There seems to be no real moat for language models
* American & Chinese companies absolutely dominate the language model space
* Mistral is the only none US/China company
* We have open models that are highly highly competitive with the best closed source models. However model weight availability does not seem to be the limitation, rather the needed hardware is the bottleneck

A couple of notes:

* Instruct/chat models were always used in the table whenever possible. 
* Sometimes the public context size disagreed with the hugginface config slightly. I always use the publicly reported context
* A yellow/blue star indicates that the best model a company is public for Facebook; Deepseek; Mistral.
* OpenAI & Anthropic (**typo in image**, see below for updated image) are the least open companies
* For xAI; 01 AI; Zhipu; Alibaba; even if their best model is closed source, they were likely derived from models they have released publicly 

https://preview.redd.it/ebtsrsa3w8yd1.png?width=2100&format=png&auto=webp&s=632fce65604c0f3327d5d152dcb8cc31f234a574",2024-11-01 00:49:43,46
luv5473,1gh03v4,Guys. It is amazing that THERE ARE open models. Thanks meta for alpaca. Where the fire started.,2024-11-01 07:47:16,11
luxbz4b,1gh03v4,"Remember to take those scores as a generalization. I run Mistral-Large-2 and Qwen-72B side by side and at most tasks, Mistral-Large is much better.",2024-11-01 14:43:56,6
luuxmig,1gh03v4,Chat Arena is a really poor benchmark of large language model quality.,2024-11-01 07:06:10,7
lutrzp8,1gh03v4,"Good stuff! Thanks for sharing!  
  
I hope we can see more variety in the country flags in the future! 🌍",2024-11-01 01:17:01,4
luv99ri,1gh03v4,"Chat arena is BS and based on ""perceived"" quality by random people (most of which are non experts for certain). Since models are ranked by general public, I expect the prompt queries to be also bogus questions like ""how many Us is queue"". 

Regardless, I find Gemma and Llama both pretty amazing",2024-11-01 08:09:30,4
luu3jle,1gh03v4,"Do we know where the new Sonnet is ranked, honestly should be #1 imo.",2024-11-01 03:28:58,4
luujy04,1gh03v4,Too many question marks I say,2024-11-01 05:43:21,2
luve6i3,1gh03v4,"What a weird data format. You have open weights under closed models, wtf",2024-11-01 08:35:40,2
luwj502,1gh03v4,Why is 405b meta's closed model? surely they have some actual closed models.,2024-11-01 12:08:37,2
luu79zk,1gh03v4,"I really love mistral, i dont need any other models but european models. Just because it has to follow the rules in EU",2024-11-01 04:04:30,5
luwau1e,1gh03v4,The duality of Llama. It's openclosed.,2024-11-01 11:25:26,1
luviucd,1gh03v4,No way GPT-2 is best open source right?,2024-11-01 08:59:59,1
luu0ei5,1gh03v4,Free tibet free xinjiang remember the falun gong massacre,2024-11-01 02:56:19,-19
luvjpte,1gh03v4, .:./_1,2024-11-01 09:04:38,0
lutzz7z,1gh03v4,I bet chinese bots are flooding the arena to crank up the scores for chinese llms.,2024-11-01 02:51:45,-22
luu0875,1gh03v4,50 cent army hard at work in the arena!,2024-11-01 02:54:26,-19
luwe6g8,1gh03v4,I don’t trust a benchmark that can’t even spell the company names right.,2024-11-01 11:42:48,-2
luudsdg,1gh03v4,"True (although, you don’t see for example qwen 2.5 32b because 72b is higher, but 32b is actually pretty close.. >90% of the mmlu pro score, I think). I wonder if it is the context size?",2024-11-01 04:58:58,17
luuzmxt,1gh03v4,What a time to be alive!,2024-11-01 07:17:19,12
luvwnlt,1gh03v4,"That's my experience with Gemma 2 too. It may even objectively get the answer wrong, but it is clear that it ""understood"" what should be done. It is by far the best model I was able to test locally.",2024-11-01 10:12:23,8
luucz07,1gh03v4,"Models are definitely converging when it comes to text, but they are diverging when it comes to features. Like it or not, between o1 and GPT-4o, OpenAI still has a sizable moat -- o1 for its reasoning ability and 4o for its ability to synthetize voice from raw tokens in real time. Some future model that combines these modalities is going to be a monster.",2024-11-01 04:52:37,20
luu6mt5,1gh03v4,Free tibet free xinjiang remember the falun gong massacre,2024-11-01 03:58:34,-11
luwccif,1gh03v4,That is the best open model from the company behind the model in the best overall ranking. The open models are not separately ranked.,2024-11-01 11:33:17,5
luufpn2,1gh03v4,"I just checked the leaderboard itself: Gemma 9B is just one step below 27B (38th - no?), which makes that one far more impressive!",2024-11-01 05:13:29,16
luyf00q,1gh03v4,"I think it's cuz it's trained similarly to Gemini, or night even be a distill (don't remember exactly, might have read that about Gemini flash)",2024-11-01 18:43:51,1
luug2l6,1gh03v4,That moat isn't going to last since AI's will code their own features soon enough. The concept of AI/AGI sort of flies in the face of 'moats' tbh.,2024-11-01 05:16:08,-8
luuqay6,1gh03v4,It's not funnier the second time,2024-11-01 06:23:57,5
luvbfdj,1gh03v4,"Arena score is deceptive imo since its based off of human interaction or chatting, I'd rather compare via MMLU-PRO [https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)",2024-11-01 08:21:03,7
luu6c7e,1ggzj03,"Plain text. Some people have had luck having an LLM rewrite their descriptions to be more concise.

Also like to put example dialogue inside the prompt now rather than instruct formatted ""messages""",2024-11-01 03:55:50,9
lutof3t,1ggzj03,What is W++?,2024-11-01 00:32:53,5
luyo6d8,1ggzj03,"I thought w++ was just worse in terms of quality and token count by a large margin since late 2023, where plain text in the character's voice gets the best results, but i could be wrong",2024-11-01 19:46:54,2
luw2i61,1ggzj03,"I don't think the format matters as much as the content, assuming you're talking about the more recent LLMs with larger context windows.  But I've taken to just filling out a [json template](https://rentry.org/3kk8sk2e) whenever i want to make/edit a character.  It's a bit token heavy so I wouldn't recommend it for something with an 8k or less context window, but if you've got the extra context to play with, most models understand json very well and it's nice to standardize.",2024-11-01 10:42:18,1
luxwcip,1ggzj03,A good rule of thumb is that smaller models need more description and large ones need less. Sometimes larger models can be constrained by more detail.,2024-11-01 16:45:02,1
luvpxq9,1ggzj03,I guess I should give plain text descriptions a try.,2024-11-01 09:37:27,3
luu8t1n,1ggzj03,"when making a character card LLM Rp'ers usually resort to format called W++ than just using natural language. format looks like this

Name: xyz  
Species: xyz  
occupation: xyz  
Relationship: xyz  
Voice: ......  
Traits: ...........  
behaviour: ..........  
and so on....

usually this format helps models to read descriptions properly and not confuse between elements of character descrption.",2024-11-01 04:18:11,-2
luuqbmz,1ggzj03,"This is not W++. This is:



Appearance: blue eyes + broad shoulders + red cape + short brown hair


W++ doesn't help with anything, and in fact, it makes the responses dumber. Even the creator of this format recommends to not use it, since it was designed for a specific set of models from when LLMs couldn't understand full instruct sentences.",2024-11-01 06:24:03,13
luurlmk,1ggzj03,I think it was also to keep character descriptions short back when we were stuck on models with only 2048 token context lengths. Seems so long ago...,2024-11-01 06:31:38,6
luvq4bj,1ggzj03,Have you personally seen a model be dumber when using W++ rather than plain text descriptions? Just wondering because this is making me think I should switch to plain text.,2024-11-01 09:38:25,2
lv5opjk,1ggzj03,"Yes, in particular Mistral Nemo and Mistral Small both work worse with W++. Before that, I also used Llama fine-tunes which exhibited similar behaviour. Characters defined using W++ were one-note and did not follow the character descriptions (or they did, rather, but the instructions were bad so the output was less than ideal).",2024-11-03 02:22:24,1
